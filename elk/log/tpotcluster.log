[2022-04-07T17:21:54,404][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-07T17:21:54,435][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-07T17:21:54,436][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-07T17:22:03,441][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-07T17:22:03,442][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-07T17:22:03,442][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-07T17:22:03,443][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-07T17:22:03,443][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-07T17:22:03,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-07T17:22:03,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-07T17:22:03,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-07T17:22:03,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-07T17:22:03,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-07T17:22:03,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-07T17:22:03,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-07T17:22:03,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-07T17:22:03,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-07T17:22:03,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-07T17:22:03,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-07T17:22:03,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-07T17:22:03,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-07T17:22:03,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-07T17:22:03,452][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-07T17:22:03,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-07T17:22:03,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-07T17:22:03,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-07T17:22:03,455][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-07T17:22:03,455][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-07T17:22:03,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-07T17:22:03,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-07T17:22:03,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-07T17:22:03,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-07T17:22:03,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-07T17:22:03,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-07T17:22:03,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-07T17:22:03,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-07T17:22:03,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-07T17:22:03,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-07T17:22:03,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-07T17:22:03,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-07T17:22:03,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-07T17:22:03,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-07T17:22:03,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-07T17:22:03,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-07T17:22:03,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-07T17:22:03,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-07T17:22:03,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-07T17:22:03,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-07T17:22:03,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-07T17:22:03,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-07T17:22:03,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-07T17:22:03,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-07T17:22:03,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-07T17:22:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-07T17:22:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-07T17:22:03,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-07T17:22:03,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-07T17:22:03,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-07T17:22:03,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-07T17:22:03,473][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-07T17:22:03,474][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-07T17:22:03,476][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-07T17:22:03,569][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.2gb], net total_space [125.8gb], types [ext4]
[2022-04-07T17:22:03,570][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-07T17:22:03,886][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-07T17:22:16,294][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-07T17:22:16,297][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-07T17:22:17,350][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-07T17:22:17,532][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-07T17:22:18,224][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-07T17:22:19,054][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-07T17:22:19,055][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-07T17:22:19,096][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-07T17:22:19,097][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-07T17:22:19,307][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-07T17:22:21,931][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-07T17:22:22,095][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2R__AD4AQIit4e9WQnq1Zg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 213, version: 7796, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2R__AD4AQIit4e9WQnq1Zg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-07T17:22:22,287][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{2R__AD4AQIit4e9WQnq1Zg}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 213, version: 7796, reason: Publication{term=213, version=7796}
[2022-04-07T17:22:22,411][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-07T17:22:22,411][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-07T17:22:23,728][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-07T17:22:23,735][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [40] indices into cluster_state
[2022-04-07T17:22:24,535][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-07T17:22:24,536][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-07T17:22:25,246][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-07T17:22:25,685][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-07T17:22:25,688][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-07T17:22:25,688][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-07T17:22:26,229][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-07T17:22:26,681][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-07T17:22:29,742][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-07T17:22:29,806][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-07T17:22:29,824][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-07T17:22:30,561][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-07T17:22:30,562][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-07T17:22:31,191][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-07T17:22:39,360][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-07T17:22:39,849][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-07T17:22:43,384][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-07T17:22:43,416][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-07T17:22:43,419][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-07T17:22:45,001][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-07T17:22:45,024][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-07T17:22:45,435][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-07T17:22:45,450][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-07T17:22:47,931][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-07T17:22:47,944][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-07T17:22:53,285][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.06][0]]]).
[2022-04-07T17:23:29,401][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 929 finished with response BulkByScrollResponse[took=165.2ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-07T17:23:31,916][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 954 finished with response BulkByScrollResponse[took=2.3s,timed_out=false,sliceId=null,updated=1021,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-07T17:23:41,705][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-07T17:24:21,512][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.07] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-07T17:24:21,707][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.07][0]]]).
[2022-04-07T17:24:21,878][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:21,998][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,007][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,126][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,312][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,417][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,533][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:22,636][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:24,196][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:24,284][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:33,965][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,118][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,232][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,247][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,484][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:24:34,645][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:09,020][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:09,274][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:09,364][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:16,298][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:27,240][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:50,365][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:25:50,494][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:26:43,467][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:27:15,388][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:27:35,427][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:27:36,430][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:37,290][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:37,388][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:37,546][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:37,663][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:38,277][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:38,353][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:38,549][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:28:39,297][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:32:04,762][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:32:04,841][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:33:40,836][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:34:15,551][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:34:35,890][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:34:35,982][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:41:37,252][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:43:00,790][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:51:54,956][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T17:54:12,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [31253ms] which is above the warn threshold of [5000ms]
[2022-04-07T17:54:25,876][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33836}] took [8623ms] which is above the warn threshold of [5000ms]
[2022-04-07T17:54:45,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017ebbb8@31fca5f0] took [7937ms] which is above the warn threshold of [5000ms]
[2022-04-07T17:55:16,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8849ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:00:24,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8849445577ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:00:25,486][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@50d26e41, interval=5s}] took [328997ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:00:25,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328997ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:00:25,826][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [328997ms] which is above the warn threshold of [5s]
[2022-04-07T18:00:25,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/328997085101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:00:34,752][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:09:57,311][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:11:37,913][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:11:38,018][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:11:38,455][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:21:48,597][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:26:47,314][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:26:47,338][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:26:47,444][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T18:27:48,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [7888ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:27:59,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [5642ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:28:15,980][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3461][68] duration [726ms], collections [1]/[2s], total [726ms]/[3.2s], memory [1.4gb]->[1.4gb]/[2gb], all_pools {[young] [1.1gb]->[0b]/[0b]}{[old] [231.8mb]->[231.8mb]/[2gb]}{[survivor] [6.9mb]->[7.3mb]/[0b]}
[2022-04-07T18:28:16,050][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][3461] overhead, spent [726ms] collecting in the last [2s]
[2022-04-07T18:28:47,181][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@50d26e41, interval=5s}] took [14464ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:28:59,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [8777ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:29:01,101][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7084ms] which is above the warn threshold of [5s]
[2022-04-07T18:29:46,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22000ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:29:47,391][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3471][69] duration [18.3s], collections [1]/[4.7s], total [18.3s]/[21.6s], memory [391.2mb]->[411.2mb]/[2gb], all_pools {[young] [172mb]->[192mb]/[0b]}{[old] [231.8mb]->[231.8mb]/[2gb]}{[survivor] [7.3mb]->[7.3mb]/[0b]}
[2022-04-07T18:29:48,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21999617535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:29:49,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][3471] overhead, spent [18.3s] collecting in the last [4.7s]
[2022-04-07T18:29:50,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [32904ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:30:12,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [5390ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:30:43,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:31:09,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [26523ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:33:36,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173036ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:35:11,045][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173256655183ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:35:37,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122519ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:36:03,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122651057349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:36:15,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@76093688, interval=5s}] took [122651ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:37:38,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:38:30,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120581543895ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:39:31,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:39:50,309][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113070760631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:40:52,527][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:41:56,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79841862698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:43:30,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/158618ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:43:56,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/158878714798ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:44:59,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88647ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:43:30,824][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35004}] took [192913ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:45:45,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/88646648755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:47:14,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/135120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:47:41,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/135119683585ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:47:53,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:47:59,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@264be63d, interval=1m}] took [39530ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:48:15,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39530588650ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:55:40,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/467034ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:55:54,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/467033840028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:56:27,380][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3475][70] duration [5.9m], collections [1]/[18.1m], total [5.9m]/[6.3m], memory [288.2mb]->[320.2mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [231.9mb]->[231.9mb]/[2gb]}{[survivor] [8.3mb]->[9.5mb]/[0b]}
[2022-04-07T18:56:31,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.7s/47760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:56:45,383][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][3475] overhead, spent [5.9m] collecting in the last [18.1m]
[2022-04-07T18:56:46,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.7s/47759418269ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:57:00,158][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [514793ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:57:03,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:57:21,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34682724464ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:56:36,949][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:35870}] took [514794ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:57:43,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:58:00,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40s/40049006132ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:58:17,695][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34578ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:58:33,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.5s/34577235486ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:57:55,789][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [40049ms] which is above the warn threshold of [5s]
[2022-04-07T18:58:52,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.5s/32577ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:06,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.5s/32577481768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:24,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:40,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.8s/33867265791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:38,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@50d26e41, interval=5s}] took [33867ms] which is above the warn threshold of [5000ms]
[2022-04-07T18:59:53,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29820ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:00:02,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29819932069ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T18:59:47,384][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [26532] timed out after [43865ms]
[2022-04-07T19:00:22,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28592ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:00:34,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.5s/28591647216ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:00:50,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27901ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:01:02,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.9s/27901041062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:02:46,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:02:53,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/116199767134ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:02,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:09,276][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3476][71] duration [1.2m], collections [1]/[14.4m], total [1.2m]/[7.6m], memory [320.2mb]->[261.4mb]/[2gb], all_pools {[young] [8mb]->[24mb]/[0b]}{[old] [231.9mb]->[232.8mb]/[2gb]}{[survivor] [9.5mb]->[8.5mb]/[0b]}
[2022-04-07T19:03:17,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16246199935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:27,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24328ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:28,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [16246ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:03:39,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24328160647ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:03:51,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24051ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:04:16,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24051298386ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:04:42,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.4s/51474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:05:11,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.4s/51473200267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:05:10,625][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017ebbb8@14a77b80] took [51473ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:05:38,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.8s/54836ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:05:52,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.8s/54836522923ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:06:37,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.4s/59435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:06:42,023][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true&track_total_hits=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33886}] took [114271ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:06:52,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.4s/59435184013ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:08,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27210ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:10,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [86645ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:07:18,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27209919236ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:30,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25845ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:38,304][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33868}] took [25845ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:07:40,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.8s/25844808055ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:07:51,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:13:13,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.1s/21113146196ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:14:20,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:15:11,829][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][3478][72] duration [3.9m], collections [1]/[1.4m], total [3.9m]/[11.6m], memory [281.4mb]->[305.4mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [232.8mb]->[232.8mb]/[2gb]}{[survivor] [8.5mb]->[7.9mb]/[0b]}
[2022-04-07T19:14:38,853][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_7.17.0/_search?from=0&rest_total_hits_as_int=true&size=20][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33846}] took [389407ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:07:52,495][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [21113ms] which is above the warn threshold of [5s]
[2022-04-07T19:16:19,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389406459956ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:16:39,921][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][3478] overhead, spent [3.9m] collecting in the last [1.4m]
[2022-04-07T19:18:35,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/241294ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:19:07,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@219b1f60, interval=1s}] took [651545ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:20:33,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/241025719786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:19:36,142][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.1s/12113ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@49939439]], which exceeds the warn threshold of [10s]
[2022-04-07T19:23:01,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:23:22,324][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@76093688, interval=5s}] took [275131ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:25:11,877][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [12.4s/12483ms] to compute cluster state update for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21ff3fea], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@824ea792]], which exceeds the warn threshold of [10s]
[2022-04-07T19:26:11,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/275131003606ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:29:37,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399225ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:32:50,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399190505779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:32:22,366][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.1s/15172ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21ff3fea], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@824ea792]], which exceeds the warn threshold of [10s]
[2022-04-07T19:35:38,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/360672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:28:54,724][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [26688] timed out after [141481ms]
[2022-04-07T19:30:11,970][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:33888}] took [1304754ms] which is above the warn threshold of [5000ms]
[2022-04-07T19:38:35,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/360724824635ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:41:42,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:45:07,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/364261256195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:46:55,276][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:49:16,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313090050034ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:52:35,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:55:22,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/322150236591ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T19:56:09,411][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [12.6m/759916ms] which is longer than the warn threshold of [300000ms]; there are currently [5] pending tasks, the oldest of which has age [13.7m/824601ms]
[2022-04-07T19:57:36,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/319896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T19:58:41,185][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [34.6m/2079313ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [33.5m/2014671ms]
[2022-04-07T20:00:56,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/319895305015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:05:04,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/420474ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:08:05,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/420333093181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:04:48,220][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.5s/14549ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.03.12-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@c50ec4db], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@49939439], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.03.26-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@21ff3fea], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@824ea792]], which exceeds the warn threshold of [10s]
[2022-04-07T20:12:32,924][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-07T20:12:33,082][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-07T20:12:33,083][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-07T20:13:16,436][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-07T20:13:16,461][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-07T20:13:16,463][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-07T20:13:26,709][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-07T20:13:26,713][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-07T20:13:26,714][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-07T20:13:26,715][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-07T20:13:26,717][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-07T20:13:26,718][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-07T20:13:26,719][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-07T20:13:26,719][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-07T20:13:26,720][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-07T20:13:26,720][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-07T20:13:26,721][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-07T20:13:26,722][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-07T20:13:26,723][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-07T20:13:26,723][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-07T20:13:26,724][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-07T20:13:26,724][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-07T20:13:26,725][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-07T20:13:26,725][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-07T20:13:26,726][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-07T20:13:26,726][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-07T20:13:26,727][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-07T20:13:26,727][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-07T20:13:26,728][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-07T20:13:26,728][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-07T20:13:26,729][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-07T20:13:26,729][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-07T20:13:26,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-07T20:13:26,730][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-07T20:13:26,731][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-07T20:13:26,732][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-07T20:13:26,732][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-07T20:13:26,733][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-07T20:13:26,733][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-07T20:13:26,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-07T20:13:26,734][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-07T20:13:26,735][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-07T20:13:26,735][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-07T20:13:26,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-07T20:13:26,736][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-07T20:13:26,737][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-07T20:13:26,738][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-07T20:13:26,738][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-07T20:13:26,739][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-07T20:13:26,740][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-07T20:13:26,740][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-07T20:13:26,740][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-07T20:13:26,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-07T20:13:26,741][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-07T20:13:26,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-07T20:13:26,742][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-07T20:13:26,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-07T20:13:26,743][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-07T20:13:26,744][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-07T20:13:26,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-07T20:13:26,745][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-07T20:13:26,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-07T20:13:26,746][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-07T20:13:26,747][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-07T20:13:26,748][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-07T20:13:26,873][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.9gb], net total_space [125.8gb], types [ext4]
[2022-04-07T20:13:26,877][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-07T20:13:27,692][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-07T20:13:44,825][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-07T20:13:44,841][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-07T20:13:44,842][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-07T20:13:44,843][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-07T20:13:44,848][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-07T20:13:44,851][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-07T20:13:44,852][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-07T20:13:44,853][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-07T20:13:44,853][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-07T20:13:44,854][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-07T20:13:44,854][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-07T20:13:44,855][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-07T20:13:44,859][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-07T20:13:44,861][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-07T20:13:44,861][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-07T20:13:46,604][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-07T20:13:46,775][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-07T20:13:48,171][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-07T20:13:49,722][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-07T20:13:49,724][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-07T20:13:49,768][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-07T20:13:49,770][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-07T20:13:50,123][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-07T20:13:53,954][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-07T20:13:54,195][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{TKnSQC7YQuGPl0h0MAYNWw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 214, version: 7913, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{TKnSQC7YQuGPl0h0MAYNWw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-07T20:13:54,499][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{TKnSQC7YQuGPl0h0MAYNWw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 214, version: 7913, reason: Publication{term=214, version=7913}
[2022-04-07T20:13:54,700][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-07T20:13:54,701][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-07T20:13:55,958][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-07T20:13:55,969][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [41] indices into cluster_state
[2022-04-07T20:13:57,861][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-07T20:13:57,862][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-07T20:13:59,281][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-07T20:13:59,684][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-07T20:14:00,441][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-07T20:14:00,452][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-07T20:14:00,455][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-07T20:14:01,279][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-07T20:14:01,918][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-07T20:14:02,409][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-07T20:14:07,966][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-07T20:14:12,044][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.03.12-000001][0]]]).
[2022-04-07T20:14:43,199][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:14:43,569][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:14:44,939][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:14:45,296][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:14:45,912][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:14:48,391][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:14:48,558][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:20:57,782][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:25:03,337][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:28:03,493][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.07/C9HYupHOQgOkWD6INDalqQ] update_mapping [_doc]
[2022-04-07T20:36:07,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [14828ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:40:52,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@333b0232, interval=1m}] took [5415ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:41:16,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9489ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:41:22,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9489217236ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:41:23,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8035ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:41:25,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8s/8035075410ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:42:45,744][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:43:17,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5450762494ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:43:38,911][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.8s/55835ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:44:51,600][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1575][45] duration [3.1s], collections [1]/[10.1s], total [3.1s]/[5.2s], memory [1.3gb]->[282mb]/[2gb], all_pools {[young] [1.1gb]->[60mb]/[0b]}{[old] [216.2mb]->[216.2mb]/[2gb]}{[survivor] [7.7mb]->[9.7mb]/[0b]}
[2022-04-07T20:45:15,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.8s/55835700513ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:45:45,117][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1575] overhead, spent [3.1s] collecting in the last [10.1s]
[2022-04-07T20:46:23,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/169538ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:46:32,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [326840ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:46:38,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/169537814906ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:46:49,434][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26470ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:46:53,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e059520, interval=5s}] took [26470ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:47:04,933][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.4s/26470015170ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:47:25,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:47:41,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.4s/35417860266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:47:54,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29802ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:48:17,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29801548428ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:48:37,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42406ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:49:00,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.4s/42406634716ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:48:00,408][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37294}] took [169537ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:48:00,408][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37288}] took [169537ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:49:18,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.8s/40854ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:49:30,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.8s/40853172685ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:50:22,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:51:07,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64769226276ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:49:24,118][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_search?ignore_unavailable=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:36140}] took [42406ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:51:17,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.6s/54669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:51:19,174][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e8b18@78334d38] took [54669ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:51:27,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.6s/54669695024ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:51:40,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23631ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:51:42,518][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:37298}] took [23630ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:51:55,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23630149227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:52:05,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24824ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:51:56,682][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23630ms] which is above the warn threshold of [5s]
[2022-04-07T20:52:21,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24824153656ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:52:29,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23930ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:52:33,143][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1576][46] duration [4.9s], collections [1]/[11.4m], total [4.9s]/[10.1s], memory [282mb]->[260.6mb]/[2gb], all_pools {[young] [60mb]->[32mb]/[0b]}{[old] [216.2mb]->[218mb]/[2gb]}{[survivor] [9.7mb]->[10.6mb]/[0b]}
[2022-04-07T20:52:33,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23930646760ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:52:45,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [40415ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:52:46,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16485ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:52:52,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16485076474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:53:04,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13394ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:53:04,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@2d196658] took [13393ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:53:15,544][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13393138482ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:53:20,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20705ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:02,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20705369304ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:06,462][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:38020}] took [20705ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:54:09,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49026ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:17,604][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1577][47] duration [21.5s], collections [1]/[1.7m], total [21.5s]/[31.6s], memory [260.6mb]->[232mb]/[2gb], all_pools {[young] [32mb]->[4mb]/[0b]}{[old] [218mb]->[222.2mb]/[2gb]}{[survivor] [10.6mb]->[5.8mb]/[0b]}
[2022-04-07T20:54:17,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49025599489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:31,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [49025ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:54:32,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:36,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.2s/23246635464ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:42,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10405ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:46,392][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10404478877ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:49,311][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:49,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e8b18@75bd2809] took [6712ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:54:52,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6712927723ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:55:00,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10992ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:55:08,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10992796442ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:54:53,007][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6713ms] which is above the warn threshold of [5s]
[2022-04-07T20:55:15,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:55:21,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [14863ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:55:26,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.8s/14863598763ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:55:30,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14721ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:55:30,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@333b0232, interval=1m}] took [14721ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:55:36,755][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14721072381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:55:41,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11173ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:55:44,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11172763519ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:55:49,245][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:55:50,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5108/0x00000008017e8b18@2d4104d6] took [7941ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:55:52,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7941103492ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:19,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:20,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e059520, interval=5s}] took [7503ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:56:23,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7503717998ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:26,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:30,677][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1581][48] duration [16.2s], collections [1]/[39.6s], total [16.2s]/[47.8s], memory [304mb]->[228.6mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [222.2mb]->[222.2mb]/[2gb]}{[survivor] [5.8mb]->[2.4mb]/[0b]}
[2022-04-07T20:56:30,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.5s/29566684419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:35,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9118ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:34,858][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1581] overhead, spent [16.2s] collecting in the last [39.6s]
[2022-04-07T20:56:38,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9117583672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:39,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [38684ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:56:42,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7916ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:44,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3d6d8dff, interval=30s}] took [7915ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:56:49,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7915946727ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:52,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9772ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:56:58,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9771905508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:02,905][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9924ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:08,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9923921456ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:10,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [9923ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:57:13,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10629ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:17,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10629482625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:21,678][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:29,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8160450868ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:35,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13839ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:41,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13839120005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:41,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [13839ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:57:47,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:51,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11468512582ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:56,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [9277ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:57:56,198][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:57:59,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9277956510ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:02,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6139ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:05,155][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [6139ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:58:06,613][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6139111722ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:10,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7839ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:14,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e059520, interval=5s}] took [7839ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:58:14,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7839026403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:19,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9224ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:20,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [9223ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:58:22,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9223209107ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:24,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:24,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@2d196658] took [5235ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:58:30,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5235476908ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:35,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:39,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [10234ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:58:39,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10234521104ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:42,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:44,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7431160194ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T20:58:44,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e059520, interval=5s}] took [7431ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:58:47,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [5462ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:59:32,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [9242ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:59:50,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e059520, interval=5s}] took [5804ms] which is above the warn threshold of [5000ms]
[2022-04-07T20:59:27,226][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [12602] timed out after [37715ms]
[2022-04-07T21:00:01,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [5507ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:00:06,038][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9311ms] which is above the warn threshold of [5s]
[2022-04-07T21:01:31,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [10924ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:01:49,404][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [3.6m/220948ms] ago, timed out [3m/183233ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{TKnSQC7YQuGPl0h0MAYNWw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [12602]
[2022-04-07T21:02:06,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [5518ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:02:50,653][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38134}] took [13794ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:03:03,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [6481ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:03:17,396][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38134}] took [5392ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:03:18,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [9537ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:03:39,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e059520, interval=5s}] took [7605ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:04:52,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [18188ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:05:49,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:06:10,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34481031668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:06:14,172][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1647][49] duration [23.5s], collections [1]/[1.4m], total [23.5s]/[1.1m], memory [272.6mb]->[232.2mb]/[2gb], all_pools {[young] [48mb]->[8mb]/[0b]}{[old] [222.2mb]->[222.2mb]/[2gb]}{[survivor] [2.4mb]->[6mb]/[0b]}
[2022-04-07T21:06:18,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36888ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:06:22,166][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1647] overhead, spent [23.5s] collecting in the last [1.4m]
[2022-04-07T21:06:22,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.8s/36887746483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:06:25,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [36887ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:06:26,065][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8104ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:06:32,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8104729427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:06:40,845][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14711ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:06:46,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14710218047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:06:53,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:07:03,875][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12318861479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:07:17,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:07:20,145][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [23887ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:07:27,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23887230312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:07:33,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16482ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:07:35,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e059520, interval=5s}] took [16482ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:07:21,121][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [23887ms] which is above the warn threshold of [5s]
[2022-04-07T21:07:47,940][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16482669654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:07:55,452][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21831ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:08:03,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21830989557ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:08:10,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:08:18,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14982778643ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:08:22,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [14982ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:08:25,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:09:45,919][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15352518547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:09:53,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:09:53,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@333b0232, interval=1m}] took [86613ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:10:01,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/86613589918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:10:13,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21044ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:10:23,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21043330553ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:10:36,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22443ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:10:42,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22443841453ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:10:49,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:10:54,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13225164236ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:10:42,155][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13050] timed out after [137993ms]
[2022-04-07T21:10:56,603][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1651][50] duration [59.3s], collections [1]/[2.4m], total [59.3s]/[2.1m], memory [268.2mb]->[298.1mb]/[2gb], all_pools {[young] [48mb]->[72mb]/[0b]}{[old] [222.2mb]->[222.2mb]/[2gb]}{[survivor] [6mb]->[7.9mb]/[0b]}
[2022-04-07T21:11:00,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11196ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:10:46,153][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.6m/160437ms] ago, timed out [22.4s/22444ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{TKnSQC7YQuGPl0h0MAYNWw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13050]
[2022-04-07T21:11:03,922][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1651] overhead, spent [59.3s] collecting in the last [2.4m]
[2022-04-07T21:11:03,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11196051052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:11:11,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [46865ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:11:13,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12632ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:11:20,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12632659502ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:12:53,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/100794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:12:58,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/100793703988ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:13:02,984][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1652][51] duration [1.1m], collections [1]/[2.4m], total [1.1m]/[3.2m], memory [298.1mb]->[236.5mb]/[2gb], all_pools {[young] [72mb]->[12mb]/[0b]}{[old] [222.2mb]->[222.2mb]/[2gb]}{[survivor] [7.9mb]->[6.3mb]/[0b]}
[2022-04-07T21:13:06,488][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:13:10,446][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1652] overhead, spent [1.1m] collecting in the last [2.4m]
[2022-04-07T21:13:15,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12678053909ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:13:17,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [12678ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:13:23,054][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16617ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:13:32,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16616464045ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:13:40,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17705ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:13:47,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17705444592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:13:52,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11699ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:14:01,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11699394308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:14:12,543][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [18663ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:14:11,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18664ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:14:22,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18663292668ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:14:38,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27105ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:14:46,355][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.1s/27105627622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:15:13,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35129ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:15:18,414][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.8m/110302ms] ago, timed out [35.1s/35129ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{TKnSQC7YQuGPl0h0MAYNWw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [13135]
[2022-04-07T21:15:45,880][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35128311920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:15:19,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [35128ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:15:50,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37442ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:16:01,502][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37442143401ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:15:46,346][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [13135] timed out after [75173ms]
[2022-04-07T21:16:12,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20861ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:16:12,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@e059520, interval=5s}] took [20861ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:16:19,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20861559007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:16:26,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14491ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:16:28,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [14490ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:16:36,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.4s/14490622971ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:16:44,144][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17929ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:16:53,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17929182239ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:16:50,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [17929ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:16:59,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15362ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:17:11,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15361804136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:17:18,042][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:17:19,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [18571ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:17:23,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18571518932ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:17:29,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T21:17:32,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [11324ms] which is above the warn threshold of [5000ms]
[2022-04-07T21:17:36,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11324149693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T21:17:41,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:11:54,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12031425570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:23:56,508][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][1659][52] duration [47m], collections [1]/[13.9s], total [47m]/[50.3m], memory [296.5mb]->[312.5mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [222.2mb]->[222.2mb]/[2gb]}{[survivor] [6.3mb]->[8.4mb]/[0b]}
[2022-04-07T22:21:38,719][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:36162}] took [3639938ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:28:21,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1h/3640312ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:29:40,832][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][1659] overhead, spent [47m] collecting in the last [13.9s]
[2022-04-07T22:32:46,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@1243b195, interval=1s}] took [3651969ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:45:24,391][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-07T22:45:24,419][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-07T22:45:24,422][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-07T22:45:34,190][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-07T22:45:34,194][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-07T22:45:34,195][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-07T22:45:34,195][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-07T22:45:34,196][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-07T22:45:34,196][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-07T22:45:34,197][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-07T22:45:34,197][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-07T22:45:34,197][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-07T22:45:34,198][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-07T22:45:34,198][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-07T22:45:34,199][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-07T22:45:34,199][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-07T22:45:34,200][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-07T22:45:34,200][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-07T22:45:34,201][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-07T22:45:34,201][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-07T22:45:34,201][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-07T22:45:34,202][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-07T22:45:34,202][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-07T22:45:34,202][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-07T22:45:34,203][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-07T22:45:34,203][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-07T22:45:34,204][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-07T22:45:34,204][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-07T22:45:34,204][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-07T22:45:34,205][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-07T22:45:34,205][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-07T22:45:34,205][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-07T22:45:34,206][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-07T22:45:34,206][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-07T22:45:34,206][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-07T22:45:34,207][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-07T22:45:34,207][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-07T22:45:34,207][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-07T22:45:34,208][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-07T22:45:34,208][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-07T22:45:34,208][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-07T22:45:34,209][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-07T22:45:34,209][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-07T22:45:34,209][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-07T22:45:34,210][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-07T22:45:34,210][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-07T22:45:34,210][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-07T22:45:34,210][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-07T22:45:34,211][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-07T22:45:34,211][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-07T22:45:34,211][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-07T22:45:34,212][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-07T22:45:34,212][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-07T22:45:34,212][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-07T22:45:34,213][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-07T22:45:34,213][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-07T22:45:34,213][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-07T22:45:34,214][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-07T22:45:34,214][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-07T22:45:34,214][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-07T22:45:34,215][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-07T22:45:34,216][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-07T22:45:34,292][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.1gb], net total_space [125.8gb], types [ext4]
[2022-04-07T22:45:34,293][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-07T22:45:34,724][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-07T22:47:08,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76734ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:47:13,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77098163273ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:47:13,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:47:13,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13702291820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:47:31,486][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-07T22:47:31,493][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-07T22:47:31,496][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-07T22:47:31,497][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-07T22:47:31,498][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-07T22:47:31,500][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-07T22:47:31,501][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-07T22:47:31,502][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-07T22:47:31,502][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-07T22:47:31,503][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-07T22:47:31,504][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-07T22:47:31,505][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-07T22:47:31,506][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-07T22:47:31,507][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-07T22:47:31,508][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-07T22:48:13,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:48:18,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8305634929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:48:14,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fc046bf, interval=5s}] took [7823ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:48:21,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7816ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:48:28,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7815818088ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:48:28,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7938ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:48:28,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7937990003ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:48:33,858][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-07T22:48:34,039][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-07T22:48:38,401][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-07T22:48:50,437][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-07T22:48:50,442][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-07T22:48:50,571][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-07T22:48:50,575][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-07T22:48:52,789][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-07T22:50:18,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [43330ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:50:51,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9881ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:51:15,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9881333340ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:51:30,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.4s/47404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:51:40,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.4s/47403451281ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:51:51,173][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21364ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:51:56,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3ce19bef, interval=1m}] took [21363ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:52:02,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21363778617ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:52:11,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19591ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:52:21,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19591321264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:52:33,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21499ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:52:43,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.4s/21498762983ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:52:51,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19526ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:52:56,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19526817813ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:53:03,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:53:09,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11092618954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:53:15,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:53:16,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@1830759] took [52118ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:53:21,748][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12932294456ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:53:31,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14764ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:53:37,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14763915799ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:53:41,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11644ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:53:41,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [14763ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:53:42,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11644093735ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:54:21,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:55:07,293][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-07T22:55:09,239][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{sFaN2ZsuSZKRM0Pk1HkrDw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 216, version: 7975, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{sFaN2ZsuSZKRM0Pk1HkrDw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-07T22:55:13,894][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{sFaN2ZsuSZKRM0Pk1HkrDw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 216, version: 7975, reason: Publication{term=216, version=7975}
[2022-04-07T22:56:08,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6959ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:56:11,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [8560ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:56:12,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6959582413ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:56:14,702][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fc046bf, interval=5s}] took [7076ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:56:14,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7076ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:56:16,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7076049618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:56:21,635][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:56:23,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6983611380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:56:23,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [6983ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:56:21,790][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7076ms] which is above the warn threshold of [5s]
[2022-04-07T22:56:55,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [6708ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:57:12,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [9771ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:57:12,521][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.8s] publication of cluster state version [7976] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{sFaN2ZsuSZKRM0Pk1HkrDw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-07T22:57:33,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [7722ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:57:31,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5722ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:57:34,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5721438739ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:57:36,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:57:39,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5742041500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:57:42,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:57:43,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [5810ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:57:47,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5810521501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:57:50,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:57:50,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39f83369, interval=5s}] took [8387ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:57:53,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8387810930ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:57:56,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:58:00,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6014441683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:58:01,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [6014ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:58:01,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5324ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T22:58:03,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5324653087ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T22:58:09,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [7318ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:58:15,221][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-07T22:58:19,245][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-07T22:58:22,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [5259ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:58:38,118][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [6003ms] which is above the warn threshold of [5000ms]
[2022-04-07T22:59:17,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [10883ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:00:10,047][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [33726ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:00:54,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@67b966a4] took [24361ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:01:13,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5496ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:01:13,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@1830759] took [7421ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:01:22,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5495299748ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:01:31,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:01:44,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17635401090ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:01:52,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:01:58,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [39304ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:02:01,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.6s/21669166300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:02:16,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39f83369, interval=5s}] took [24190ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:02:16,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:02:03,543][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=216, version=7976}] took [59.5s] which is above the warn threshold of [30s]: [running task [Publication{term=216, version=7976}]] took [84ms], [connecting to new nodes] took [18687ms], [org.elasticsearch.repositories.RepositoriesService@376ca3ba] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@5b6a5538] took [125ms], [org.elasticsearch.script.ScriptService@1e3a34be] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@127f9132] took [0ms], [org.elasticsearch.snapshots.RestoreService@405c81b4] took [0ms], [org.elasticsearch.ingest.IngestService@4916cf12] took [0ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6c69d4a] took [135ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ca860@685e6aa1] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@4c2b173f] took [0ms], [org.elasticsearch.tasks.TaskManager@33ba2ff7] took [255ms], [org.elasticsearch.snapshots.SnapshotsService@1a4d807] took [6249ms], [org.elasticsearch.cluster.InternalClusterInfoService@333601d1] took [201ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@65a0350e] took [604ms], [org.elasticsearch.indices.SystemIndexManager@25ae4a63] took [330ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@96f2b5b] took [82ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@7d564266] took [1731ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32f5c331] took [829ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6580d1b3] took [67ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@67f4a055] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@271cec45] took [325ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@f4242a2] took [168ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@42171492] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@581c5116] took [160ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@22557d03] took [0ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@604c544f] took [300ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@170f4026] took [391ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2da13730] took [559ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@127f9132] took [720ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@2495eae2] took [155ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@26793c13] took [772ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@64efc592] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@22259c00] took [290ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@256fd2af] took [279ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7ce468f5] took [413ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@722f3ef5] took [322ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@298e60b5] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@21cfeec0] took [497ms], [org.elasticsearch.node.ResponseCollectorService@7e3389cc] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@75e0c869] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2f3185a8] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@71d6d760] took [521ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@648c65f6] took [267ms], [org.elasticsearch.indices.store.IndicesStore@57aaaaa] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@60d3fb44] took [417ms], [org.elasticsearch.license.LicenseService@11dd601e] took [483ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@16f90406] took [910ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4606d389] took [184ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@30a53593] took [334ms], [org.elasticsearch.gateway.GatewayService@591a1ea0] took [441ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@4523e052] took [66ms]
[2022-04-07T23:02:29,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24190142602ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:02:37,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21358ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:02:44,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.3s/21357701232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:02:43,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@3af650dc] took [21357ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:02:49,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:02:44,876][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [21358ms] which is above the warn threshold of [5s]
[2022-04-07T23:02:54,536][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12067319506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:02:58,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [12067ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:03:01,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:03:09,683][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11757618215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:03:19,715][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16923ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:03:22,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fc046bf, interval=5s}] took [16923ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:03:29,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.9s/16923127272ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:03:38,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20123ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:03:47,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20122772312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:03:43,956][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.2m/374212ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [7.7m/463826ms]
[2022-04-07T23:03:52,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14307ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:03:52,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [20122ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:04:00,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.3s/14306907223ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:04:10,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17306ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:04:18,628][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.3s/17305920816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:04:18,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@6d61af85] took [17305ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:04:23,671][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13712ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:04:26,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fc046bf, interval=5s}] took [13712ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:04:31,205][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13712116197ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:04:36,369][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12378ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:04:43,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [12378ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:04:44,189][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12378503528ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:04:51,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@1830759] took [14516ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:04:50,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:04:56,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14516281535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:05:10,509][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19657ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:05:17,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19657287286ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:05:20,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [19657ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:05:23,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:05:29,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12998790295ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:05:43,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:05:45,173][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20165ms] which is above the warn threshold of [5s]
[2022-04-07T23:05:45,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@157aaa3b] took [20165ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:05:52,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20165436808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:05:54,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11380ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:05:57,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [11380ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:05:57,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11380081395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:02,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39f83369, interval=5s}] took [6480ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:06:01,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6480ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:04,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6480072080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:07,476][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5911ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:11,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5910411287ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:13,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [5910ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:06:15,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8856ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:19,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8856396708ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:23,431][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7260ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:27,703][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7259974592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:29,521][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [7259ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:06:33,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:38,107][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10093391672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:44,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10972ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:44,325][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fc046bf, interval=5s}] took [10971ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:06:49,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10971941261ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:06:57,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12416ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:07,508][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.4s/12415836367ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:08,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [12415ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:07:14,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17727ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:20,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.7s/17726858163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:27,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12280ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:32,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12279502565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:32,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [12279ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:07:36,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9163ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:38,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3ce19bef, interval=1m}] took [9163ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:07:42,563][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9163228817ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:48,252][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12102ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:49,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39f83369, interval=5s}] took [12102ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:07:53,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12102206296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:07:58,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:08:05,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11030496778ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:08:08,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@395a7276] took [11030ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:08:10,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11788ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:08:17,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11788372491ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:08:27,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:08:29,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fc046bf, interval=5s}] took [16257ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:08:37,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16257209225ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:08:48,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:08:55,929][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [20140ms] which is above the warn threshold of [5s]
[2022-04-07T23:08:58,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20140309854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:09:04,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:09:05,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [37414ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:09:10,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17273852541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:09:17,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:09:21,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12780171693ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:09:29,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@31a2b531] took [12780ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:09:29,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12646ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:09:32,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12645139365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:09:42,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:09:45,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [10999ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:09:51,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10999020483ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:09:57,783][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39f83369, interval=5s}] took [15678ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:09:57,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15678ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:02,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15678731397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:08,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11278ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:14,675][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11277202760ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:16,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [11277ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:10:23,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14532ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:30,036][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14532169569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:35,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13065ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:39,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13s/13065569387ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:39,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@515f3907] took [13065ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:10:43,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3ce19bef, interval=1m}] took [8303ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:10:43,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8304ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:47,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8303733015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:52,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7949ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:10:58,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [7948ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:10:57,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7948912806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:02,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10647ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:05,841][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fc046bf, interval=5s}] took [10647ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:11:10,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10647055838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:19,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:24,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [16851ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:11:27,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16851558563ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:32,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12827ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:32,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@1830759] took [12827ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:11:38,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12827868535ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:38,951][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [7.6m/457054ms] to compute cluster state update for [cluster_reroute(post-join reroute)], which exceeds the warn threshold of [10s]
[2022-04-07T23:11:42,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10503ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:43,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39f83369, interval=5s}] took [10502ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:11:47,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10502161072ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:47,172][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10503ms] which is above the warn threshold of [5s]
[2022-04-07T23:11:49,793][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7199ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:52,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7199620229ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:11:54,031][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@11346a65] took [7199ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:11:59,099][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9040ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:12:03,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9039794359ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:12:09,552][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [26.1s/26163ms] to notify listeners on unchanged cluster state for [cluster_reroute(post-join reroute)], which exceeds the warn threshold of [10s]
[2022-04-07T23:12:10,918][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3ce19bef, interval=1m}] took [11505ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:12:10,286][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11506ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:12:13,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11505738530ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:12:13,830][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [15.5m/934992ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [16.9m/1019314ms]
[2022-04-07T23:12:32,653][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.checkGlobalBlock(TransportSingleShardAction.java:112) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.<init>(TransportSingleShardAction.java:146) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.<init>(TransportSingleShardAction.java:130) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:98) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:51) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:512) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.document.RestGetAction.lambda$prepareRequest$0(RestGetAction.java:91) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-07T23:12:35,479][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-07T23:12:41,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@6e3b2cc4] took [6186ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:13:13,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [11827ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:13:25,489][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=216, version=7977}] took [59.7s] which is above the warn threshold of [30s]: [running task [Publication{term=216, version=7977}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@376ca3ba] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@5b6a5538] took [43ms], [org.elasticsearch.script.ScriptService@1e3a34be] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@127f9132] took [5030ms], [org.elasticsearch.snapshots.RestoreService@405c81b4] took [0ms], [org.elasticsearch.ingest.IngestService@4916cf12] took [7339ms], [org.elasticsearch.action.ingest.IngestActionForwarder@6c69d4a] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4527/0x00000008016ca860@685e6aa1] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@4c2b173f] took [34ms], [org.elasticsearch.tasks.TaskManager@33ba2ff7] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@1a4d807] took [1ms], [org.elasticsearch.cluster.InternalClusterInfoService@333601d1] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@65a0350e] took [10ms], [org.elasticsearch.indices.SystemIndexManager@25ae4a63] took [130ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@96f2b5b] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013014e8@7d564266] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@32f5c331] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@6580d1b3] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x0000000801402000@67f4a055] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@271cec45] took [1ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@f4242a2] took [389ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@42171492] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@581c5116] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@22557d03] took [13ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@604c544f] took [10ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@170f4026] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@2da13730] took [22ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@127f9132] took [44ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@2495eae2] took [3ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@26793c13] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@64efc592] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@22259c00] took [6ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@256fd2af] took [6ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@7ce468f5] took [1ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@722f3ef5] took [31ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@298e60b5] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@21cfeec0] took [1ms], [org.elasticsearch.node.ResponseCollectorService@7e3389cc] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@75e0c869] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@2f3185a8] took [26ms], [org.elasticsearch.shutdown.PluginShutdownService@71d6d760] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@648c65f6] took [1ms], [org.elasticsearch.indices.store.IndicesStore@57aaaaa] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@60d3fb44] took [7ms], [org.elasticsearch.license.LicenseService@11dd601e] took [1064ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@16f90406] took [1671ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@4606d389] took [95ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@30a53593] took [671ms], [org.elasticsearch.gateway.GatewayService@591a1ea0] took [16ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@4523e052] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$2@78ffa3]]]] took [42500ms]
[2022-04-07T23:13:48,108][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [41] indices into cluster_state
[2022-04-07T23:15:23,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@5d269bd5] took [119504ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:15:37,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [10006ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:15:59,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [6203ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:16:03,224][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9406ms] which is above the warn threshold of [5s]
[2022-04-07T23:16:54,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7531ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:17:06,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7531222584ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:17:17,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24195ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:17:29,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24195091357ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:17:37,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21834ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:16:13,587][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_update/usage-counters%3AeventLoop%3A07042022%3Acount%3Adelay_threshold_exceeded, params: {require_alias=true, refresh=wait_for, index=.kibana_7.17.0, _source=true, id=usage-counters:eventLoop:07042022:count:delay_threshold_exceeded}
org.elasticsearch.action.UnavailableShardsException: [.kibana_7.17.0_001][0] [1] shardIt, [0] active : Timeout waiting for [1m], request: indices:data/write/update
	at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction.retry(TransportInstanceSingleOperationAction.java:231) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction.doStart(TransportInstanceSingleOperationAction.java:181) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction$2.onTimeout(TransportInstanceSingleOperationAction.java:254) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:345) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:263) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:660) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-07T23:17:45,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21833544625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:17:51,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:17:57,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13911883627ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:18:09,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:18:21,441][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.9s/17974866526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:03,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:09,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.8s/52866220541ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:13,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:16,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11072910461ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:20,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6698ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:18,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@352dae2] took [190114ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:19:24,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6698588846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:30,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10228ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:30,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [10227ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:19:35,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10227250355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:42,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11796ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:46,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [11796ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:19:48,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11796593752ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:46,527][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38862}] took [22024ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:19:53,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:19:57,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [10866ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:19:58,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10866097394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:20:06,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12958ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:20:12,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.9s/12957906992ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:20:28,180][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38862}] took [31085ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:20:24,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:20:36,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.1s/18127269325ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:20:43,092][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19061ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:20:48,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19s/19060167164ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:20:54,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11062ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:21:02,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11062062298ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:21:10,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:21:16,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15288407447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:21:22,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12899ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:21:30,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12899139235ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:21:40,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17637ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:21:48,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.6s/17636551791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:21:56,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16230ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:22:02,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16230426420ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:22:10,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14078ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:22:17,719][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14077942610ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:22:28,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:22:37,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.5s/17504114751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:22:44,211][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16006ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:22:51,348][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16005384356ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:23:00,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16022ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:23:09,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16021963860ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:23:16,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16155ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:23:24,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16155282575ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:23:32,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16295ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:23:40,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.2s/16295208015ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:23:48,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15695ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:23:58,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15694975256ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:24:07,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18918ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:24:11,761][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@64d9807a] took [253934ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:24:16,829][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18918036346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:24:24,574][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39f83369, interval=5s}] took [16873ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:24:24,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16873ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:24:36,553][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16873165076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:24:46,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [22422ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:24:46,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22423ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:24:55,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.4s/22422369377ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:25:04,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17264ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:25:05,105][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@39f83369, interval=5s}] took [17264ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:25:11,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17264674815ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:25:18,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14725ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:25:18,320][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14725ms] which is above the warn threshold of [5s]
[2022-04-07T23:25:24,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14724930721ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:25:34,711][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15333ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:25:44,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15333226767ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:25:50,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:25:54,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.7s/16768833883ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:25:59,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:26:03,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9084954289ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:26:05,732][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6473ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:26:09,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6473177116ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:26:24,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18960ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:26:41,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.9s/18959893773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:26:48,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23740ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:26:52,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.7s/23739819571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:27:04,307][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.1m/791087ms] to compute cluster state update for [cluster_reroute(async_shard_fetch)], which exceeds the warn threshold of [10s]
[2022-04-07T23:27:04,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:27:09,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15584301700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:27:14,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10170ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:27:21,295][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10170000025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:27:25,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:27:30,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10671973222ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:27:39,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:27:50,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14189969479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:28:09,006][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29682ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:28:03,744][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38882}] took [14190ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:28:38,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29681914686ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:29:04,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.3s/55357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:29:13,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.3s/55356278167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:29:23,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:29:21,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@3aa20ca9] took [240739ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:29:35,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19409870253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:29:44,600][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:29:51,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [20772ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:29:51,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20772684698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:29:59,414][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14535ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:30:03,855][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fc046bf, interval=5s}] took [14534ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:30:07,252][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:38882}] took [54717ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:30:12,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14534936233ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:30:29,305][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@3ce19bef, interval=1m}] took [29426ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:30:28,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29426ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:30:52,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.4s/29426050571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:31:32,908][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:32:17,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61292876029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:32:48,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77220ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:33:14,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/77219797598ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:32:40,127][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [61293ms] which is above the warn threshold of [5s]
[2022-04-07T23:34:10,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.3s/49397ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:34:28,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.3s/49397339903ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:34:46,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69163ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:35:46,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69162533699ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:36:01,962][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74476ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:36:15,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/74476669527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:36:29,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:36:46,435][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28701219254ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:37:07,740][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37457ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:37:26,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.4s/37457838753ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:38:02,611][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:38:35,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53736188918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:39:11,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63587ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:39:45,082][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63587881175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:40:09,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:40:24,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65150904709ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:40:40,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32082ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:40:51,280][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32082079442ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:41:11,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28681ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:41:37,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28680557733ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:42:02,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.5s/51544ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:42:17,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.5s/51544446714ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:42:28,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26731ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:42:39,754][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26730710102ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:42:51,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22640ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:43:24,522][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.6s/22640201507ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:43:43,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.1s/52176ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:43:53,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.1s/52175382567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:44:04,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21928ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:44:17,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21928288989ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:44:28,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22995ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:44:38,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.9s/22995459968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:44:53,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24964ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:45:12,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.9s/24963391931ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:45:24,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32376ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:45:42,330][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5117/0x00000008017ecd70@740cb19] took [896300ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:45:35,801][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.3s/32376527058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:45:56,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30812ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:46:10,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30811208035ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:46:24,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28651ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:46:31,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@b2e19e9, interval=1s}] took [59463ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:46:37,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.6s/28651908583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:46:50,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5fc046bf, interval=5s}] took [23492ms] which is above the warn threshold of [5000ms]
[2022-04-07T23:46:48,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23493ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:47:03,326][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23492424727ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:47:19,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:47:41,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.9s/29999363615ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:47:56,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:48:10,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.1s/38140148655ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:48:22,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26246ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:48:37,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.2s/26245837062ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:48:49,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26786ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:48:59,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26785917784ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:49:14,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:49:35,436][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25274431142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:50:45,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89635ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:51:16,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.4m/89635672480ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:51:37,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52s/52008ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:52:00,840][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52s/52007491500ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:52:55,983][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79742ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:53:18,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/79742170439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:54:15,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:54:38,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/78857730136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:54:59,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43769ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:55:26,624][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.7s/43769676533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:55:50,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50443ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:56:17,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.4s/50443097057ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:56:55,587][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:57:17,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/65070522130ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:57:56,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-07T23:58:16,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/62234717567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-07T23:59:13,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/76580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-08T02:18:10,847][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-08T02:18:10,933][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-08T02:18:10,935][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
