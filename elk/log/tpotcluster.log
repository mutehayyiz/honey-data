[2022-04-11T15:52:44,135][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-11T15:52:44,172][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-11T15:52:44,173][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-11T15:52:50,304][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-11T15:52:50,305][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-11T15:52:50,306][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-11T15:52:50,306][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-11T15:52:50,306][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-11T15:52:50,307][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-11T15:52:50,307][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-11T15:52:50,308][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-11T15:52:50,308][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-11T15:52:50,308][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-11T15:52:50,309][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-11T15:52:50,309][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-11T15:52:50,310][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-11T15:52:50,310][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-11T15:52:50,311][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-11T15:52:50,311][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-11T15:52:50,311][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-11T15:52:50,312][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-11T15:52:50,312][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-11T15:52:50,313][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-11T15:52:50,313][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-11T15:52:50,313][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-11T15:52:50,314][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-11T15:52:50,314][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-11T15:52:50,315][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-11T15:52:50,315][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-11T15:52:50,316][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-11T15:52:50,316][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-11T15:52:50,317][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-11T15:52:50,317][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-11T15:52:50,317][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-11T15:52:50,318][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-11T15:52:50,318][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-11T15:52:50,319][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-11T15:52:50,319][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-11T15:52:50,320][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-11T15:52:50,320][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-11T15:52:50,320][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-11T15:52:50,321][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-11T15:52:50,321][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-11T15:52:50,322][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-11T15:52:50,322][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-11T15:52:50,323][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-11T15:52:50,323][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-11T15:52:50,324][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-11T15:52:50,324][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-11T15:52:50,324][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-11T15:52:50,325][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-11T15:52:50,325][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-11T15:52:50,326][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-11T15:52:50,326][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-11T15:52:50,327][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-11T15:52:50,327][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-11T15:52:50,327][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-11T15:52:50,328][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-11T15:52:50,328][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-11T15:52:50,329][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-11T15:52:50,329][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-11T15:52:50,330][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-11T15:52:50,397][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [106.7gb], net total_space [125.8gb], types [ext4]
[2022-04-11T15:52:50,398][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-11T15:52:50,648][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-11T15:53:02,147][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-11T15:53:02,151][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-11T15:53:03,318][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-11T15:53:03,448][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-11T15:53:04,204][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-11T15:53:04,920][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-11T15:53:04,921][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-11T15:53:04,954][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-11T15:53:04,956][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-11T15:53:05,181][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-11T15:53:07,668][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-11T15:53:07,776][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{kZcfg0TbRnKG2C5trZnzkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 229, version: 8817, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{kZcfg0TbRnKG2C5trZnzkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-11T15:53:07,962][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{kZcfg0TbRnKG2C5trZnzkA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 229, version: 8817, reason: Publication{term=229, version=8817}
[2022-04-11T15:53:08,091][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-11T15:53:08,092][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-11T15:53:08,886][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-11T15:53:08,892][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [45] indices into cluster_state
[2022-04-11T15:53:09,593][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-11T15:53:09,594][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-11T15:53:10,238][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-ASN.mmdb] is up to date, updated timestamp
[2022-04-11T15:53:10,389][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-City.mmdb] is up to date, updated timestamp
[2022-04-11T15:53:10,762][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] geoip database [GeoLite2-Country.mmdb] is up to date, updated timestamp
[2022-04-11T15:53:10,829][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-11T15:53:10,835][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-11T15:53:10,839][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-11T15:53:11,294][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T15:53:11,427][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T15:53:14,200][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-11T15:53:23,440][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.10][0]]]).
[2022-04-11T15:53:29,522][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-11T15:53:29,738][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-11T15:54:13,342][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 640 finished with response BulkByScrollResponse[took=357.7ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-11T15:54:15,013][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 666 finished with response BulkByScrollResponse[took=1.6s,timed_out=false,sliceId=null,updated=1016,created=0,deleted=0,batches=2,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-11T15:54:24,222][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-11T15:55:05,723][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.11] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-11T15:55:05,863][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.11][0]]]).
[2022-04-11T15:55:06,032][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,148][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,180][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,199][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,346][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,353][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,442][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,614][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,633][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,793][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:06,890][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:07,042][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,355][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,423][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,432][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,440][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:08,567][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:10,361][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:14,384][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:14,474][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:55:50,421][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:07,250][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:09,477][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:09,611][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:11,483][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:14,464][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:20,507][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:20,579][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:56:34,486][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:57:22,573][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:57:31,658][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:57:32,371][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:57:46,685][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T15:59:08,694][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:01:54,862][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:03:08,750][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] to [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:08,805][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] to [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:08,863][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [.kibana-event-log-7.16.2-000002] creating index, cause [rollover_index], templates [.kibana-event-log-7.16.2-template], shards [1]/[1]
[2022-04-11T16:03:08,867][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] updating number_of_replicas to [0] for indices [.kibana-event-log-7.16.2-000002]
[2022-04-11T16:03:08,974][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [.ds-ilm-history-5-2022.04.11-000002] creating index, cause [rollover_data_stream], templates [ilm-history], shards [1]/[0]
[2022-04-11T16:03:09,099][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.ds-ilm-history-5-2022.04.11-000002][0]]]).
[2022-04-11T16:03:09,134][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000002] from [null] to [{"phase":"new","action":"complete","name":"complete"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,136][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] to [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,137][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.04.11-000002] from [null] to [{"phase":"new","action":"complete","name":"complete"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,138][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"attempt-rollover"}] to [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,189][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.04.11-000002] from [{"phase":"new","action":"complete","name":"complete"}] to [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,191][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000002] from [{"phase":"new","action":"complete","name":"complete"}] to [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,195][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] to [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,197][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] to [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,198][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"wait-for-active-shards"}] to [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,200][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"update-rollover-lifecycle-date"}] to [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,386][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.04.11-000002] from [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] to [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,387][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000002] from [{"phase":"hot","action":"unfollow","name":"branch-check-unfollow-prerequisites"}] to [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}] in policy [kibana-event-log-policy]
[2022-04-11T16:03:09,388][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.ds-ilm-history-5-2022.03.12-000001] from [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] to [{"phase":"hot","action":"complete","name":"complete"}] in policy [ilm-history-ilm-policy]
[2022-04-11T16:03:09,389][INFO ][o.e.x.i.IndexLifecycleTransition] [tpotcluster-node-01] moving index [.kibana-event-log-7.16.2-000001] from [{"phase":"hot","action":"rollover","name":"set-indexing-complete"}] to [{"phase":"hot","action":"complete","name":"complete"}] in policy [kibana-event-log-policy]
[2022-04-11T16:04:29,673][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:04:29,999][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:04:30,085][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:08:34,780][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:08:35,054][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:10:07,043][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:10:07,115][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:10:08,045][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:12:23,402][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:18:35,730][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:18:35,815][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:21:19,330][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:31:50,554][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:36:57,756][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:36:58,560][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:36:58,626][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:37:02,405][INFO ][o.e.c.m.MetadataDeleteIndexService] [tpotcluster-node-01] [logstash-1970.01.01/0jugN3uXT1aOat1QmEPt3w] deleting index
[2022-04-11T16:39:49,877][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:40:12,730][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:40:12,846][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:40:14,891][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:40:37,755][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:52:37,693][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.11/I4qSs1YcSCGPOEWZhQlVbw] update_mapping [_doc]
[2022-04-11T16:53:09,741][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [25202ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:53:50,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [13068ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:54:23,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@50261f32, interval=5s}] took [8366ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:55:08,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [6214ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:56:08,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [27524ms] which is above the warn threshold of [5000ms]
[2022-04-11T16:58:51,468][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5064ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:02:47,148][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/295274ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:04:07,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.9m/295520430430ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:04:17,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/215723ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:04:50,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/215722607389ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:06:13,336][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:08:04,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/90505192583ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:08:20,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/151780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:08:43,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/152353407516ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:10:06,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103754ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:11:08,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/103468523952ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:13:01,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173718ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:15:10,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173572821534ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:16:43,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@840d4cb, interval=1s}] took [173572ms] which is above the warn threshold of [5000ms]
[2022-04-11T17:16:44,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/225235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:14:43,696][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:57516}] took [152354ms] which is above the warn threshold of [5000ms]
[2022-04-11T17:18:55,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/225665885443ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:21:08,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/261349ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:21:56,927][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/260978937718ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:23:43,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/155818ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:25:45,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/156188086658ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:26:16,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7d4f44c5, interval=5s}] took [417167ms] which is above the warn threshold of [5000ms]
[2022-04-11T17:27:16,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/212458ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:29:18,766][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/212151066267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:31:20,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/242693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:33:16,053][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4m/242857030006ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:36:08,323][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287585ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:38:22,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/287728436838ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:36:16,844][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [242857ms] which is above the warn threshold of [5s]
[2022-04-11T17:40:35,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268588ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:39:46,247][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14s/14058ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@4f5159d4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e3ea0679]], which exceeds the warn threshold of [10s]
[2022-04-11T17:42:54,255][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268587441654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:45:48,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:45:19,897][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.4s/13423ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@feb6b09c], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@9d860bdd]], which exceeds the warn threshold of [10s]
[2022-04-11T17:47:56,556][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/313137089808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:49:35,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/226870ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:52:22,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/226909676362ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:55:07,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314280ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T17:57:29,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/314399869232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T17:59:53,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/303472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:02:25,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/303137068077ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:04:44,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290909ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:05:20,007][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [9.6m/581724ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [12.1m/728430ms]
[2022-04-11T18:06:36,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290760863056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:08:01,517][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [28.6m/1716932ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [13.9m/834543ms]
[2022-04-11T18:09:17,693][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/273057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:11:27,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.5m/273197806781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:11:20,264][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15s/15085ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@4f5159d4], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@e3ea0679]], which exceeds the warn threshold of [10s]
[2022-04-11T18:13:14,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/236759ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:13:34,484][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [37.1m/2226907ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [10.7m/643608ms]
[2022-04-11T18:15:51,679][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/236777489874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:18:23,499][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:21:08,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/308549713001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:23:47,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:26:23,876][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325095965728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:28:11,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/264073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:30:31,224][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.3m/263752169920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T18:33:16,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304071ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T18:35:49,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304232246380ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:09:14,679][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-11T19:09:14,699][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-11T19:09:14,702][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-11T19:09:21,065][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-11T19:09:21,068][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-11T19:09:21,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-11T19:09:21,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-11T19:09:21,069][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-11T19:09:21,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-11T19:09:21,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-11T19:09:21,070][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-11T19:09:21,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-11T19:09:21,071][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-11T19:09:21,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-11T19:09:21,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-11T19:09:21,072][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-11T19:09:21,075][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-11T19:09:21,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-11T19:09:21,076][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-11T19:09:21,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-11T19:09:21,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-11T19:09:21,077][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-11T19:09:21,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-11T19:09:21,078][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-11T19:09:21,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-11T19:09:21,079][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-11T19:09:21,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-11T19:09:21,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-11T19:09:21,080][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-11T19:09:21,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-11T19:09:21,081][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-11T19:09:21,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-11T19:09:21,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-11T19:09:21,082][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-11T19:09:21,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-11T19:09:21,083][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-11T19:09:21,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-11T19:09:21,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-11T19:09:21,084][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-11T19:09:21,085][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-11T19:09:21,085][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-11T19:09:21,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-11T19:09:21,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-11T19:09:21,086][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-11T19:09:21,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-11T19:09:21,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-11T19:09:21,087][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-11T19:09:21,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-11T19:09:21,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-11T19:09:21,088][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-11T19:09:21,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-11T19:09:21,089][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-11T19:09:21,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-11T19:09:21,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-11T19:09:21,090][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-11T19:09:21,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-11T19:09:21,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-11T19:09:21,091][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-11T19:09:21,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-11T19:09:21,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-11T19:09:21,092][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-11T19:09:21,093][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-11T19:09:21,194][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [106.5gb], net total_space [125.8gb], types [ext4]
[2022-04-11T19:09:21,196][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-11T19:09:21,739][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-11T19:09:31,999][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-11T19:09:32,010][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-11T19:09:32,012][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-11T19:09:32,015][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-11T19:09:32,016][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T19:09:32,017][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-11T19:09:32,020][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-11T19:09:32,021][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T19:09:32,023][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-11T19:09:32,024][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-11T19:09:32,025][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-11T19:09:32,026][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T19:09:32,027][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T19:09:32,028][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-11T19:09:32,029][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-11T19:09:33,111][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-11T19:09:33,285][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-11T19:09:34,056][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-11T19:09:34,858][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-11T19:09:34,859][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-11T19:09:34,958][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-11T19:09:34,963][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-11T19:09:35,232][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-11T19:09:37,673][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-11T19:09:37,846][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 230, version: 8948, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-11T19:09:38,028][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 230, version: 8948, reason: Publication{term=230, version=8948}
[2022-04-11T19:09:38,143][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-11T19:09:38,146][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-11T19:09:38,895][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-11T19:09:38,907][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [47] indices into cluster_state
[2022-04-11T19:09:41,593][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-11T19:09:41,622][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-11T19:13:39,019][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [2.9m/179178ms] to notify listeners on successful publication of cluster state (version: 8952, uuid: OgWZStazRUq684aukeT3lQ) for [shard-started StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [172], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [172], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-11T19:12:03,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:15:24,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9273075080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:15:46,331][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324592ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:14:52,377][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@466b153f, interval=5s}] took [13961ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:16:42,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324591934119ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:16:51,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66121ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:17:01,194][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/66120545706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:17:53,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63158ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:02,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63157895588ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:12,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18697ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:20,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18697633436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:24,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [81855ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:18:34,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20706ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:45,586][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.7s/20705724732ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:18:54,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21594ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:05,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21594229627ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:10,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14755ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:07,819][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [52.3s/52300ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [114], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [114], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.kibana_task_manager_7.16.2_001][0]], allocationId [8F4wVIqbTByoiw_lk78s7g], primary term [172], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.kibana_task_manager_7.16.2_001][0]], allocationId [8F4wVIqbTByoiw_lk78s7g], primary term [172], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[.kibana_task_manager_7.16.2_001][0]], allocationId [8F4wVIqbTByoiw_lk78s7g], primary term [172], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.kibana_task_manager_7.16.2_001][0]], allocationId [8F4wVIqbTByoiw_lk78s7g], primary term [172], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [114], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[.kibana_7.17.0_001][0]], allocationId [0sc8FCzORWi-9ycmZlz_dw], primary term [114], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-11T19:19:58,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14754602111ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:58,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49982ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:59,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.9s/49982002165ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:19:59,710][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59676}] took [65646ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:20:00,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@acb6090] took [88040ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:20:01,131][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [9.6m/581760ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [10.3m/619186ms]
[2022-04-11T19:20:07,996][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake
	at sun.security.ssl.SSLSocketImpl.handleEOF(SSLSocketImpl.java:1715) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1514) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
	Suppressed: java.net.SocketException: Broken pipe
		at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
		at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
		at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
		at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
		at sun.security.ssl.SSLSocketOutputRecord.encodeAlert(SSLSocketOutputRecord.java:82) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:400) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:312) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:463) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
		at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
		at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
		at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
		at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
		at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: java.io.EOFException: SSL peer shut down incorrectly
	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:483) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:472) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:160) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:111) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	... 25 more
[2022-04-11T19:20:14,704][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-11T19:20:24,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:20:25,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7347603309ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:20:26,639][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-11T19:20:28,390][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-11T19:21:06,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [5655ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:21:25,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:29,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8409194725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:33,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7811ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:44,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7810226333ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:51,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:52,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13517025970ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:21:52,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@2221fab3] took [30336ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:21:55,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:22:05,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9101138162ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:22:06,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10459ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:22:07,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10459700521ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:22:06,839][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59708}] took [10460ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:22:11,328][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [77162ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [44] unchanged indices
[2022-04-11T19:22:11,447][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.3m] publication of cluster state version [8961] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-11T19:22:35,030][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][46] overhead, spent [485ms] collecting in the last [1.4s]
[2022-04-11T19:22:36,795][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59708}] took [5248ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:22:36,789][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [21302ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:23:06,380][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [11813ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:23:22,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [5209ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:23:52,210][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-11T19:23:52,210][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-11T19:23:51,313][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [66584ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [46] unchanged indices
[2022-04-11T19:23:53,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@34c70944] took [14784ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:23:55,512][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.2m] publication of cluster state version [8962] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-11T19:24:17,866][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15701ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:22,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.7s/15701009904ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:25,274][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7641ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:28,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6s/7641428985ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:29,675][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59726}] took [7641ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:24:32,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:36,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7420418102ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:42,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9505ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:44,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9504380025ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:24:43,118][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59726}] took [9504ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:24:45,902][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][56][12] duration [11.6s], collections [1]/[18.5s], total [11.6s]/[13.5s], memory [187.9mb]->[105.9mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [95.9mb]->[95.9mb]/[2gb]}{[survivor] [8mb]->[9.9mb]/[0b]}
[2022-04-11T19:24:48,841][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][56] overhead, spent [11.6s] collecting in the last [18.5s]
[2022-04-11T19:24:53,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [35871ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:25:01,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@490ed723] took [5277ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:25:29,897][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=230, version=8962}] took [1.3m] which is above the warn threshold of [30s]: [running task [Publication{term=230, version=8962}]] took [76ms], [connecting to new nodes] took [46ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@da71c6e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1e79b753] took [29934ms], [org.elasticsearch.script.ScriptService@1b3834c1] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5499c9a8] took [1ms], [org.elasticsearch.snapshots.RestoreService@61e8eb39] took [0ms], [org.elasticsearch.ingest.IngestService@32780b78] took [10457ms], [org.elasticsearch.action.ingest.IngestActionForwarder@683cafca] took [176ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d2640@7272efec] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@54e573b8] took [6ms], [org.elasticsearch.tasks.TaskManager@26349197] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@2b7b3864] took [117ms], [org.elasticsearch.cluster.InternalClusterInfoService@4f422da9] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@11f2c0c1] took [0ms], [org.elasticsearch.indices.SystemIndexManager@3fe52349] took [959ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@3bda7ebe] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@74c62010] took [405ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7731947] took [1ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@448dbc41] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013ba000@5189d28] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@4a4e7366] took [60ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1a79674e] took [30846ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@a100ad6] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@36a69f18] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3c6a65a0] took [6673ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@207c6ae6] took [99ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@61349821] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@63688540] took [1741ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5499c9a8] took [36ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@cc0bbe8] took [370ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f28cbcc] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@246b80b2] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@21a0698] took [2ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@41ebba80] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@370ef08a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@753c2c19] took [38ms], [org.elasticsearch.node.ResponseCollectorService@691fa784] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@40f6a806] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3c9a32] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@14f432c8] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@454bf96f] took [0ms], [org.elasticsearch.indices.store.IndicesStore@610d8b7c] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6350bd9] took [0ms], [org.elasticsearch.license.LicenseService@7e5054da] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@69116b70] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3d6dc9c5] took [0ms], [org.elasticsearch.gateway.GatewayService@14843365] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@33f528e4] took [0ms]
[2022-04-11T19:25:42,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [8111ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:26:48,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16332ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:26:51,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16332570136ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:01,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:10,115][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13658727913ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:20,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19346ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:32,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.3s/19346266631ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:40,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19424ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:27:50,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19423875845ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:01,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21545ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:12,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.5s/21544947806ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:21,367][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:27,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19493835335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:28,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@1b9eb550] took [148387ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:28:34,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12757ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:40,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12756532327ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:46,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@28976841, interval=5s}] took [12243ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:28:46,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12243ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:51,040][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.2s/12243515448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:55,351][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9012ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:28:59,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9011875458ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:03,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8495ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:08,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8494735711ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:13,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9507ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:17,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9506856892ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:21,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8954ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:25,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8954281751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:25,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [8954ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:29:30,511][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:29,642][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8954ms] which is above the warn threshold of [5s]
[2022-04-11T19:29:32,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8559279738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:35,894][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5353ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:39,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [5352ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:29:39,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5352579788ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:42,195][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6500ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:45,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6499859307ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:45,174][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [212357ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [45] unchanged indices
[2022-04-11T19:29:47,598][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:50,938][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5480915140ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:53,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5467ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:29:54,599][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [10948ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:29:50,699][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [3.8m] publication of cluster state version [8963] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{nWOG4iQKS6aZqt_pISFuKw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-11T19:29:55,381][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5467593308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:30:01,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@28976841, interval=5s}] took [7962ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:31:16,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24619ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:19,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.6s/24618382148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:25,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9576ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:29,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9576439562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:34,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:37,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9395965854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:38,360][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59750}] took [9396ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:31:44,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:31:52,041][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5s/8592619796ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:00,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5110/0x00000008017e0960@35cd7aa3] took [112244ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:31:59,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16127ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:06,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16127582896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:13,985][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13957ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:19,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13956378652ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:24,484][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:24,484][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:59750}] took [13956ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:32:31,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10580728981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:31,240][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][69][13] duration [16.5s], collections [1]/[2.3m], total [16.5s]/[30.1s], memory [177.9mb]->[110.9mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [95.9mb]->[100.2mb]/[2gb]}{[survivor] [9.9mb]->[6.7mb]/[0b]}
[2022-04-11T19:32:40,291][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [24537ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:32:40,373][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:46,645][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15328778045ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:53,622][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13673ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:32:58,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13672486579ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:03,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:08,727][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10472388688ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:13,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9318ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:12,127][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=230, version=8963}] took [3.1m] which is above the warn threshold of [30s]: [running task [Publication{term=230, version=8963}]] took [65ms], [connecting to new nodes] took [406ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@da71c6e] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@1e79b753] took [13164ms], [org.elasticsearch.script.ScriptService@1b3834c1] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5499c9a8] took [0ms], [org.elasticsearch.snapshots.RestoreService@61e8eb39] took [0ms], [org.elasticsearch.ingest.IngestService@32780b78] took [6041ms], [org.elasticsearch.action.ingest.IngestActionForwarder@683cafca] took [69ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d2640@7272efec] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@54e573b8] took [67ms], [org.elasticsearch.tasks.TaskManager@26349197] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@2b7b3864] took [111ms], [org.elasticsearch.cluster.InternalClusterInfoService@4f422da9] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@11f2c0c1] took [71ms], [org.elasticsearch.indices.SystemIndexManager@3fe52349] took [3635ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@3bda7ebe] took [72ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008012d94e8@74c62010] took [443ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7731947] took [156ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@448dbc41] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x00000008013ba000@5189d28] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@4a4e7366] took [73ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1a79674e] took [82691ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@a100ad6] took [1ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@36a69f18] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3c6a65a0] took [22672ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@207c6ae6] took [1330ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@61349821] took [527ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@63688540] took [20443ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@5499c9a8] took [930ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@cc0bbe8] took [17431ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4f28cbcc] took [177ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@246b80b2] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@21a0698] took [11814ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@41ebba80] took [6478ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@370ef08a] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@753c2c19] took [1566ms], [org.elasticsearch.node.ResponseCollectorService@691fa784] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@40f6a806] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@3c9a32] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@14f432c8] took [161ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@454bf96f] took [149ms], [org.elasticsearch.indices.store.IndicesStore@610d8b7c] took [294ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6350bd9] took [0ms], [org.elasticsearch.license.LicenseService@7e5054da] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@69116b70] took [80ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@3d6dc9c5] took [0ms], [org.elasticsearch.gateway.GatewayService@14843365] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@33f528e4] took [0ms]
[2022-04-11T19:33:18,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9318075504ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:27,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:33,453][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [7.8m/471501ms] which is longer than the warn threshold of [300000ms]; there are currently [8] pending tasks, the oldest of which has age [10.7m/645342ms]
[2022-04-11T19:33:34,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14693026452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:40,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12885ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:31,182][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14693ms] which is above the warn threshold of [5s]
[2022-04-11T19:33:45,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [12885ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:33:46,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12885162150ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:51,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@466b153f, interval=5s}] took [10411ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:33:51,404][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:33:57,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10411997646ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:02,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11314ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:09,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11313843586ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:15,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:19,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@6a37f7bf, interval=1s}] took [12596ms] which is above the warn threshold of [5000ms]
[2022-04-11T19:34:19,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12596497893ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:23,957][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:28,287][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8693472973ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:34:46,617][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [59.1s/59185ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-11T19:36:00,322][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92219ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:37:44,178][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/92218373512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:39:52,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/229649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:41:35,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/229649346783ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:42:50,002][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/172490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:44:24,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/172240658825ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:46:03,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/195551ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:48:12,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.2m/195800793034ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:50:31,972][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/268494ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T19:54:00,076][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/267993854474ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T19:57:10,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396654ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:00:38,733][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396678744625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:03:25,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367727ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:06:36,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367954352579ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:10:17,163][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:13:04,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/415659430698ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:16:15,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:19:44,302][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/340176881629ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:23:11,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/423672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:26:24,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/424185702636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:29:22,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/383559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:32:50,503][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/383104077230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:36:28,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/421078ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:39:57,489][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7m/421355020849ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:43:10,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/405021ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-11T20:46:29,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/404867254215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-11T20:50:08,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7m/402656ms] on absolute clock which is above the warn threshold of [5000ms]
