[2022-04-13T01:51:13,431][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-13T01:51:13,516][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-13T01:51:13,518][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-13T01:51:33,121][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-13T01:51:33,126][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-13T01:51:33,126][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-13T01:51:33,127][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-13T01:51:33,127][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-13T01:51:33,127][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-13T01:51:33,128][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-13T01:51:33,128][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-13T01:51:33,129][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-13T01:51:33,129][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-13T01:51:33,130][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-13T01:51:33,130][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-13T01:51:33,131][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-13T01:51:33,131][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-13T01:51:33,131][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-13T01:51:33,132][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-13T01:51:33,132][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-13T01:51:33,132][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-13T01:51:33,133][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-13T01:51:33,134][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-13T01:51:33,134][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-13T01:51:33,134][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-13T01:51:33,135][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-13T01:51:33,135][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-13T01:51:33,136][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-13T01:51:33,136][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-13T01:51:33,136][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-13T01:51:33,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-13T01:51:33,137][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-13T01:51:33,138][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-13T01:51:33,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-13T01:51:33,139][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-13T01:51:33,140][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-13T01:51:33,140][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-13T01:51:33,140][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-13T01:51:33,140][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-13T01:51:33,141][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-13T01:51:33,141][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-13T01:51:33,142][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-13T01:51:33,142][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-13T01:51:33,143][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-13T01:51:33,143][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-13T01:51:33,144][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-13T01:51:33,144][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-13T01:51:33,144][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-13T01:51:33,145][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-13T01:51:33,145][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-13T01:51:33,145][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-13T01:51:33,146][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-13T01:51:33,146][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-13T01:51:33,146][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-13T01:51:33,147][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-13T01:51:33,147][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-13T01:51:33,147][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-13T01:51:33,148][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-13T01:51:33,148][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-13T01:51:33,148][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-13T01:51:33,149][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-13T01:51:33,150][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-13T01:51:33,243][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [106.5gb], net total_space [125.8gb], types [ext4]
[2022-04-13T01:51:33,245][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-13T01:51:33,866][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-13T01:53:25,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9647ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:53:25,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-13T01:54:17,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9647228246ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:54:33,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80972ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:54:48,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/80972283282ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:55:01,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28790ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:55:10,015][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28790194918ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:55:15,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14293ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:55:21,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14292329267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:55:27,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58c9ff22, interval=30s}] took [11858ms] which is above the warn threshold of [5000ms]
[2022-04-13T01:55:27,837][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:55:33,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11858245982ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:55:45,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:55:48,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@551d4e32, interval=1m}] took [17147ms] which is above the warn threshold of [5000ms]
[2022-04-13T01:55:58,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17147803920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:56:09,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23676ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:56:10,285][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [23676ms] which is above the warn threshold of [5000ms]
[2022-04-13T01:56:17,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23676051184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:56:30,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20255ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:56:48,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20254754645ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:57:09,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:57:25,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.7s/38737093208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:57:45,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:58:01,361][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.2s/31297735545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:58:10,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31964ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:58:10,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [31963ms] which is above the warn threshold of [5000ms]
[2022-04-13T01:58:18,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31963401263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:58:31,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:58:44,225][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20254422524ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:58:56,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24718ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:58:57,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [24717ms] which is above the warn threshold of [5000ms]
[2022-04-13T01:59:06,060][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24717659786ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:59:16,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20388ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:59:16,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [20387ms] which is above the warn threshold of [5000ms]
[2022-04-13T01:59:21,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20387710505ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:59:25,705][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9178ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:59:32,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1s/9178764216ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T01:59:41,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [15692ms] which is above the warn threshold of [5000ms]
[2022-04-13T01:59:41,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15693ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T01:59:55,576][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.6s/15692414427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:00:12,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29892ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:00:13,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [29892ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:00:29,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.8s/29892088747ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:00:47,379][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35041ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:00:51,687][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@551d4e32, interval=1m}] took [35040ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:01:04,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35040858595ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:01:21,677][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:01:22,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58c9ff22, interval=30s}] took [34329ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:01:40,686][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34329813751ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:01:58,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36578ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:02:01,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@551d4e32, interval=1m}] took [36577ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:02:09,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [36.5s/36577605551ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:02:23,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25504ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:02:26,151][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [25504ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:02:36,824][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25504102805ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:02:59,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35229ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:03:03,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58c9ff22, interval=30s}] took [35229ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:03:18,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35229127731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:03:53,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54756ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:03:57,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58c9ff22, interval=30s}] took [54755ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:04:06,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.7s/54755646807ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:04:20,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27463ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:04:33,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.4s/27462740896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:04:44,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24198ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:04:53,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.1s/24198067174ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:04:57,010][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13431ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:04:59,341][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13430730336ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:06:40,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [7877ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:07:09,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58c9ff22, interval=30s}] took [6614ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:07:52,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [8751ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:08:58,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@551d4e32, interval=1m}] took [8838ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:10:07,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58c9ff22, interval=30s}] took [11676ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:10:59,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [10465ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:11:32,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@551d4e32, interval=1m}] took [6914ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:12:12,701][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@58c9ff22, interval=30s}] took [5428ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:12:45,995][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [6744ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:13:08,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7560ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:13:25,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7559803726ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:13:42,558][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34401ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:14:02,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34401402266ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:14:13,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.2s/33275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:14:15,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.2s/33274515040ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:14:49,148][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-13T02:14:49,162][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-13T02:14:50,321][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-13T02:14:50,474][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-13T02:14:51,380][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-13T02:14:52,219][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-13T02:14:52,220][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-13T02:14:52,329][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-13T02:14:52,331][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-13T02:14:52,586][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-13T02:14:55,011][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-13T02:14:55,156][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 237, version: 9326, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-13T02:14:55,324][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 237, version: 9326, reason: Publication{term=237, version=9326}
[2022-04-13T02:14:55,493][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-13T02:14:55,496][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-13T02:14:57,120][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-13T02:14:57,165][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [49] indices into cluster_state
[2022-04-13T02:16:58,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:17:21,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6179466184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:17:30,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/135728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:17:41,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.2m/135728439821ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:17:49,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [170836ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:17:49,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20143ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:18:01,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20142760663ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:18:14,316][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23554ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:18:30,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23554195974ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:18:43,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:18:46,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [29261ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:18:53,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29261551001ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:03,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20435ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:10,395][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.4s/20434864393ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:11,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [20434ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:19:15,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:21,491][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.8s/12880092700ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:29,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13248ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:36,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13248750466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:45,312][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:50,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15416557725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:55,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10315ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:19:58,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10314911986ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:05,770][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10153ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:11,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10153444167ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:18,136][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12005ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:23,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12004594429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:29,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11606ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:38,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11605934821ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:46,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16647ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:51,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16646793248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:54,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9800ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:20:56,729][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9800029345ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:21:03,525][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@d085479] took [108125ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:21:12,795][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=237, version=9328}] took [6.2m] which is above the warn threshold of [30s]: [running task [Publication{term=237, version=9328}]] took [0ms], [connecting to new nodes] took [1ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54e87bc8] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@505edde5] took [372236ms], [org.elasticsearch.script.ScriptService@55e3c4de] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [0ms], [org.elasticsearch.snapshots.RestoreService@58ea61a0] took [0ms], [org.elasticsearch.ingest.IngestService@7e7d2ead] took [43ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3573d088] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d6200@218115b9] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2a6d8748] took [29ms], [org.elasticsearch.tasks.TaskManager@3df2dcee] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@162ebc48] took [17ms], [org.elasticsearch.cluster.InternalClusterInfoService@613d88e2] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7d167f9c] took [0ms], [org.elasticsearch.indices.SystemIndexManager@1e88ce70] took [239ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@788cd0f2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@50d1da72] took [29ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@ca906e3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@48c2079b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@306a1be9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@3c8e9e3] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1d5a5d5d] took [1260ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4c035c28] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1be389ae] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4d5774e6] took [359ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@22f1a612] took [41ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@126b3627] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@15c2633d] took [133ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [25ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@63c4d527] took [54ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4c2a3e7b] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@44ce2370] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@56f0886] took [44ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@21bf71a5] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@49950e40] took [17ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5b04f430] took [65ms], [org.elasticsearch.node.ResponseCollectorService@1ec94ac] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1623dabb] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@77a53131] took [108ms], [org.elasticsearch.shutdown.PluginShutdownService@b7cee81] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@60163f24] took [51ms], [org.elasticsearch.indices.store.IndicesStore@53054dcf] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6cdf28d6] took [0ms], [org.elasticsearch.license.LicenseService@33c2caf] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@2b909849] took [23ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@505b569a] took [0ms], [org.elasticsearch.gateway.GatewayService@1935bde] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@eef62c1] took [0ms]
[2022-04-13T02:21:14,362][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [6.2m/376918ms] which is longer than the warn threshold of [300000ms]; there are currently [2] pending tasks, the oldest of which has age [6.2m/377756ms]
[2022-04-13T02:21:36,893][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-13T02:21:37,908][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-13T02:22:11,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6708ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:12,757][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [13787ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:22:16,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6707926172ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:19,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7970ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:19,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@45a86a1c] took [7969ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:22:22,816][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7969526097ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:27,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8164ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:30,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8164461833ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:33,657][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:37,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6054ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:22:37,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6054561592ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:46,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12607ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:51,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [12607ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:22:52,235][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12607580415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:22:59,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11467ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:23:01,637][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [11467ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:23:06,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11467333417ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:23:20,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20818ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:23:28,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20817217282ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:23:38,016][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18781ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:23:46,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18781155252ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:23:57,929][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19694ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:24:09,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.6s/19693839581ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:24:20,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:24:22,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [60491ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:24:31,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22016938057ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:24:41,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21700ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:24:53,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.7s/21700741738ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:25:11,236][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27721ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:25:23,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.7s/27720980616ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:24:56,682][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [67] timed out after [18578ms]
[2022-04-13T02:25:42,668][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:26:03,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30866118536ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:26:23,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.3s/42307ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:26:38,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.3s/42306112395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:26:53,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30481ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:27:08,535][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30481367684ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:26:46,466][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [42306ms] which is above the warn threshold of [5s]
[2022-04-13T02:27:24,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [72787ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:27:29,575][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33364ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:27:44,696][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.3s/33364320990ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:27:55,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29242ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:28:07,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.2s/29241501142ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:28:19,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23972ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:28:33,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.9s/23972555407ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:28:47,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27584ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:29:06,023][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.5s/27584030186ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:29:18,296][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31462ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:29:17,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [31461ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:29:30,591][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.4s/31461805258ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:29:46,184][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27250ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:30:01,249][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27250185184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:30:12,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:30:46,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.3s/26354277215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:30:59,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48180ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:31:09,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.1s/48180563697ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:31:19,930][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:31:25,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20188395531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:31:31,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11732ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:31:37,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11732645996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:31:41,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10334ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:31:46,864][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10333843675ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:31:52,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10432ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:31:56,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10431656793ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:02,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10409ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:07,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10409728282ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:13,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10679ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:17,713][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10678405366ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:22,665][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:31,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6s/8604376814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:38,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16072ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:44,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16071390191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:48,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10593ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:32:56,345][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10592859385ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:33:05,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16060ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:33:13,022][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16060212562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:33:19,261][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:33:27,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.2s/14290905834ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:33:33,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14912ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:33:40,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14912792083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:33:49,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15247ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:33:40,966][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [11.6m/700018ms] ago, timed out [11.3m/681440ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [67]
[2022-04-13T02:33:55,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15246497418ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:02,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12741ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:06,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12741459005ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:12,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:18,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.8s/10890972493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:22,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:26,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9300064226ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:32,318][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10009ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:37,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10008108562ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:42,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:48,169][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10625553807ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:52,439][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:34:58,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9563997269ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:35:02,626][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9794ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:35:17,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9793801095ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:35:21,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19511ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:35:27,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19511551466ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:35:33,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11663ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:35:39,413][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11662404285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:35:40,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@3e110b17] took [321832ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:35:45,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11867ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:35:50,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11867463453ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:35:54,631][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:36:11,204][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9253855032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:36:11,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [9253ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:36:28,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34017ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:36:40,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34s/34017163865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:36:52,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24490ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:37:03,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.4s/24490009131ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:37:03,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:37:06,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [11455ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:37:06,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.4s/11455259200ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:37:33,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5212ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:39:05,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@1739ea2b] took [88555ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:39:05,601][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54816}] took [976523ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:39:54,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [29571ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:40:02,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [5773ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:40:06,193][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.1m/66909ms] to compute cluster state update for [ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8101bbc7]], which exceeds the warn threshold of [10s]
[2022-04-13T02:40:21,351][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54832}] took [8605ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:40:27,742][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54868}] took [5104ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:40:27,742][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5104ms] which is above the warn threshold of [5s]
[2022-04-13T02:40:28,633][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54828}] took [25593ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:40:28,633][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54830}] took [32567ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:40:45,404][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54878}] took [5676ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:40:54,937][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54888}] took [7838ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:41:21,027][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54888}] took [13608ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:41:46,532][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54888}] took [6804ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:42:15,746][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6242ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:42:20,682][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6241891499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:42:27,011][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11104ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:42:31,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@50cc0bd3] took [118420ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:42:31,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.1s/11103618911ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:42:38,694][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11843ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:42:42,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11843462050ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:42:47,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:42:51,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9265492628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:42:50,943][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [21108ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:42:52,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [5191ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:43:31,773][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25631ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:43:35,524][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.6s/25630656673ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:43:39,193][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7468ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:43:42,258][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7468364846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:43:42,124][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54888}] took [7468ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:43:44,418][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5492ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:43:43,903][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [7468ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:43:45,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5491849968ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:43:45,391][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5492ms] which is above the warn threshold of [5s]
[2022-04-13T02:43:56,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@4d3399e7] took [8887ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:44:38,551][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5161ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:44:40,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5161233022ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:44:42,046][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@31dca3de] took [10896ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:44:51,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [7604ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:44:59,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6529ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:45:12,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [7035ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:45:25,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [9071ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:45:36,851][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6439ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:45:37,483][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [6839ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:45:38,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6438868152ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:45:40,237][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54748}] took [19029ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:45:51,042][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54916}] took [5574ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:45:52,394][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [11385ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:46:01,028][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54918}] took [5423ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:46:14,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6687ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:46:14,016][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54918}] took [9337ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:46:30,822][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54918}] took [6221ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:47:12,543][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54918}] took [6672ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:47:13,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@db46cf] took [56888ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:47:36,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [12740ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:47:51,710][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5591ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:48:05,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9368ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:48:07,028][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9367790178ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:48:10,833][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54930}] took [13971ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:48:11,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6255ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:48:11,387][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6255329851ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:48:17,391][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@7d06b364] took [24606ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:48:21,806][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [28.6s/28639ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [178], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[.tasks][0]], allocationId [mXnj0VgeR4q7un5j66voxA], primary term [178], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-13T02:48:40,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5253ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:48:44,941][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54930}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:48:52,026][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5236ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:48:57,911][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [16638ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [48] unchanged indices
[2022-04-13T02:48:58,735][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [32.4s] publication of cluster state version [9330] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T02:49:23,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [7429ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:50:06,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@2478992e] took [6020ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:51:00,399][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [12613ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:51:00,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.8s/22898ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:51:01,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24032166098ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:51:02,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [24926ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:51:09,861][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54942}] took [6026ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:49:39,813][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake
	at sun.security.ssl.SSLSocketImpl.handleEOF(SSLSocketImpl.java:1715) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1514) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1416) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:451) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
	Suppressed: java.net.SocketException: Broken pipe
		at sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420) ~[?:?]
		at sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440) ~[?:?]
		at sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826) ~[?:?]
		at java.net.Socket$SocketOutputStream.write(Socket.java:1045) ~[?:?]
		at sun.security.ssl.SSLSocketOutputRecord.encodeAlert(SSLSocketOutputRecord.java:82) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:400) ~[?:?]
		at sun.security.ssl.TransportContext.fatal(TransportContext.java:312) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:463) ~[?:?]
		at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:422) ~[?:?]
		at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:574) ~[?:?]
		at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:183) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
		at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
		at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
		at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
		at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
		at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
		at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: java.io.EOFException: SSL peer shut down incorrectly
	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:483) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:472) ~[?:?]
	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:160) ~[?:?]
	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:111) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1506) ~[?:?]
	... 25 more
[2022-04-13T02:52:10,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@34fe261f] took [25481ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:52:28,432][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54950}] took [8647ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:52:34,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5933ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:53:03,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6203ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:53:16,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@7b4cbac5] took [10606ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:55:10,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11276ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:55:12,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11276280692ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:55:16,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6922ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:55:21,902][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6922236349ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:55:23,579][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54970}] took [6922ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:55:24,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7582ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:55:27,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7581949036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:55:31,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6606ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:55:32,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@3ec7b56d] took [100751ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:55:32,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6605589653ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:55:37,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [5979ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:55:49,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [7804ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:56:00,412][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [7404ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:56:09,574][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54970}] took [15409ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:56:20,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T02:56:22,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4s/7417393740ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T02:56:21,742][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [10659ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:56:23,539][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [136] timed out after [41042ms]
[2022-04-13T02:56:23,663][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9818ms] which is above the warn threshold of [5s]
[2022-04-13T02:56:34,088][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5003ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:56:47,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6604ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:56:42,549][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.1m/67263ms] ago, timed out [26.2s/26221ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [136]
[2022-04-13T02:57:00,745][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [7004ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:57:22,327][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54970}] took [9406ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:57:47,100][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54978}] took [9405ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:57:48,639][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54980}] took [7005ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:57:59,420][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@226f72b1] took [49762ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:58:00,680][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54976}] took [12408ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:58:17,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5365ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:58:48,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@2180c74c] took [9486ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:59:12,904][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-13T02:59:50,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [8142ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:01:00,079][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:54990}] took [11407ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:01:17,089][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55022}] took [6889ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:01:17,089][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55024}] took [6689ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:01:26,091][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55020}] took [8005ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:01:30,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@7f261297] took [91983ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:02:15,920][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55038}] took [5218ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:02:56,925][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=237, version=9332}] took [13.5m] which is above the warn threshold of [30s]: [running task [Publication{term=237, version=9332}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54e87bc8] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@505edde5] took [20635ms], [org.elasticsearch.script.ScriptService@55e3c4de] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [0ms], [org.elasticsearch.snapshots.RestoreService@58ea61a0] took [0ms], [org.elasticsearch.ingest.IngestService@7e7d2ead] took [170ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3573d088] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d6200@218115b9] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2a6d8748] took [115ms], [org.elasticsearch.tasks.TaskManager@3df2dcee] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@162ebc48] took [107ms], [org.elasticsearch.cluster.InternalClusterInfoService@613d88e2] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7d167f9c] took [48ms], [org.elasticsearch.indices.SystemIndexManager@1e88ce70] took [1237ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@788cd0f2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@50d1da72] took [166ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@ca906e3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@48c2079b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@306a1be9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@3c8e9e3] took [103ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1d5a5d5d] took [19537ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4c035c28] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1be389ae] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4d5774e6] took [4570ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@22f1a612] took [122ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@126b3627] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@15c2633d] took [1450ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [72ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@63c4d527] took [2881ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4c2a3e7b] took [33ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@44ce2370] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@56f0886] took [2085ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@21bf71a5] took [2048ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@49950e40] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5b04f430] took [0ms], [org.elasticsearch.node.ResponseCollectorService@1ec94ac] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1623dabb] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@77a53131] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@b7cee81] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@60163f24] took [1ms], [org.elasticsearch.indices.store.IndicesStore@53054dcf] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6cdf28d6] took [0ms], [org.elasticsearch.license.LicenseService@33c2caf] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@2b909849] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@505b569a] took [756669ms], [org.elasticsearch.gateway.GatewayService@1935bde] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@eef62c1] took [0ms]
[2022-04-13T03:03:09,562][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [10.4s] publication of cluster state version [9333] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T03:03:32,648][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:03:41,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [11886ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:03:44,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10085292598ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:04:08,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.8s/34875ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:04:35,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.7s/34750569444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:04:47,854][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.4s/39446ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:05:05,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.5s/39570123608ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:05:01,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [74320ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:05:36,633][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49049ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:05:53,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49s/49049079217ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:06:12,547][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35826ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:06:51,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.8s/35825954779ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:07:15,939][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64110ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:08:23,122][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64110356945ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:08:27,342][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:08:30,233][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72328826210ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:08:39,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10910ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:09:05,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10910038808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:09:24,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45185ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:09:38,134][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.1s/45185001774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:09:54,492][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28832ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:10:04,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.8s/28832171774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:10:18,172][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24758ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:10:36,888][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.7s/24757708902ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:10:49,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30539ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:11:19,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.5s/30539679130ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:11:38,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49151ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:11:51,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49150156823ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:12:04,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25419ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:12:16,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25419352060ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:12:29,900][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27059ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:12:33,960][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@106e4f4d] took [414118ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:12:46,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27059391499ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:13:25,689][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53700ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:13:50,602][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.7s/53700084175ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:14:23,089][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.4s/58455ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:14:43,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.4s/58454675138ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:15:06,641][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44s/44079ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:15:23,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44s/44079090385ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:15:38,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31893ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:15:39,584][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [75972ms] which is above the warn threshold of [5s]
[2022-04-13T03:15:54,915][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31892946267ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:16:12,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:16:28,359][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32.9s/32936554416ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:16:45,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33915ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:16:44,185][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=237, version=9333}] took [12.9m] which is above the warn threshold of [30s]: [running task [Publication{term=237, version=9333}]] took [75ms], [connecting to new nodes] took [433ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54e87bc8] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@505edde5] took [3083ms], [org.elasticsearch.script.ScriptService@55e3c4de] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [0ms], [org.elasticsearch.snapshots.RestoreService@58ea61a0] took [1ms], [org.elasticsearch.ingest.IngestService@7e7d2ead] took [72ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3573d088] took [22ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d6200@218115b9] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2a6d8748] took [0ms], [org.elasticsearch.tasks.TaskManager@3df2dcee] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@162ebc48] took [27ms], [org.elasticsearch.cluster.InternalClusterInfoService@613d88e2] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7d167f9c] took [1ms], [org.elasticsearch.indices.SystemIndexManager@1e88ce70] took [88ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@788cd0f2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@50d1da72] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@ca906e3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@48c2079b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@306a1be9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@3c8e9e3] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1d5a5d5d] took [406ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4c035c28] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1be389ae] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4d5774e6] took [72ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@22f1a612] took [44ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@126b3627] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@15c2633d] took [50ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [0ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@63c4d527] took [244ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4c2a3e7b] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@44ce2370] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@56f0886] took [18ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@21bf71a5] took [1ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@49950e40] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5b04f430] took [43ms], [org.elasticsearch.node.ResponseCollectorService@1ec94ac] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1623dabb] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@77a53131] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@b7cee81] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@60163f24] took [50ms], [org.elasticsearch.indices.store.IndicesStore@53054dcf] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6cdf28d6] took [0ms], [org.elasticsearch.license.LicenseService@33c2caf] took [1ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@2b909849] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@505b569a] took [0ms], [org.elasticsearch.gateway.GatewayService@1935bde] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@eef62c1] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@7eba315]]]] took [57855ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@58f81922]]]] took [74164ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@7bb9ccbf]]]] took [44353ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@48222fab]]]] took [58657ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@128bceb1]]]] took [67938ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@a22e4cb]]]] took [11376ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@3fcb0d04]]]] took [43133ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@664d2221]]]] took [35398ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@2a7f861f]]]] took [41690ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@37663b27]]]] took [44062ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@50a2b3e2]]]] took [43564ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@5f71cffd]]]] took [28624ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@9e8e572]]]] took [75991ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@f51586b]]]] took [55270ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@4a29c7c0]]]] took [42773ms], [ClusterStateObserver[null]] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@1b76e75f]]]] took [54248ms]
[2022-04-13T03:17:00,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.9s/33915669429ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:17:17,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:17:33,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.1s/31139592610ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:17:51,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33125ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:17:54,087][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [64264ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:18:03,073][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [33.1s/33125060662ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:18:04,770][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [14.8m/890489ms] which is longer than the warn threshold of [300000ms]; there are currently [8] pending tasks, the oldest of which has age [27.7m/1664956ms]
[2022-04-13T03:18:31,901][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [34407ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:18:24,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:18:51,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34407259434ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:19:16,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50206ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:22:28,912][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [22.5s/22555ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@3067128f], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@8101bbc7], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@159a686c], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cf366dd0]], which exceeds the warn threshold of [10s]
[2022-04-13T03:22:31,796][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [50.2s/50205926444ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:22:46,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/210146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:22:53,722][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [210146ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:22:56,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/210146244489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:23:08,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24896ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:23:18,646][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.8s/24895160029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:23:27,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18667ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:23:40,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6s/18667901954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:23:49,444][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21983ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:23:59,206][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.9s/21982060370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:24:01,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [21982ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:24:10,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20351ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:24:19,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.3s/20351494358ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:24:28,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:24:43,823][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18783375942ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:24:40,395][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18783ms] which is above the warn threshold of [5s]
[2022-04-13T03:24:55,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:25:12,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.2s/25299628672ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:25:25,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30957ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:25:48,068][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.9s/30956731202ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:26:00,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:26:13,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34605136395ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:26:22,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23411ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:26:35,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23411192800ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:26:48,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:27:00,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.4s/25417280115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:27:21,841][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:54664}] took [1440335ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:27:20,108][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:27:36,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.7s/30765058253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:27:54,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35030ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:28:06,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35s/35030027433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:28:22,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27810ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:28:34,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.8s/27809553296ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:28:31,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@601ea763] took [252077ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:28:54,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:29:10,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31011275862ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:29:35,636][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.9s/41900ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:29:48,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.8s/41899306981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:30:04,220][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27669ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:30:23,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27669746058ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:30:38,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35125ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:30:55,279][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.1s/35124644759ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:31:05,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [35124ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:31:11,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:31:27,013][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.5s/31514167763ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:31:49,739][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39857ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:32:14,442][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.8s/39857001102ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:32:31,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.4s/40465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:32:48,596][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [199] timed out after [207076ms]
[2022-04-13T03:32:54,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [40.4s/40464945489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:33:12,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42651ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:33:28,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42650973775ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:33:50,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37313ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:33:56,981][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [79964ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:34:21,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37313188184ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:34:36,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:34:46,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [45685ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:35:03,481][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.6s/45685269972ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:36:29,123][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113291ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:36:48,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113291067999ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:37:14,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:36:37,109][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.2m/373191ms] ago, timed out [2.7m/166115ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [199]
[2022-04-13T03:37:35,478][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.1s/44144394542ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:38:02,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:38:16,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.6s/47689883557ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:38:29,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28735ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:38:42,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [28735ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:38:43,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7s/28735326467ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:38:52,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23355ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:39:08,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.3s/23354665342ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:39:21,469][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28488ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:39:32,333][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.4s/28488019080ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:39:47,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25944ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:39:58,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.9s/25943785533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:40:17,664][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:41:16,063][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [18.3m/1098572ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-13T03:41:15,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.4s/30402228450ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:41:28,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69833ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:41:54,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69832753191ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:42:10,176][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43237ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:42:27,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.2s/43237369659ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:42:40,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29636ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:42:55,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [29.6s/29635726851ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:43:30,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49454ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:44:49,254][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.4s/49454734613ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:45:09,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94310ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:45:23,271][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/94309212163ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:45:42,327][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37599ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:46:04,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.5s/37599161564ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:46:28,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46574ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:46:48,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.5s/46573890728ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:47:11,865][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:47:08,623][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@2af40b63] took [478831ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:47:30,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.8s/43880650507ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:47:45,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31932ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:48:16,304][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.9s/31931793478ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:48:29,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44943ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:48:43,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [44.9s/44942306108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:48:58,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:48:58,898][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@137e5a0d, interval=1m}] took [30236ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:48:30,722][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [31932ms] which is above the warn threshold of [5s]
[2022-04-13T03:49:11,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.2s/30236513265ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:49:42,128][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:50:01,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [41.1s/41165370031ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:50:18,288][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37154ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:50:38,244][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.1s/37153910112ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:51:04,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [76005ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:50:58,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:51:24,582][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.8s/38851454817ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:51:46,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:52:17,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.1s/49140637249ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:52:36,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.5s/49579ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:52:51,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [49.5s/49578788176ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:53:04,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30843ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:53:11,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [30842ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:53:44,617][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.8s/30842652457ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:53:51,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.9s/46927ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:53:38,695][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [218] timed out after [366881ms]
[2022-04-13T03:54:02,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.9s/46927237447ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:54:17,259][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22733ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:53:54,882][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [6.6m/397724ms] ago, timed out [30.8s/30843ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [218]
[2022-04-13T03:54:28,203][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.7s/22732888625ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:54:42,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26731ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:54:53,978][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26.7s/26731141787ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:55:11,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:55:27,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.1s/28119687794ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:55:34,298][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [28119ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:55:42,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:55:49,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31.8s/31891144473ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:56:00,952][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:56:12,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.4s/18413124037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:56:38,223][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37327ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:56:50,020][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [37.3s/37326680505ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:56:20,483][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [18413ms] which is above the warn threshold of [5s]
[2022-04-13T03:57:05,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27007ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:57:24,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27s/27007029593ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:57:41,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:58:01,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [35.2s/35254121076ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:58:24,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.9s/43903ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:58:40,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [43.9s/43902763467ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:58:44,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T03:58:45,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21081285503ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T03:58:47,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@5153c7e1] took [131160ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:58:50,339][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [475308ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [45] unchanged indices
[2022-04-13T03:58:51,719][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [11.4m] publication of cluster state version [9334] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T03:58:58,581][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-13T03:58:58,807][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-13T03:58:58,831][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-13T03:59:03,341][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][236] overhead, spent [440ms] collecting in the last [1.3s]
[2022-04-13T03:59:11,178][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][241] overhead, spent [442ms] collecting in the last [1.1s]
[2022-04-13T03:59:15,942][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][244][12] duration [1s], collections [1]/[1.9s], total [1s]/[2.5s], memory [171.7mb]->[104.7mb]/[2gb], all_pools {[young] [80mb]->[4mb]/[0b]}{[old] [86mb]->[92.7mb]/[2gb]}{[survivor] [9.7mb]->[11.9mb]/[0b]}
[2022-04-13T03:59:16,321][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][244] overhead, spent [1s] collecting in the last [1.9s]
[2022-04-13T03:59:21,909][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-13T03:59:22,093][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][248][14] duration [930ms], collections [1]/[1.5s], total [930ms]/[3.7s], memory [180.7mb]->[105.1mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [95.4mb]->[100mb]/[2gb]}{[survivor] [9.2mb]->[5.1mb]/[0b]}
[2022-04-13T03:59:22,186][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][248] overhead, spent [930ms] collecting in the last [1.5s]
[2022-04-13T03:59:21,909][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-13T03:59:25,775][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][250] overhead, spent [676ms] collecting in the last [2s]
[2022-04-13T03:59:36,583][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [8044ms] which is above the warn threshold of [5000ms]
[2022-04-13T03:59:47,438][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [20238ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [47] unchanged indices
[2022-04-13T03:59:47,306][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][256][16] duration [1s], collections [1]/[1.8s], total [1s]/[5.5s], memory [180.5mb]->[188.5mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [100mb]->[102.2mb]/[2gb]}{[survivor] [8.5mb]->[6.9mb]/[0b]}
[2022-04-13T03:59:47,655][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][256] overhead, spent [1s] collecting in the last [1.8s]
[2022-04-13T03:59:48,005][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [21.3s] publication of cluster state version [9341] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T03:59:50,067][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-13T04:00:00,237][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][264][17] duration [1.7s], collections [1]/[3.3s], total [1.7s]/[7.2s], memory [161.1mb]->[111.5mb]/[2gb], all_pools {[young] [56mb]->[0b]/[0b]}{[old] [102.2mb]->[102.2mb]/[2gb]}{[survivor] [6.9mb]->[9.3mb]/[0b]}
[2022-04-13T04:00:01,375][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][264] overhead, spent [1.7s] collecting in the last [3.3s]
[2022-04-13T04:00:02,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5084ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:00:08,210][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][266][19] duration [1.3s], collections [1]/[2.6s], total [1.3s]/[9.2s], memory [171.5mb]->[198mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [102.2mb]->[106.1mb]/[2gb]}{[survivor] [9.3mb]->[10.2mb]/[0b]}
[2022-04-13T04:00:08,723][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][266] overhead, spent [1.3s] collecting in the last [2.6s]
[2022-04-13T04:00:09,851][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:54768}] took [5023ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:00:34,241][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][285] overhead, spent [564ms] collecting in the last [1.6s]
[2022-04-13T04:00:40,190][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][288][22] duration [1.5s], collections [1]/[3.2s], total [1.5s]/[11.6s], memory [208.9mb]->[133.1mb]/[2gb], all_pools {[young] [84mb]->[0b]/[0b]}{[old] [121.5mb]->[121.5mb]/[2gb]}{[survivor] [3.4mb]->[11.6mb]/[0b]}
[2022-04-13T04:00:40,552][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][288] overhead, spent [1.5s] collecting in the last [3.2s]
[2022-04-13T04:00:43,273][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][289][23] duration [943ms], collections [1]/[3.2s], total [943ms]/[12.5s], memory [133.1mb]->[132mb]/[2gb], all_pools {[young] [0b]->[4mb]/[0b]}{[old] [121.5mb]->[129.7mb]/[2gb]}{[survivor] [11.6mb]->[2.3mb]/[0b]}
[2022-04-13T04:00:43,400][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][289] overhead, spent [943ms] collecting in the last [3.2s]
[2022-04-13T04:00:46,126][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55152}] took [5473ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:00:59,751][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][296][25] duration [2.3s], collections [1]/[5.3s], total [2.3s]/[15.1s], memory [208.7mb]->[212.7mb]/[2gb], all_pools {[young] [72mb]->[88mb]/[0b]}{[old] [129.7mb]->[129.7mb]/[2gb]}{[survivor] [7mb]->[7mb]/[0b]}
[2022-04-13T04:00:59,964][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][296] overhead, spent [2.3s] collecting in the last [5.3s]
[2022-04-13T04:01:04,593][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][299][26] duration [758ms], collections [1]/[2.1s], total [758ms]/[15.9s], memory [210.2mb]->[146.7mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [129.7mb]->[135.6mb]/[2gb]}{[survivor] [12.5mb]->[11.1mb]/[0b]}
[2022-04-13T04:01:05,028][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][299] overhead, spent [758ms] collecting in the last [2.1s]
[2022-04-13T04:01:35,336][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.13] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-13T04:01:51,499][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [46.7s/46777ms] to compute cluster state update for [auto create [logstash-2022.04.13]], which exceeds the warn threshold of [10s]
[2022-04-13T04:02:17,688][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55168}] took [13964ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:02:17,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11927ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:02:18,341][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][330][27] duration [9.6s], collections [1]/[6.6s], total [9.6s]/[25.6s], memory [218.7mb]->[222.7mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [135.6mb]->[141.1mb]/[2gb]}{[survivor] [11.1mb]->[14.6mb]/[0b]}
[2022-04-13T04:02:18,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11927508115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:02:18,800][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][330] overhead, spent [9.6s] collecting in the last [6.6s]
[2022-04-13T04:02:19,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [12527ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:02:23,830][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [27798ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [47] unchanged indices
[2022-04-13T04:02:31,805][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [38.8s] publication of cluster state version [9351] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T04:02:40,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@7fcfd351] took [5171ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:03:19,297][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6433ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:03:20,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6432593570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:03:30,425][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11300ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:03:30,794][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11300053450ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:03:30,820][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@8bcf209] took [17732ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:03:43,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9880ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:03:44,273][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9879690983ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:03:46,995][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][344][28] duration [6.9s], collections [1]/[2.3s], total [6.9s]/[32.5s], memory [219.7mb]->[157.4mb]/[2gb], all_pools {[young] [68mb]->[4mb]/[0b]}{[old] [141.1mb]->[146.9mb]/[2gb]}{[survivor] [14.6mb]->[10.4mb]/[0b]}
[2022-04-13T04:03:51,915][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][344] overhead, spent [6.9s] collecting in the last [2.3s]
[2022-04-13T04:03:53,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [19710ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:04:05,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [7088ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:05:00,861][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7215ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:05:04,388][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7215124448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:05:14,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14633ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:05:36,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14633228835ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:05:39,593][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24342ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:05:49,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.3s/24341169811ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:06:08,454][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:06:09,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@4617f356] took [104052ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:06:11,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15015171754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:06:21,079][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27209ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:06:29,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.2s/27209590723ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:06:31,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10100ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:06:36,986][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [10100ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:06:37,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.1s/10100168455ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:06:38,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7227ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:06:39,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [7226ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:06:40,973][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7226394376ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:07:02,300][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5994ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:07:02,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [14901ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:07:17,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5993929828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:07:19,240][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [16460ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:07:18,704][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16461ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:07:20,944][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.4s/16460868489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:07:36,806][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [8509ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:07:37,050][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:07:39,299][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9428593294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:07:46,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@1a7be0b1] took [5002ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:08:03,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:08:04,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12748993000ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:08:05,659][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][351][29] duration [8.7s], collections [1]/[15.1s], total [8.7s]/[41.3s], memory [201.4mb]->[157.3mb]/[2gb], all_pools {[young] [44mb]->[4mb]/[0b]}{[old] [146.9mb]->[152.2mb]/[2gb]}{[survivor] [10.4mb]->[5.1mb]/[0b]}
[2022-04-13T04:08:06,602][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][351] overhead, spent [8.7s] collecting in the last [15.1s]
[2022-04-13T04:08:07,673][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [16329ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:08:25,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [8569ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:08:36,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6333ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:08:46,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@2fcbad15] took [5430ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:08:58,051][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=237, version=9351}] took [5.8m] which is above the warn threshold of [30s]: [running task [Publication{term=237, version=9351}]] took [0ms], [connecting to new nodes] took [277ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54e87bc8] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@505edde5] took [304517ms], [org.elasticsearch.script.ScriptService@55e3c4de] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [0ms], [org.elasticsearch.snapshots.RestoreService@58ea61a0] took [0ms], [org.elasticsearch.ingest.IngestService@7e7d2ead] took [58ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3573d088] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d6200@218115b9] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2a6d8748] took [0ms], [org.elasticsearch.tasks.TaskManager@3df2dcee] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@162ebc48] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@613d88e2] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7d167f9c] took [0ms], [org.elasticsearch.indices.SystemIndexManager@1e88ce70] took [120ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@788cd0f2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@50d1da72] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@ca906e3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@48c2079b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@306a1be9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@3c8e9e3] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1d5a5d5d] took [36246ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4c035c28] took [1ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1be389ae] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4d5774e6] took [1851ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@22f1a612] took [123ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@126b3627] took [1ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@15c2633d] took [2283ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [91ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@63c4d527] took [4458ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4c2a3e7b] took [36ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@44ce2370] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@56f0886] took [2927ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@21bf71a5] took [618ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@49950e40] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5b04f430] took [81ms], [org.elasticsearch.node.ResponseCollectorService@1ec94ac] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1623dabb] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@77a53131] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@b7cee81] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@60163f24] took [49ms], [org.elasticsearch.indices.store.IndicesStore@53054dcf] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6cdf28d6] took [0ms], [org.elasticsearch.license.LicenseService@33c2caf] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@2b909849] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@505b569a] took [0ms], [org.elasticsearch.gateway.GatewayService@1935bde] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@eef62c1] took [0ms]
[2022-04-13T04:09:00,112][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [8.1m/489446ms] which is longer than the warn threshold of [300000ms]; there are currently [12] pending tasks, the oldest of which has age [8.2m/493845ms]
[2022-04-13T04:09:00,807][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.4:54984}] took [460280ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:09:12,016][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][368][30] duration [1.8s], collections [1]/[4.2s], total [1.8s]/[43.1s], memory [221.3mb]->[159.6mb]/[2gb], all_pools {[young] [72mb]->[0b]/[0b]}{[old] [152.2mb]->[152.2mb]/[2gb]}{[survivor] [5.1mb]->[7.3mb]/[0b]}
[2022-04-13T04:09:12,517][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][368] overhead, spent [1.8s] collecting in the last [4.2s]
[2022-04-13T04:09:15,089][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10848ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [48] unchanged indices
[2022-04-13T04:09:17,929][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][370][31] duration [1.4s], collections [1]/[4s], total [1.4s]/[44.6s], memory [235.6mb]->[164.5mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [152.2mb]->[152.2mb]/[2gb]}{[survivor] [7.3mb]->[12.3mb]/[0b]}
[2022-04-13T04:09:18,012][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][370] overhead, spent [1.4s] collecting in the last [4s]
[2022-04-13T04:09:18,050][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [16.2s] publication of cluster state version [9352] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T04:09:21,220][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][372] overhead, spent [524ms] collecting in the last [1.6s]
[2022-04-13T04:09:29,996][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [10323ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [2] indices and skipped [48] unchanged indices
[2022-04-13T04:09:33,640][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14.2s] publication of cluster state version [9353] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_APPLY_COMMIT]
[2022-04-13T04:09:49,081][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5148ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:09:50,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5147588865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:09:51,700][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][380][33] duration [3.8s], collections [1]/[6.5s], total [3.8s]/[49s], memory [205.8mb]->[169.5mb]/[2gb], all_pools {[young] [40mb]->[0b]/[0b]}{[old] [158.7mb]->[158.7mb]/[2gb]}{[survivor] [7mb]->[10.7mb]/[0b]}
[2022-04-13T04:09:52,431][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][380] overhead, spent [3.8s] collecting in the last [6.5s]
[2022-04-13T04:09:54,084][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [10341ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:10:09,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5073ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:10:16,219][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@4734d925] took [5581ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:10:19,771][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [30865ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [46] unchanged indices
[2022-04-13T04:10:22,929][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [39.4s] publication of cluster state version [9354] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T04:10:38,856][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6106ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:10:55,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [7010ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:11:08,905][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7634ms] which is above the warn threshold of [5s]
[2022-04-13T04:11:29,124][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55272}] took [11607ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:11:55,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15376ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:11:55,668][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55272}] took [17378ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:11:55,747][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@7b5d4c61] took [55147ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:11:55,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15376303459ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:11:56,237][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][393][34] duration [9.5s], collections [1]/[1.1m], total [9.5s]/[58.6s], memory [221.5mb]->[171.7mb]/[2gb], all_pools {[young] [56mb]->[4mb]/[0b]}{[old] [158.7mb]->[163.7mb]/[2gb]}{[survivor] [10.7mb]->[4mb]/[0b]}
[2022-04-13T04:12:20,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:12:21,109][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [8214ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:12:21,352][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5s/5596584053ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:12:21,367][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55276}] took [8414ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:12:21,367][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55278}] took [8414ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:12:42,796][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][405][35] duration [3.6s], collections [1]/[5.4s], total [3.6s]/[1m], memory [243.7mb]->[171.9mb]/[2gb], all_pools {[young] [76mb]->[0b]/[0b]}{[old] [163.7mb]->[163.7mb]/[2gb]}{[survivor] [4mb]->[8.2mb]/[0b]}
[2022-04-13T04:12:43,092][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][405] overhead, spent [3.6s] collecting in the last [5.4s]
[2022-04-13T04:12:43,093][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5209ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:12:48,840][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][406][36] duration [2.4s], collections [1]/[1.7s], total [2.4s]/[1m], memory [171.9mb]->[255.9mb]/[2gb], all_pools {[young] [0b]->[0b]/[0b]}{[old] [163.7mb]->[163.7mb]/[2gb]}{[survivor] [8.2mb]->[6mb]/[0b]}
[2022-04-13T04:12:49,472][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][406] overhead, spent [2.4s] collecting in the last [1.7s]
[2022-04-13T04:12:49,644][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5536ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:13:05,232][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=237, version=9354}] took [2.5m] which is above the warn threshold of [30s]: [running task [Publication{term=237, version=9354}]] took [272ms], [connecting to new nodes] took [63ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54e87bc8] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@505edde5] took [131050ms], [org.elasticsearch.script.ScriptService@55e3c4de] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [0ms], [org.elasticsearch.snapshots.RestoreService@58ea61a0] took [0ms], [org.elasticsearch.ingest.IngestService@7e7d2ead] took [175ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3573d088] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d6200@218115b9] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2a6d8748] took [0ms], [org.elasticsearch.tasks.TaskManager@3df2dcee] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@162ebc48] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@613d88e2] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7d167f9c] took [0ms], [org.elasticsearch.indices.SystemIndexManager@1e88ce70] took [87ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@788cd0f2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@50d1da72] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@ca906e3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@48c2079b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@306a1be9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@3c8e9e3] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1d5a5d5d] took [5317ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4c035c28] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1be389ae] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4d5774e6] took [613ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@22f1a612] took [41ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@126b3627] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@15c2633d] took [1428ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [2527ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@63c4d527] took [7148ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4c2a3e7b] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@44ce2370] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@56f0886] took [1893ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@21bf71a5] took [1796ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@49950e40] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5b04f430] took [283ms], [org.elasticsearch.node.ResponseCollectorService@1ec94ac] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1623dabb] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@77a53131] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@b7cee81] took [90ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@60163f24] took [13ms], [org.elasticsearch.indices.store.IndicesStore@53054dcf] took [101ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6cdf28d6] took [0ms], [org.elasticsearch.license.LicenseService@33c2caf] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@2b909849] took [53ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@505b569a] took [0ms], [org.elasticsearch.gateway.GatewayService@1935bde] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@eef62c1] took [0ms]
[2022-04-13T04:13:32,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [13943ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:13:40,011][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [24.4s/24430ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.03.29][0]], allocationId [ogFN9-ZNSoyo0TcfQ4mNJQ], primary term [55], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.03.29][0]], allocationId [ogFN9-ZNSoyo0TcfQ4mNJQ], primary term [55], message [after existing store recovery; bootstrap_history_uuid=false]}]], which exceeds the warn threshold of [10s]
[2022-04-13T04:14:04,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:14:05,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22s/22057359670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:14:11,309][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][416][37] duration [15.1s], collections [1]/[47.5s], total [15.1s]/[1.3m], memory [249.7mb]->[175.7mb]/[2gb], all_pools {[young] [80mb]->[8mb]/[0b]}{[old] [163.7mb]->[163.7mb]/[2gb]}{[survivor] [6mb]->[12mb]/[0b]}
[2022-04-13T04:14:13,271][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][416] overhead, spent [15.1s] collecting in the last [47.5s]
[2022-04-13T04:14:14,931][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [10825ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:14:33,675][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55284}] took [9163ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:15:03,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@6b28094b] took [41944ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:15:26,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6764ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:15:56,472][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6229ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:15:56,429][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [100059ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [49] unchanged indices
[2022-04-13T04:15:55,610][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [914] timed out after [48083ms]
[2022-04-13T04:16:02,803][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55284}] took [25534ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:15:58,745][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8030ms] which is above the warn threshold of [5s]
[2022-04-13T04:16:05,900][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [1.9m] publication of cluster state version [9355] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T04:16:09,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6644ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:16:50,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27674ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:16:54,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27673274409ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:17:08,874][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:17:09,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [46765ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:17:10,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.8s/17891431404ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:17:19,340][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:17:19,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.7s/10728217832ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:17:20,090][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.2m/137261ms] ago, timed out [1.4m/89178ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [914]
[2022-04-13T04:17:24,580][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55294}] took [5022ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:17:31,108][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=237, version=9355}] took [1.2m] which is above the warn threshold of [30s]: [running task [Publication{term=237, version=9355}]] took [150ms], [connecting to new nodes] took [645ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54e87bc8] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@505edde5] took [65968ms], [org.elasticsearch.script.ScriptService@55e3c4de] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [0ms], [org.elasticsearch.snapshots.RestoreService@58ea61a0] took [0ms], [org.elasticsearch.ingest.IngestService@7e7d2ead] took [824ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3573d088] took [114ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d6200@218115b9] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2a6d8748] took [0ms], [org.elasticsearch.tasks.TaskManager@3df2dcee] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@162ebc48] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@613d88e2] took [3110ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7d167f9c] took [47ms], [org.elasticsearch.indices.SystemIndexManager@1e88ce70] took [526ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@788cd0f2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@50d1da72] took [158ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@ca906e3] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@48c2079b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@306a1be9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@3c8e9e3] took [90ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1d5a5d5d] took [2323ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4c035c28] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1be389ae] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4d5774e6] took [403ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@22f1a612] took [52ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@126b3627] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@15c2633d] took [49ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [54ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@63c4d527] took [30ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4c2a3e7b] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@44ce2370] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@56f0886] took [1ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@21bf71a5] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@49950e40] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5b04f430] took [82ms], [org.elasticsearch.node.ResponseCollectorService@1ec94ac] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1623dabb] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@77a53131] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@b7cee81] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@60163f24] took [40ms], [org.elasticsearch.indices.store.IndicesStore@53054dcf] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6cdf28d6] took [0ms], [org.elasticsearch.license.LicenseService@33c2caf] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@2b909849] took [40ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@505b569a] took [0ms], [org.elasticsearch.gateway.GatewayService@1935bde] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@eef62c1] took [0ms]
[2022-04-13T04:17:45,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6099ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:17:45,479][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:55294}] took [5201ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:17:45,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [6499ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:17:46,590][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6099373244ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:17:48,456][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][428][38] duration [3.6s], collections [1]/[10.6s], total [3.6s]/[1.3m], memory [255.7mb]->[181.9mb]/[2gb], all_pools {[young] [80mb]->[0b]/[0b]}{[old] [163.7mb]->[167.4mb]/[2gb]}{[survivor] [12mb]->[14.4mb]/[0b]}
[2022-04-13T04:17:51,663][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][428] overhead, spent [3.6s] collecting in the last [10.6s]
[2022-04-13T04:17:52,405][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [7089ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:18:06,033][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11031ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [47] unchanged indices
[2022-04-13T04:18:11,900][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [17.8s] publication of cluster state version [9356] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T04:18:24,827][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][438][39] duration [2.3s], collections [1]/[4.4s], total [2.3s]/[1.4m], memory [233.9mb]->[182.9mb]/[2gb], all_pools {[young] [52mb]->[8mb]/[0b]}{[old] [167.4mb]->[175.1mb]/[2gb]}{[survivor] [14.4mb]->[7.8mb]/[0b]}
[2022-04-13T04:18:25,132][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][438] overhead, spent [2.3s] collecting in the last [4.4s]
[2022-04-13T04:18:42,705][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11286ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [46] unchanged indices
[2022-04-13T04:18:44,054][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [13.9s] publication of cluster state version [9357] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T04:18:45,977][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][450] overhead, spent [434ms] collecting in the last [1.3s]
[2022-04-13T04:18:58,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10072ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:18:59,470][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10072576173ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:18:59,472][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][452][41] duration [5.6s], collections [1]/[11.1s], total [5.6s]/[1.5m], memory [234.4mb]->[188.6mb]/[2gb], all_pools {[young] [64mb]->[4mb]/[0b]}{[old] [175.1mb]->[180.1mb]/[2gb]}{[survivor] [11.3mb]->[8.5mb]/[0b]}
[2022-04-13T04:18:59,629][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][452] overhead, spent [5.6s] collecting in the last [11.1s]
[2022-04-13T04:18:59,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [10072ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:19:44,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@3351025d] took [41301ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:20:04,653][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [1096] timed out after [20012ms]
[2022-04-13T04:20:07,055][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [22s/22013ms] ago, timed out [2s/2001ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{wsdTPMlrTYax3mSrizmRCA}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [1096]
[2022-04-13T04:20:13,883][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:20:32,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [6403ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:20:45,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10429ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:20:45,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [10428ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:20:45,649][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10428665369ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:20:47,037][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][461][42] duration [6.2s], collections [1]/[20.5s], total [6.2s]/[1.6m], memory [256.6mb]->[191.2mb]/[2gb], all_pools {[young] [68mb]->[8mb]/[0b]}{[old] [180.1mb]->[181.3mb]/[2gb]}{[survivor] [8.5mb]->[5.8mb]/[0b]}
[2022-04-13T04:20:47,456][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][461] overhead, spent [6.2s] collecting in the last [20.5s]
[2022-04-13T04:20:48,189][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=237, version=9357}] took [2m] which is above the warn threshold of [30s]: [running task [Publication{term=237, version=9357}]] took [0ms], [connecting to new nodes] took [29ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@54e87bc8] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@505edde5] took [100176ms], [org.elasticsearch.script.ScriptService@55e3c4de] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [0ms], [org.elasticsearch.snapshots.RestoreService@58ea61a0] took [0ms], [org.elasticsearch.ingest.IngestService@7e7d2ead] took [3148ms], [org.elasticsearch.action.ingest.IngestActionForwarder@3573d088] took [69ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d6200@218115b9] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2a6d8748] took [71ms], [org.elasticsearch.tasks.TaskManager@3df2dcee] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@162ebc48] took [128ms], [org.elasticsearch.cluster.InternalClusterInfoService@613d88e2] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@7d167f9c] took [139ms], [org.elasticsearch.indices.SystemIndexManager@1e88ce70] took [1997ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@788cd0f2] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x00000008013094e8@50d1da72] took [136ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@ca906e3] took [137ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@48c2079b] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140ac08@306a1be9] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@3c8e9e3] took [101ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@1d5a5d5d] took [15569ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@4c035c28] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@1be389ae] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@4d5774e6] took [256ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@22f1a612] took [211ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@126b3627] took [1ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@15c2633d] took [292ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@4fcb65aa] took [42ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@63c4d527] took [419ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@4c2a3e7b] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@44ce2370] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@56f0886] took [35ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@21bf71a5] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@49950e40] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@5b04f430] took [57ms], [org.elasticsearch.node.ResponseCollectorService@1ec94ac] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1623dabb] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@77a53131] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@b7cee81] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@60163f24] took [24ms], [org.elasticsearch.indices.store.IndicesStore@53054dcf] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@6cdf28d6] took [0ms], [org.elasticsearch.license.LicenseService@33c2caf] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@2b909849] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@505b569a] took [0ms], [org.elasticsearch.gateway.GatewayService@1935bde] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@eef62c1] took [0ms]
[2022-04-13T04:20:53,391][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][464][43] duration [1.4s], collections [1]/[3.1s], total [1.4s]/[1.6m], memory [251.2mb]->[197.1mb]/[2gb], all_pools {[young] [72mb]->[4mb]/[0b]}{[old] [181.3mb]->[181.3mb]/[2gb]}{[survivor] [5.8mb]->[11.8mb]/[0b]}
[2022-04-13T04:20:54,131][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][464] overhead, spent [1.4s] collecting in the last [3.1s]
[2022-04-13T04:21:07,757][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][473][44] duration [857ms], collections [1]/[2.2s], total [857ms]/[1.6m], memory [261.1mb]->[202.7mb]/[2gb], all_pools {[young] [68mb]->[16mb]/[0b]}{[old] [181.3mb]->[186.7mb]/[2gb]}{[survivor] [11.8mb]->[16mb]/[0b]}
[2022-04-13T04:21:08,017][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][473] overhead, spent [857ms] collecting in the last [2.2s]
[2022-04-13T04:21:22,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@692dc6ee, interval=5s}] took [11383ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:22:41,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6011ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:25:14,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9s/5917465342ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:25:46,632][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/208864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:26:22,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [209013ms] which is above the warn threshold of [5000ms]
[2022-04-13T04:26:19,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.4m/209013628828ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:27:20,731][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69559ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:27:41,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/69558826669ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:28:12,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:29:34,769][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/72505766021ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:30:54,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/157589ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:31:36,572][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.6m/157708001570ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:32:57,589][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/129189ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:35:02,964][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/129189018816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:38:00,228][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304292ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:40:24,914][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304292625521ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:42:53,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/281417ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:46:08,398][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.6m/281026732425ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:49:33,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/409530ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:52:59,121][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/409615010840ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T04:57:04,666][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/445002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T04:59:59,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/444835845301ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T05:02:37,185][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [41.3m/2479497ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-13T05:03:29,419][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389858ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T05:06:16,559][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389695942356ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T05:09:21,778][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/353952ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T05:12:43,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.9m/354189487083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T05:14:53,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5095/0x00000008017edd70@6369afbe] took [1879363ms] which is above the warn threshold of [5000ms]
[2022-04-13T05:16:12,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/408764ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T05:19:18,828][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/408800120253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:08:05,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.5m/3090430ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T06:11:19,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.5m/3090339545614ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:14:20,019][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389914ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T06:19:00,375][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/390231903099ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:21:32,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/441002ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T06:24:26,135][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3m/440692336453ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:27:00,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320095ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T06:29:25,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320324341215ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:31:54,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/303571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T06:34:19,143][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/303783542949ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:37:42,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/346275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T06:40:40,788][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7m/345789659781ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:43:14,566][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/332571ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T06:45:55,616][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.5m/332625840905ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:49:32,946][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/377898ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T06:52:16,831][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/377784553433ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:54:52,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/310817ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T06:57:29,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/310954876867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T06:55:53,742][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [688739ms] which is above the warn threshold of [5s]
[2022-04-13T07:03:00,886][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2m/496773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:05:07,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@2ae8019a, interval=1s}] took [496650ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:05:37,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.2m/496650764682ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:08:19,310][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/318645ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:12:27,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/319041957547ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:14:57,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:16:37,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@82eae97, interval=5s}] took [717480ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:17:38,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/398438939791ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:20:15,257][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/315850ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:23:19,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2m/316261772156ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:29:57,486][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-13T07:29:57,502][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-13T07:29:57,503][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-13T07:30:02,217][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-13T07:30:02,220][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-13T07:30:02,221][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-13T07:30:02,221][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-13T07:30:02,222][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-13T07:30:02,222][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-13T07:30:02,222][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-13T07:30:02,224][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-13T07:30:02,224][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-13T07:30:02,224][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-13T07:30:02,225][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-13T07:30:02,225][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-13T07:30:02,226][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-13T07:30:02,226][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-13T07:30:02,227][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-13T07:30:02,227][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-13T07:30:02,227][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-13T07:30:02,228][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-13T07:30:02,228][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-13T07:30:02,229][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-13T07:30:02,229][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-13T07:30:02,229][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-13T07:30:02,230][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-13T07:30:02,230][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-13T07:30:02,231][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-13T07:30:02,231][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-13T07:30:02,231][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-13T07:30:02,232][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-13T07:30:02,232][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-13T07:30:02,232][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-13T07:30:02,233][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-13T07:30:02,233][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-13T07:30:02,233][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-13T07:30:02,234][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-13T07:30:02,234][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-13T07:30:02,235][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-13T07:30:02,235][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-13T07:30:02,235][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-13T07:30:02,236][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-13T07:30:02,236][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-13T07:30:02,237][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-13T07:30:02,237][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-13T07:30:02,237][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-13T07:30:02,238][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-13T07:30:02,238][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-13T07:30:02,239][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-13T07:30:02,239][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-13T07:30:02,239][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-13T07:30:02,240][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-13T07:30:02,240][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-13T07:30:02,240][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-13T07:30:02,241][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-13T07:30:02,241][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-13T07:30:02,241][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-13T07:30:02,242][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-13T07:30:02,242][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-13T07:30:02,242][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-13T07:30:02,243][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-13T07:30:02,244][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-13T07:30:02,319][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.5gb], net total_space [125.8gb], types [ext4]
[2022-04-13T07:30:02,319][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-13T07:30:02,759][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-13T07:30:13,203][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-13T07:30:13,214][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-13T07:30:13,217][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-13T07:30:13,219][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-13T07:30:13,220][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-13T07:30:13,221][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-13T07:30:13,223][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-13T07:30:13,224][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-13T07:30:13,224][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-13T07:30:13,225][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-13T07:30:13,225][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-13T07:30:13,226][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-13T07:30:13,227][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-13T07:30:13,228][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-13T07:30:13,228][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-13T07:30:14,474][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-13T07:30:14,664][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-13T07:30:15,783][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-13T07:30:17,024][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-13T07:30:17,037][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-13T07:30:17,206][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-13T07:30:17,213][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-13T07:30:17,677][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-13T07:36:41,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378453ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:36:41,833][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][4][8] duration [759ms], collections [1]/[1.6s], total [759ms]/[964ms], memory [237.9mb]->[66.3mb]/[2gb], all_pools {[young] [180mb]->[0b]/[0b]}{[old] [11.3mb]->[46.3mb]/[2gb]}{[survivor] [46.5mb]->[20mb]/[0b]}
[2022-04-13T07:36:42,652][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/378452897981ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:36:42,683][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][4] overhead, spent [759ms] collecting in the last [1.6s]
[2022-04-13T07:36:42,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [379259ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:37:34,604][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][30][9] duration [2.6s], collections [1]/[7.4s], total [2.6s]/[3.6s], memory [138.3mb]->[73mb]/[2gb], all_pools {[young] [76mb]->[4mb]/[0b]}{[old] [46.3mb]->[65mb]/[2gb]}{[survivor] [20mb]->[4mb]/[0b]}
[2022-04-13T07:37:35,570][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][30] overhead, spent [2.6s] collecting in the last [7.4s]
[2022-04-13T07:37:45,062][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [7283ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:38:20,803][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [7683ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:38:30,878][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [5670ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:38:44,549][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [7715ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:39:08,948][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [5603ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:40:20,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [50699ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:40:22,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6891ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:40:41,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6890936920ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:40:51,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:40:52,102][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@20c8695f] took [42689ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:40:55,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [42.6s/42689874866ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:40:59,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:41:03,790][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [8139ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:41:03,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8139830051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:41:06,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6681ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:41:08,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6681510130ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:41:11,319][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [5010ms] which is above the warn threshold of [5s]
[2022-04-13T07:41:15,240][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][49] overhead, spent [517ms] collecting in the last [1.9s]
[2022-04-13T07:41:27,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [7671ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:41:35,274][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][57] overhead, spent [425ms] collecting in the last [1.2s]
[2022-04-13T07:42:26,321][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [17554ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:43:15,383][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [17994ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:44:01,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [13712ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:44:11,030][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [11407ms] which is above the warn threshold of [5s]
[2022-04-13T07:44:25,292][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [8676ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:44:57,823][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [195442ms] which is above the warn threshold of [10s]; wrote full state with [50] indices
[2022-04-13T07:47:12,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [118974ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:49:42,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@529dd0fe, interval=5s}] took [38115ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:51:28,496][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@115f3411, interval=5s}] took [23367ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:51:27,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14159ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:52:51,364][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14158962218ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T07:53:45,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/129359ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T07:56:45,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [129358ms] which is above the warn threshold of [5000ms]
[2022-04-13T07:56:45,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.1m/129358582083ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:02:53,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2m/553060ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:04:10,221][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@529dd0fe, interval=5s}] took [552527ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:05:59,751][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2m/552527318865ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:08:11,456][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:08:17,497][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@20c8695f] took [321148ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:09:43,853][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/321148355008ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:10:51,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/162747ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:11:52,157][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/162621832187ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:11:53,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [162621ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:12:39,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:14:42,791][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.7m/107706656472ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:16:30,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/228917ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:18:38,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/228768114553ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:18:53,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [228768ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:20:23,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/234517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:21:33,934][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.9m/234666244435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:19:53,512][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [228768ms] which is above the warn threshold of [5s]
[2022-04-13T08:22:28,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121827ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:23:23,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/121826944047ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:23:43,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [121826ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:24:41,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138556ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:25:56,827][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/138555354221ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:27:11,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/151508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:27:37,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/151507981978ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:28:03,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.5s/51595ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:28:21,920][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [203103ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:28:26,800][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.5s/51595175593ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:28:55,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:29:00,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@529dd0fe, interval=5s}] took [51775ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:29:25,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.7s/51775811067ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:29:53,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.4s/57407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:30:26,120][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.4s/57407216464ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:30:49,112][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.1s/56178ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:31:12,098][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [56.1s/56178055652ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:31:12,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [56178ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:31:37,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48725ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:32:10,335][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [48.7s/48725005822ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:31:03,459][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [56178ms] which is above the warn threshold of [5s]
[2022-04-13T08:32:31,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.3s/54322ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:33:11,390][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.3s/54322135384ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:33:55,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81223ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:34:09,117][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [81222ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:37:58,517][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81222363208ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:48:42,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7m/884440ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:49:27,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@529dd0fe, interval=5s}] took [884066ms] which is above the warn threshold of [5000ms]
[2022-04-13T08:52:33,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7m/884066986169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:55:34,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/411737ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T08:56:36,264][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [411831ms] which is above the warn threshold of [5s]
[2022-04-13T08:58:41,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8m/411831268683ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T08:59:56,119][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [411831ms] which is above the warn threshold of [5000ms]
[2022-04-13T09:02:07,659][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T09:05:29,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.3m/379537865341ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T09:08:39,601][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T09:08:43,545][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@20c8695f] took [399224ms] which is above the warn threshold of [5000ms]
[2022-04-13T09:11:30,891][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/399224166364ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T09:14:27,349][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352348ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T09:16:22,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [352573ms] which is above the warn threshold of [5000ms]
[2022-04-13T09:17:21,114][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/352573789335ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T09:20:35,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/368387ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T09:21:25,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@529dd0fe, interval=5s}] took [367916ms] which is above the warn threshold of [5000ms]
[2022-04-13T09:23:45,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/367916295720ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T09:27:32,554][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/396548ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T09:29:33,508][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [397200ms] which is above the warn threshold of [5s]
[2022-04-13T09:31:36,501][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6m/397199640435ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T09:31:30,812][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [397199ms] which is above the warn threshold of [5000ms]
[2022-04-13T09:35:47,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/514956ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T09:38:49,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.5m/514525009264ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T09:42:16,190][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/388937ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T09:44:00,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@79d3479b, interval=1s}] took [389225ms] which is above the warn threshold of [5000ms]
[2022-04-13T09:45:33,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4m/389225769415ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T09:48:52,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/395649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T09:49:02,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@115f3411, interval=5s}] took [395339ms] which is above the warn threshold of [5000ms]
[2022-04-13T09:54:18,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5m/395339351910ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T09:57:35,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.6m/520375ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:02:00,061][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-13T10:02:00,076][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-13T10:02:00,077][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-13T10:02:04,868][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-13T10:02:04,871][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-13T10:02:04,872][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-13T10:02:04,872][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-13T10:02:04,873][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-13T10:02:04,873][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-13T10:02:04,874][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-13T10:02:04,874][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-13T10:02:04,875][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-13T10:02:04,875][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-13T10:02:04,875][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-13T10:02:04,876][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-13T10:02:04,876][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-13T10:02:04,877][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-13T10:02:04,877][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-13T10:02:04,877][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-13T10:02:04,878][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-13T10:02:04,878][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-13T10:02:04,878][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-13T10:02:04,878][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-13T10:02:04,879][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-13T10:02:04,879][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-13T10:02:04,879][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-13T10:02:04,880][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-13T10:02:04,880][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-13T10:02:04,880][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-13T10:02:04,881][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-13T10:02:04,881][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-13T10:02:04,881][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-13T10:02:04,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-13T10:02:04,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-13T10:02:04,882][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-13T10:02:04,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-13T10:02:04,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-13T10:02:04,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-13T10:02:04,883][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-13T10:02:04,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-13T10:02:04,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-13T10:02:04,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-13T10:02:04,884][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-13T10:02:04,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-13T10:02:04,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-13T10:02:04,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-13T10:02:04,885][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-13T10:02:04,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-13T10:02:04,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-13T10:02:04,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-13T10:02:04,886][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-13T10:02:04,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-13T10:02:04,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-13T10:02:04,887][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-13T10:02:04,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-13T10:02:04,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-13T10:02:04,888][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-13T10:02:04,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-13T10:02:04,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-13T10:02:04,889][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-13T10:02:04,890][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-13T10:02:04,891][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-13T10:02:04,968][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [105.6gb], net total_space [125.8gb], types [ext4]
[2022-04-13T10:02:04,969][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-13T10:02:05,410][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-13T10:02:15,655][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-13T10:02:15,661][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-13T10:02:16,865][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-13T10:02:17,043][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-13T10:02:17,889][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-13T10:02:37,793][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-13T10:02:37,841][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-13T10:02:38,337][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-13T10:02:38,338][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-13T10:02:38,638][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-13T10:07:09,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/265662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:07:10,132][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/265662126204ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:07:27,329][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-13T10:08:09,137][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [11407ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:08:58,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [25786ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:09:02,945][WARN ][o.e.n.Node               ] [tpotcluster-node-01] timed out while waiting for initial discovery state - timeout: 30s
[2022-04-13T10:09:51,208][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [28685ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:10:56,526][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [33084ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:11:08,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12774ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:10:34,281][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [16273ms] which is above the warn threshold of [5s]
[2022-04-13T10:11:18,385][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12773794330ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:11:21,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26093ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:11:25,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [26092ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:11:24,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [26s/26092684439ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:11:29,443][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7578ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:11:35,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7578681403ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:11:40,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:11:49,548][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [11395ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:11:47,795][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11395669241ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:11:55,859][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14534ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:02,571][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.5s/14533741768ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:09,473][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13902ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:11,690][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [13901ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:12:15,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.9s/13901885773ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:20,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@633b0ff6, interval=5s}] took [10933ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:12:20,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10933ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:23,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.9s/10933553950ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:27,069][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7142ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:30,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.1s/7141823507ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:31,615][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [7141ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:12:34,570][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7569ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:39,264][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7568971642ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:42,207][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7746ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:12:43,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [7745ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:12:44,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7745880411ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:13:24,911][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [5.8m/351050ms] to compute cluster state update for [elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{5ahgUaWTQ96g8Em6EUZqbQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_]], which exceeds the warn threshold of [10s]
[2022-04-13T10:13:28,684][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{5ahgUaWTQ96g8Em6EUZqbQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 241, version: 9360, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{5ahgUaWTQ96g8Em6EUZqbQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-13T10:13:45,699][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-13T10:13:47,481][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-13T10:14:00,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6238ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:02,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6237536868ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:05,980][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [7238ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:14:08,262][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7773ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:15,485][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7s/7772764075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:20,384][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12120ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:25,963][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.1s/12119922496ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:34,049][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:36,460][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [26123ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:14:40,996][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14003911181ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:49,877][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16073ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:52,954][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16s/16072175320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:54,074][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [16072ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:14:55,971][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6173ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:14:58,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6173352609ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:06,268][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10097ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:14,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10s/10096673954ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:15,382][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [10096ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:15:19,994][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13748ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:25,356][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13748102370ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:30,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10666ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:36,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10666297118ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:39,777][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [10666ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:15:41,514][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10521ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:48,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.5s/10520707049ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:54,753][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:15:56,820][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [13537ms] which is above the warn threshold of [5s]
[2022-04-13T10:15:58,402][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.5s/13537062964ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:16:04,712][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [23813ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:16:04,764][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10277ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:16:07,879][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10276667262ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:16:10,177][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.indices.IndicesService$CacheCleaner@cd9eeb3] took [5456ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:16:10,055][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5456ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:16:11,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4s/5456262385ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:16:21,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [6998ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:17:02,386][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [12193ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:17:49,003][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [26742ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:19:39,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [77232ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:22:24,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@39774750, interval=30s}] took [10048ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:22:28,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101659ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:23:01,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.6m/101658870476ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:23:32,639][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64802ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:26:46,021][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6.9m] publication of cluster state version [9360] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{5ahgUaWTQ96g8Em6EUZqbQ}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-13T10:26:46,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64801894577ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:27:17,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/225840ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:30:18,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/225840112875ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:30:51,655][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [225840ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:30:54,199][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/215906ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:27:21,337][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [64802ms] which is above the warn threshold of [5s]
[2022-04-13T10:31:25,560][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.5m/215906014312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:31:45,400][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@79550bad, interval=5s}] took [52465ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:31:45,924][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:32:08,490][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [52.4s/52465377155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:32:34,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45284ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:33:09,403][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [45.2s/45283663742ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:33:32,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61701ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:33:44,012][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [106985ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:34:42,141][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/61701507820ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:38:01,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/265155ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:38:47,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.4m/265154433398ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:38:28,881][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [265154ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:39:32,860][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91565ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:40:14,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91564992895ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:42:50,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/200055ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:42:55,566][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [200055ms] which is above the warn threshold of [5s]
[2022-04-13T10:44:04,147][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.3m/200054777848ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:44:45,256][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [200054ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:45:13,263][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/141145ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:46:21,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/141145249725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:47:13,849][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119336ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:48:39,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/119336523732ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:49:37,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/142003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:49:43,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [142002ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:50:59,738][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.3m/142002492385ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:51:33,750][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114242ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:52:14,597][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/114241826037ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:52:49,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:53:20,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/81925801932ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:53:45,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [81925ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:54:15,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82407ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:53:30,994][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [81925ms] which is above the warn threshold of [5s]
[2022-04-13T10:54:48,247][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82406657376ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:55:33,584][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82461ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:56:15,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.3m/82460971714ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:56:44,990][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70533ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T10:56:49,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [152994ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:57:13,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.1m/70533051397ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T10:57:54,762][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@633b0ff6, interval=5s}] took [57615ms] which is above the warn threshold of [5000ms]
[2022-04-13T10:57:42,848][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57616ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:00:53,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [57.6s/57615483177ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:01:34,998][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/233043ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:02:38,314][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.8m/233043575933ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:03:08,106][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91636ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:03:10,170][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [91635ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:03:33,043][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.5m/91635917451ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:04:17,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73060ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:04:19,717][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@3abace42, interval=1m}] took [73060ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:06:29,090][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.2m/73060183741ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:07:05,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/165622ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:07:07,418][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [238683ms] which is above the warn threshold of [5s]
[2022-04-13T11:07:58,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.7m/165622197064ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:08:30,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [165622ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:09:07,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:10:29,782][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/122404002990ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:11:03,833][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113425ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:11:39,427][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/113424940754ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:13:57,021][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108395ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:14:20,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.8m/108394629816ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:14:49,175][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120953ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:15:10,234][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [120953ms] which is above the warn threshold of [5s]
[2022-04-13T11:15:14,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2m/120952881955ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:16:04,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [184893ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:15:55,167][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63941ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:16:31,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63940967441ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:16:57,916][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63793ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:16:57,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@79550bad, interval=5s}] took [63793ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:18:09,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/63793479630ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:19:39,642][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/151557ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:20:52,650][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [151557ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:20:49,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.5m/151557065994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:21:30,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118392ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:21:53,301][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@633b0ff6, interval=5s}] took [118391ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:22:55,779][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1.9m/118391754314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:24:27,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173472ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:26:19,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.8m/173471698776ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:26:45,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [173471ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:27:39,629][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [173472ms] which is above the warn threshold of [5s]
[2022-04-13T11:28:19,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/226081ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:30:31,709][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [3.7m/226081066880ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:33:07,260][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/286095ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:34:53,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [286094ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:35:40,528][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.7m/286094534450ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:37:48,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/289884ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:38:17,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@633b0ff6, interval=5s}] took [289562ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:39:57,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/289562168997ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:42:00,485][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@39745382, interval=1m}] took [252049ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:42:07,607][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.1m/251728ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:44:31,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.2m/252049991285ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T11:47:39,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325330ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T11:48:53,223][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [324887ms] which is above the warn threshold of [5s]
[2022-04-13T11:50:46,562][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [324887ms] which is above the warn threshold of [5000ms]
[2022-04-13T11:54:58,850][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/324887683874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T12:06:50,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [1119054ms] which is above the warn threshold of [5000ms]
[2022-04-13T12:06:19,462][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6m/1118783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T12:17:01,510][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.6m/1119054469375ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T12:34:57,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7m/1724745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T12:35:12,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@39774750, interval=30s}] took [1724916ms] which is above the warn threshold of [5000ms]
[2022-04-13T12:38:45,950][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [28.7m/1724916587155ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T12:42:43,685][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/457072ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T12:43:07,211][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [2181401ms] which is above the warn threshold of [5s]
[2022-04-13T12:46:10,281][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.6m/456484186664ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T12:47:11,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [456484ms] which is above the warn threshold of [5000ms]
[2022-04-13T12:50:11,001][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/449536ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T12:55:16,133][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/449627203755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T12:59:27,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/546776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T13:01:57,165][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [547272ms] which is above the warn threshold of [5000ms]
[2022-04-13T13:03:18,495][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.1m/547272656056ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T13:07:05,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/467399ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T13:07:28,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@633b0ff6, interval=5s}] took [466729ms] which is above the warn threshold of [5000ms]
[2022-04-13T13:11:25,989][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.7m/466729671994ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-13T13:15:35,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/508288ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-13T13:17:09,760][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [508790ms] which is above the warn threshold of [5s]
[2022-04-13T13:19:36,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@7f9086d1, interval=1s}] took [508789ms] which is above the warn threshold of [5000ms]
[2022-04-13T13:19:57,945][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4m/508789969892ns] on relative clock which is above the warn threshold of [5000ms]
