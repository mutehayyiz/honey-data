[2022-04-15T16:28:58,859][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-15T16:28:58,879][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-15T16:28:58,880][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-15T16:29:06,442][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-15T16:29:06,443][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-15T16:29:06,444][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-15T16:29:06,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-15T16:29:06,445][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-15T16:29:06,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-15T16:29:06,446][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-15T16:29:06,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-15T16:29:06,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-15T16:29:06,447][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-15T16:29:06,448][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-15T16:29:06,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-15T16:29:06,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-15T16:29:06,449][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-15T16:29:06,450][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-15T16:29:06,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-15T16:29:06,451][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-15T16:29:06,452][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-15T16:29:06,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-15T16:29:06,453][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-15T16:29:06,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-15T16:29:06,454][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-15T16:29:06,455][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-15T16:29:06,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-15T16:29:06,456][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-15T16:29:06,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-15T16:29:06,457][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-15T16:29:06,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-15T16:29:06,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-15T16:29:06,458][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-15T16:29:06,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-15T16:29:06,459][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-15T16:29:06,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-15T16:29:06,460][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-15T16:29:06,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-15T16:29:06,461][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-15T16:29:06,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-15T16:29:06,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-15T16:29:06,462][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-15T16:29:06,463][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-15T16:29:06,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-15T16:29:06,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-15T16:29:06,464][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-15T16:29:06,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-15T16:29:06,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-15T16:29:06,465][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-15T16:29:06,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-15T16:29:06,466][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-15T16:29:06,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-15T16:29:06,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-15T16:29:06,467][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-15T16:29:06,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-15T16:29:06,468][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-15T16:29:06,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-15T16:29:06,469][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-15T16:29:06,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-15T16:29:06,470][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-15T16:29:06,471][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-15T16:29:06,472][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-15T16:29:06,571][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.4gb], net total_space [125.8gb], types [ext4]
[2022-04-15T16:29:06,572][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-15T16:29:06,938][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-15T16:29:19,789][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-15T16:29:19,792][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-15T16:29:20,870][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-15T16:29:20,996][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-15T16:29:21,710][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-15T16:29:22,464][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-15T16:29:22,465][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-15T16:29:22,496][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-15T16:29:22,497][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-15T16:29:22,683][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-15T16:29:25,138][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-15T16:29:25,256][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tvlTHsT-R0OTJNm1Xk-avw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 249, version: 9743, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tvlTHsT-R0OTJNm1Xk-avw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-15T16:29:25,475][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{tvlTHsT-R0OTJNm1Xk-avw}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 249, version: 9743, reason: Publication{term=249, version=9743}
[2022-04-15T16:29:25,590][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-15T16:29:25,590][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-15T16:29:26,281][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-15T16:29:26,287][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [51] indices into cluster_state
[2022-04-15T16:29:26,993][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-15T16:29:26,995][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-15T16:29:27,876][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-ASN.mmdb]
[2022-04-15T16:29:28,196][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T16:29:28,201][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T16:29:28,202][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T16:29:29,050][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T16:29:29,149][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T16:29:31,540][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T16:29:31,592][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-ASN.mmdb]
[2022-04-15T16:29:31,614][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-City.mmdb]
[2022-04-15T16:29:32,208][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T16:29:32,212][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T16:29:32,409][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T16:29:39,657][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T16:29:39,712][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-City.mmdb]
[2022-04-15T16:29:39,715][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip database [GeoLite2-Country.mmdb]
[2022-04-15T16:29:40,523][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T16:29:40,533][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updated geoip database [GeoLite2-Country.mmdb]
[2022-04-15T16:29:40,734][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T16:29:40,740][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T16:29:42,103][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [tpotcluster-node-01] evicted [0] entries from cache after reloading database [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T16:29:42,103][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T16:29:43,761][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[logstash-2022.04.14][0]]]).
[2022-04-15T16:29:46,738][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] removing template [logstash]
[2022-04-15T16:29:46,966][INFO ][o.e.c.m.MetadataIndexTemplateService] [tpotcluster-node-01] adding template [logstash] for index patterns [logstash-*]
[2022-04-15T16:30:31,251][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 1033 finished with response BulkByScrollResponse[took=180.4ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-15T16:30:33,246][INFO ][o.e.t.LoggingTaskListener] [tpotcluster-node-01] 1062 finished with response BulkByScrollResponse[took=1.8s,timed_out=false,sliceId=null,updated=923,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2022-04-15T16:30:41,334][INFO ][o.e.x.i.a.TransportPutLifecycleAction] [tpotcluster-node-01] updating index lifecycle policy [.alerts-ilm-policy]
[2022-04-15T16:31:02,632][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-2022.04.15] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-15T16:31:02,832][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-2022.04.15][0]]]).
[2022-04-15T16:31:02,961][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,059][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,075][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,164][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,265][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,561][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,634][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:03,713][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:14,461][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:18,315][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:25,484][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:25,624][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:31:58,278][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [21021ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:31:58,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:56200}] took [17702ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:32:45,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [13619ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:34:39,752][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [50698ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:34:44,857][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:39,670][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5052873314ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,007][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.8m/110289ms] to compute cluster state update for [put-mapping [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ][_doc]], which exceeds the warn threshold of [10s]
[2022-04-15T16:34:45,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.9s/59935ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,131][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.9s/59935170929ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7c2b5886, interval=5s}] took [59935ms] which is above the warn threshold of [5000ms]
[2022-04-15T16:34:45,370][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [60136ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T16:34:45,577][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][127][15] duration [2.5s], collections [1]/[1.8m], total [2.5s]/[3.4s], memory [621.2mb]->[289.2mb]/[2gb], all_pools {[young] [372mb]->[32mb]/[0b]}{[old] [179.2mb]->[223.2mb]/[2gb]}{[survivor] [74mb]->[34mb]/[0b]}
[2022-04-15T16:34:45,715][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,050][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,320][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,471][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,661][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,748][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,845][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:50,947][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:51,028][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:51,977][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:54,992][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:55,066][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:34:55,710][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:02,996][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:21,593][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:29,601][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:35:45,108][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:37:02,824][INFO ][o.e.c.m.MetadataDeleteIndexService] [tpotcluster-node-01] [logstash-1970.01.01/BByN9-ZNSay2Jx7Y4INJ4w] deleting index
[2022-04-15T16:37:57,040][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:40:57,767][INFO ][o.e.c.m.MetadataCreateIndexService] [tpotcluster-node-01] [logstash-1970.01.01] creating index, cause [auto(bulk api)], templates [logstash], shards [1]/[0]
[2022-04-15T16:40:57,861][INFO ][o.e.c.r.a.AllocationService] [tpotcluster-node-01] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[logstash-1970.01.01][0]]]).
[2022-04-15T16:40:57,918][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-1970.01.01/768VpBTdRaq7sY8TL1VTBA] update_mapping [_doc]
[2022-04-15T16:41:16,407][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:41:16,889][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:41:17,421][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:45:09,590][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:46:27,647][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:47:18,011][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:48:39,762][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:49:53,858][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:49:54,805][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:50:55,859][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:50:55,920][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:51:04,952][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:53:35,049][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:54:20,231][INFO ][o.e.c.m.MetadataMappingService] [tpotcluster-node-01] [logstash-2022.04.15/wB7dhNWZRN-l0aVlm7ztBQ] update_mapping [_doc]
[2022-04-15T16:57:28,804][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@57eecd65, interval=1s}] took [39778ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:02:44,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6s/5608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T16:57:27,241][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:47982}] took [20072ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:09:27,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.7s/5738986263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:11:57,523][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7m/827710ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:14:44,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7m/827351886048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:17:02,448][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292514ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:19:35,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/292872190294ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:23:19,887][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320797ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:26:33,567][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/320186177290ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:27:16,116][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@7c2b5886, interval=5s}] took [320186ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:29:25,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433743ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:32:46,736][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2m/433852585741ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:35:27,440][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361509ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:38:45,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6m/361533922365ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:39:35,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5106/0x00000008017e7548@229537ec] took [361533ms] which is above the warn threshold of [5000ms]
[2022-04-15T17:41:57,885][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/376272ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:45:58,110][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2m/376234576774ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:49:08,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/445157ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:52:33,561][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.4m/445510161957ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T17:55:28,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/368032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T17:57:59,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1m/368063366226ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:00:20,339][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/304905ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:01:13,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.search.SearchService$Reaper@150a185c, interval=1m}] took [305035ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:04:30,852][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305035025168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:03:41,331][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48024}] took [305035ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:07:19,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417765ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:05:25,987][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [1.5m/90898ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6e931853]], which exceeds the warn threshold of [10s]
[2022-04-15T18:11:59,139][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9m/417581340987ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:11:59,139][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_xpack?accept_enterprise=true][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:48014}] took [417581ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:16:16,541][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9m/534783ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:14:49,712][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [14.2s/14203ms] to notify listeners on unchanged cluster state for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@cfc3bd12], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@205e664a], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b4f712ef]], which exceeds the warn threshold of [10s]
[2022-04-15T18:20:41,175][INFO ][o.e.n.Node               ] [tpotcluster-node-01] version[7.17.0], pid[1], build[default/tar/bee86328705acaa9a6daede7140defd4d9ec56bd/2022-01-28T08:36:04.875279988Z], OS[Linux/4.19.0-20-cloud-amd64/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/16.0.2/16.0.2+7-alpine-r1]
[2022-04-15T18:20:41,199][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM home [/usr/lib/jvm/java-16-openjdk], using bundled JDK [false]
[2022-04-15T18:20:41,200][INFO ][o.e.n.Node               ] [tpotcluster-node-01] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Xms2048m, -Xmx2048m, -XX:MaxDirectMemorySize=1073741824, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/usr/share/elasticsearch, -Des.path.conf=/usr/share/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2022-04-15T18:20:47,916][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [aggs-matrix-stats]
[2022-04-15T18:20:47,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [analysis-common]
[2022-04-15T18:20:47,924][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [constant-keyword]
[2022-04-15T18:20:47,925][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [frozen-indices]
[2022-04-15T18:20:47,925][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-common]
[2022-04-15T18:20:47,925][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-geoip]
[2022-04-15T18:20:47,926][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [ingest-user-agent]
[2022-04-15T18:20:47,926][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [kibana]
[2022-04-15T18:20:47,927][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-expression]
[2022-04-15T18:20:47,927][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-mustache]
[2022-04-15T18:20:47,927][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [lang-painless]
[2022-04-15T18:20:47,928][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [legacy-geo]
[2022-04-15T18:20:47,928][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-extras]
[2022-04-15T18:20:47,929][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [mapper-version]
[2022-04-15T18:20:47,929][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [parent-join]
[2022-04-15T18:20:47,929][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [percolator]
[2022-04-15T18:20:47,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [rank-eval]
[2022-04-15T18:20:47,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [reindex]
[2022-04-15T18:20:47,930][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repositories-metering-api]
[2022-04-15T18:20:47,931][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-encrypted]
[2022-04-15T18:20:47,931][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [repository-url]
[2022-04-15T18:20:47,931][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [runtime-fields-common]
[2022-04-15T18:20:47,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [search-business-rules]
[2022-04-15T18:20:47,932][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [searchable-snapshots]
[2022-04-15T18:20:47,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [snapshot-repo-test-kit]
[2022-04-15T18:20:47,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [spatial]
[2022-04-15T18:20:47,933][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transform]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [transport-netty4]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [unsigned-long]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vector-tile]
[2022-04-15T18:20:47,934][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [vectors]
[2022-04-15T18:20:47,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [wildcard]
[2022-04-15T18:20:47,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-aggregate-metric]
[2022-04-15T18:20:47,935][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-analytics]
[2022-04-15T18:20:47,936][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async]
[2022-04-15T18:20:47,936][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-async-search]
[2022-04-15T18:20:47,936][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-autoscaling]
[2022-04-15T18:20:47,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ccr]
[2022-04-15T18:20:47,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-core]
[2022-04-15T18:20:47,937][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-data-streams]
[2022-04-15T18:20:47,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-deprecation]
[2022-04-15T18:20:47,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-enrich]
[2022-04-15T18:20:47,938][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-eql]
[2022-04-15T18:20:47,939][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-fleet]
[2022-04-15T18:20:47,939][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-graph]
[2022-04-15T18:20:47,939][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-identity-provider]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ilm]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-logstash]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-monitoring]
[2022-04-15T18:20:47,940][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-ql]
[2022-04-15T18:20:47,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-rollup]
[2022-04-15T18:20:47,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-security]
[2022-04-15T18:20:47,941][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-shutdown]
[2022-04-15T18:20:47,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-sql]
[2022-04-15T18:20:47,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-stack]
[2022-04-15T18:20:47,942][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-text-structure]
[2022-04-15T18:20:47,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-voting-only-node]
[2022-04-15T18:20:47,943][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] loaded module [x-pack-watcher]
[2022-04-15T18:20:47,944][INFO ][o.e.p.PluginsService     ] [tpotcluster-node-01] no plugins loaded
[2022-04-15T18:20:48,043][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] using [1] data paths, mounts [[/data (/dev/sda1)]], net usable_space [104.3gb], net total_space [125.8gb], types [ext4]
[2022-04-15T18:20:48,044][INFO ][o.e.e.NodeEnvironment    ] [tpotcluster-node-01] heap size [2gb], compressed ordinary object pointers [true]
[2022-04-15T18:20:48,638][INFO ][o.e.n.Node               ] [tpotcluster-node-01] node name [tpotcluster-node-01], node ID [t9hfPgy_RyC9LOJUxQUrSQ], cluster name [tpotcluster], roles [transform, data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
[2022-04-15T18:22:26,218][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9465ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:33,743][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.4s/9465130028ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:35,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [46991ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:22:35,771][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38262ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:35,772][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [38.2s/38261975048ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:22:58,278][INFO ][o.e.i.g.ConfigDatabases  ] [tpotcluster-node-01] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/usr/share/elasticsearch/config/ingest-geoip] for changes
[2022-04-15T18:22:58,287][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_LICENSE.txt]
[2022-04-15T18:22:58,288][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T18:22:58,290][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_LICENSE.txt]
[2022-04-15T18:22:58,291][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T18:22:58,291][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_COPYRIGHT.txt]
[2022-04-15T18:22:58,292][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_COPYRIGHT.txt]
[2022-04-15T18:22:58,293][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T18:22:58,293][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_README.txt]
[2022-04-15T18:22:58,294][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb_COPYRIGHT.txt]
[2022-04-15T18:22:58,295][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb_LICENSE.txt]
[2022-04-15T18:22:58,295][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T18:22:58,296][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T18:22:58,298][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] deleting stale file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb_elastic-geoip-database-service-agreement-LICENSE.txt]
[2022-04-15T18:22:58,299][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] initialized database registry, using geoip-databases directory [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ]
[2022-04-15T18:22:59,379][INFO ][o.e.t.NettyAllocator     ] [tpotcluster-node-01] creating NettyAllocator with the following configs: [name=elasticsearch_configured, chunk_size=1mb, suggested_max_allocation_size=1mb, factors={es.unsafe.use_netty_default_chunk_and_page_size=false, g1gc_enabled=true, g1gc_region_size=4mb}]
[2022-04-15T18:22:59,547][INFO ][o.e.d.DiscoveryModule    ] [tpotcluster-node-01] using discovery type [single-node] and seed hosts providers [settings]
[2022-04-15T18:23:00,595][INFO ][o.e.g.DanglingIndicesState] [tpotcluster-node-01] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2022-04-15T18:23:01,469][INFO ][o.e.n.Node               ] [tpotcluster-node-01] initialized
[2022-04-15T18:23:01,469][INFO ][o.e.n.Node               ] [tpotcluster-node-01] starting ...
[2022-04-15T18:23:01,598][INFO ][o.e.x.s.c.f.PersistentCache] [tpotcluster-node-01] persistent cache index loaded
[2022-04-15T18:23:01,599][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [tpotcluster-node-01] deprecation component started
[2022-04-15T18:23:01,847][INFO ][o.e.t.TransportService   ] [tpotcluster-node-01] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2022-04-15T18:23:04,347][INFO ][o.e.c.c.Coordinator      ] [tpotcluster-node-01] cluster UUID [Qbw2pof0QzSXC1OvK0aFSw]
[2022-04-15T18:23:04,521][INFO ][o.e.c.s.MasterService    ] [tpotcluster-node-01] elected-as-master ([1] nodes joined)[{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 250, version: 9869, delta: master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}
[2022-04-15T18:23:04,739][INFO ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] master node changed {previous [], current [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}]}, term: 250, version: 9869, reason: Publication{term=250, version=9869}
[2022-04-15T18:23:04,887][INFO ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] publish_address {192.168.48.2:9200}, bound_addresses {0.0.0.0:9200}
[2022-04-15T18:23:04,888][INFO ][o.e.n.Node               ] [tpotcluster-node-01] started
[2022-04-15T18:24:31,474][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [55.1s/55133ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:24:47,588][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [54.8s/54877333755ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:24:49,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [70411ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:24:52,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.3s/34371ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:24:56,638][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.6s/34627385527ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:00,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7852ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:03,104][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.8s/7852133259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:24,581][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24031ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:28,786][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24s/24030552232ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:32,360][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7580ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:36,024][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7580037029ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:42,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10235ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:46,094][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10234621280ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:49,935][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:25:56,955][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517664436ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:02,410][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11698ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:06,014][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11698226405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:10,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8869ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:13,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.8s/8868062263ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:16,200][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:18,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8s/5874287011ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:21,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:24,811][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5295815051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:28,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7061ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:32,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7061536935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:36,518][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7240ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:40,438][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.2s/7240051537ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:40,792][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5250ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:40,836][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5249757195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:26:41,606][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57126}] took [5946ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:26:43,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@653741e5] took [111244ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:26:45,862][INFO ][o.e.l.LicenseService     ] [tpotcluster-node-01] license [06f03395-4097-4495-8e63-b3dc92f4de14] mode [basic] - valid
[2022-04-15T18:26:49,469][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9870}] took [3.7m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9870}]] took [0ms], [connecting to new nodes] took [1ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [1ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [87ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [458ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [3ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [9ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [5ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [217896ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [49ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [35ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [22ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [145ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [58ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [13ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingComponent@6b5d5b43] took [0ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [1ms], [org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor@70204568] took [129ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [1ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [1ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [77ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [1ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [3355ms], [org.elasticsearch.xpack.monitoring.exporter.local.LocalExporter@7de096e0] took [1574ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [10ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [15ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:26:50,074][INFO ][o.e.g.GatewayService     ] [tpotcluster-node-01] recovered [52] indices into cluster_state
[2022-04-15T18:27:13,705][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [13620ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [48] unchanged indices
[2022-04-15T18:27:14,749][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [15.4s] publication of cluster state version [9871] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57126}] took [23626ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57130}] took [17718ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57132}] took [17518ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:16,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57128}] took [17718ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:41,934][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_cat/health][Netty4HttpChannel{localAddress=/127.0.0.1:9200, remoteAddress=/127.0.0.1:39092}] took [22653ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:27:54,414][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager_7.17.0/_doc/task%3AAlerting-alerting_health_check, params: {index=.kibana_task_manager_7.17.0, id=task:Alerting-alerting_health_check}
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.kibana_task_manager_7.17.0][_doc][task:Alerting-alerting_health_check]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:217) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:194) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:98) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:51) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:512) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.document.RestGetAction.lambda$prepareRequest$0(RestGetAction.java:91) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-15T18:27:55,246][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_7.17.0/_search, params: {rest_total_hits_as_int=true, size=20, index=.kibana_7.17.0, from=0}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-15T18:27:55,246][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-15T18:27:55,881][WARN ][r.suppressed             ] [tpotcluster-node-01] path: /.kibana_task_manager/_search, params: {ignore_unavailable=true, index=.kibana_task_manager, track_total_hits=true}
org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:725) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:412) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:757) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:509) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.performPhaseOnShard(AbstractSearchAsyncAction.java:320) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:256) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:466) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:211) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1048) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:763) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:399) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:487) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:285) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:101) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:122) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:109) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:327) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:393) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:245) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:382) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:461) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:357) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:32) [transport-netty4-client-7.17.0.jar:7.17.0]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:18) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:48) [transport-netty4-client-7.17.0.jar:7.17.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
	at java.lang.Thread.run(Thread.java:831) [?:?]
Caused by: org.elasticsearch.action.NoShardAvailableActionException
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:544) ~[elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:491) [elasticsearch-7.17.0.jar:7.17.0]
	... 79 more
[2022-04-15T18:27:57,188][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9871}] took [42.3s] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9871}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [41356ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [20ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [0ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [0ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [36ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [66ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [367ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [106ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [24ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [44ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [2ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [42ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [0ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [19ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [34ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [17ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:28:16,943][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] updating geoip databases
[2022-04-15T18:28:19,464][INFO ][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] fetching geoip databases overview from [https://geoip.elastic.co/v1/database?elastic_geoip_service_tos=agree]
[2022-04-15T18:28:21,248][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4614a747] took [5424ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:28:30,237][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [6795ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:28:47,787][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [74] timed out after [24152ms]
[2022-04-15T18:28:49,647][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [28.3s/28353ms] ago, timed out [4.2s/4201ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [74]
[2022-04-15T18:29:05,683][ERROR][o.e.i.g.GeoIpDownloader  ] [tpotcluster-node-01] exception during geoip databases update
java.net.UnknownHostException: geoip.elastic.co
	at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567) ~[?:?]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:333) ~[?:?]
	at java.net.Socket.connect(Socket.java:645) ~[?:?]
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300) ~[?:?]
	at sun.net.NetworkClient.doConnect(NetworkClient.java:177) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:497) ~[?:?]
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:600) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:265) ~[?:?]
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:379) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:189) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1232) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1120) ~[?:?]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:175) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1653) ~[?:?]
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1577) ~[?:?]
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527) ~[?:?]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:308) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.lambda$get$0(HttpClient.java:55) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?]
	at org.elasticsearch.ingest.geoip.HttpClient.doPrivileged(HttpClient.java:97) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.get(HttpClient.java:49) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.HttpClient.getBytes(HttpClient.java:40) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.fetchDatabasesOverview(GeoIpDownloader.java:135) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.updateDatabases(GeoIpDownloader.java:123) ~[ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloader.runDownloader(GeoIpDownloader.java:260) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:97) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.ingest.geoip.GeoIpDownloaderTaskExecutor.nodeOperation(GeoIpDownloaderTaskExecutor.java:43) [ingest-geoip-7.17.0.jar:7.17.0]
	at org.elasticsearch.persistent.NodePersistentTasksExecutor$1.doRun(NodePersistentTasksExecutor.java:42) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.17.0.jar:7.17.0]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.17.0.jar:7.17.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630) [?:?]
	at java.lang.Thread.run(Thread.java:831) [?:?]
[2022-04-15T18:29:16,880][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb.tmp.gz]
[2022-04-15T18:29:16,922][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb.tmp.gz]
[2022-04-15T18:29:16,922][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb.tmp.gz]
[2022-04-15T18:29:23,810][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@7a91bbf] took [5644ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:29:30,794][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-Country.mmdb]
[2022-04-15T18:29:32,406][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-ASN.mmdb]
[2022-04-15T18:29:32,717][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][89] overhead, spent [468ms] collecting in the last [1.7s]
[2022-04-15T18:29:36,258][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][91] overhead, spent [378ms] collecting in the last [1.2s]
[2022-04-15T18:30:29,577][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11549ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:30:49,979][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.5s/11549193224ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:12,925][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30167ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:17,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [30.1s/30167218663ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:26,210][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27690ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:31,307][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [27.6s/27689965628ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:31:49,319][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24275ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:06,153][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [24.2s/24274841745ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:14,756][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25384ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:24,183][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.3s/25384450876ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:29,627][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15200ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:25,767][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9885}] took [2.4m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9885}]] took [0ms], [connecting to new nodes] took [69ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [1004ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [4235ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [30ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [33ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [65ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [26ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [50ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [501ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [1894ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [229ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [0ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [3127ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [1491ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [12504ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [53ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [37883ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [23721ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [227ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [1527ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [78ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [15ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [71ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [201ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [67ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@5e0c4e03]]]] took [15732ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@ba0f0a3]]]] took [30743ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@25ef4db1]]]] took [9197ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@4f53ad21]]]] took [12567ms]
[2022-04-15T18:32:32,407][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15200112406ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:44,630][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57180}] took [15200ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:32:45,539][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:48,674][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.3s/15395924278ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:51,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6412ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:51,585][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4f53c0da] took [176426ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:32:53,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.4s/6411901117ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:56,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5357ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:32:58,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [5356ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:32:59,101][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3s/5356698394ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:02,059][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [5097ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:33:31,781][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:32,620][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12031283867ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:39,681][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:41,550][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7726ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:33:42,654][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6129855847ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:46,663][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7094ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:44,602][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [7726ms] which is above the warn threshold of [5s]
[2022-04-15T18:33:49,774][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7s/7094112922ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:53,480][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:33:58,063][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6574704227ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:05,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11396ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:06,119][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57180}] took [17970ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:12,209][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.3s/11396172032ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:18,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13245ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:23,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [38309ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:23,138][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.2s/13244869723ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:26,928][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8984ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:31,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.9s/8984618601ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:30,835][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57182}] took [8985ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:34,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6925ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:35,287][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57186}] took [6925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:34,867][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57184}] took [6925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:34:43,401][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6925091854ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:50,759][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16604ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:53,429][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.6s/16603400115ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:34:55,846][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:00,374][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1s/5144155346ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:07,343][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11639ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:23,661][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11639440320ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:27,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20085ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:32,424][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20s/20085199770ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:27,442][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [124325ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T18:35:38,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:41,889][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11s/11019135935ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:44,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6688ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:35:43,687][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [2.7m] publication of cluster state version [9886] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:35:55,238][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.6s/6688525846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:00,097][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15016ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:03,140][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15s/15015661204ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:07,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:12,917][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.5s/7517402059ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:18,416][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10658ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:21,794][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [10658ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:22,432][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10657671230ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:26,457][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:30,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8146308736ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:35,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8727ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:40,113][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.7s/8726906670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:44,790][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [8727ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:44,699][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9780ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:49,376][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@63b2653e] took [109275ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:48,182][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.7s/9779743896ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:51,358][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:36:51,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [6562ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:51,938][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [6562ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:36:55,596][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.5s/6562058405ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:01,347][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9539ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:02,445][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9538ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:05,159][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9538709861ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:10,799][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9634ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:15,716][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9634308706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:20,871][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9806ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:24,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9806416089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:30,594][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9964ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:35,308][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9963917148ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:36,760][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9963ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:38,937][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8413ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:37,083][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9964ms] which is above the warn threshold of [5s]
[2022-04-15T18:37:44,395][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57190}] took [8413ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:43,958][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.4s/8412625501ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:47,921][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9245ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:49,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9245ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:37:50,542][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.2s/9245455814ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:53,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5226ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:37:55,629][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5225695569ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:00,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:01,135][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9886}] took [2.2m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9886}]] took [176ms], [connecting to new nodes] took [944ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [14923ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [9153ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [215ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [205ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [58ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [19698ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [589ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [1164ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [334ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [51ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [110ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [46156ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [13477ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [438ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [110ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [3657ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [267ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [2972ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [42ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [4365ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [1210ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [73ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [0ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:38:01,164][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6886616489ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:01,761][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [5.2m/314904ms] which is longer than the warn threshold of [300000ms]; there are currently [3] pending tasks, the oldest of which has age [8.2m/494217ms]
[2022-04-15T18:38:19,951][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [12.4s] publication of cluster state version [9887] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:38:32,847][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12604ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:38:49,984][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15457ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:38:55,091][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.4s/15457270051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:03,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12377ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:09,832][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.3s/12376792147ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:10,636][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57200}] took [12377ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:39:19,737][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17236ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:28,034][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.2s/17235685524ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:45,540][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25032ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:39:47,479][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [25031ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:39:55,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25s/25031605108ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:06,908][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57200}] took [25031ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:40:07,555][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21805ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:20,805][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21805840195ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:32,941][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25776ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:46,354][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.7s/25775216831ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:40:56,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23451ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:07,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.4s/23451873749ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:17,999][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22351ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:28,926][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [22.3s/22350245512ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:39,004][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20874ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:49,201][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.8s/20873953033ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:41:59,471][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20152ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:08,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.1s/20152186343ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:17,893][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18515ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:24,368][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.5s/18514968381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:30,922][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13330ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:36,835][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13330442511ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:42,275][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11213ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:46,033][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11212412231ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:50,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8319ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:54,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.3s/8319361007ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:57,009][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6799ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:42:59,188][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@7c3c46bd] took [192585ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:43:00,337][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6798756459ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:02,913][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6057ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:03,928][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9887}] took [4.1m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9887}]] took [98ms], [connecting to new nodes] took [1023ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [7798ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [14005ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [484ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [282ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [170ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [6036ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [2157ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [499ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [182ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [155547ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [79ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [22266ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [711ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [402ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [13093ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [479ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [7929ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [54ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [4823ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [2525ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [959ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [114ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [227ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [0ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [62ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms], [ClusterStateObserver[ObservingContext[ContextPreservingListener[org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2@4b0a729d]]]] took [3028ms]
[2022-04-15T18:43:06,450][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6057664874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:09,092][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [10.2m/617952ms] which is longer than the warn threshold of [300000ms]; there are currently [1] pending tasks, the oldest of which has age [10.3m/621014ms]
[2022-04-15T18:43:09,160][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6257ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:09,289][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.2s/6256501427ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:43:23,590][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11760ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-15T18:43:24,239][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [14.1s] publication of cluster state version [9888] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:43:30,648][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/.kibana_task_manager/_update_by_query?ignore_unavailable=true&refresh=true&conflicts=proceed][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.3:49028}] took [17801ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:43:45,853][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][137][11] duration [2.1s], collections [1]/[1.1s], total [2.1s]/[3.2s], memory [261.5mb]->[305.5mb]/[2gb], all_pools {[young] [156mb]->[0b]/[0b]}{[old] [98.7mb]->[98.7mb]/[2gb]}{[survivor] [6.7mb]->[13.4mb]/[0b]}
[2022-04-15T18:43:47,560][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][137] overhead, spent [2.1s] collecting in the last [1.1s]
[2022-04-15T18:43:50,834][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [10589ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:43:53,552][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [13.8s/13857ms] to compute cluster state update for [ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.16.2-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@1377531d], ilm-set-step-info {policy [ilm-history-ilm-policy], index [.ds-ilm-history-5-2022.04.11-000002], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@6411fc55], ilm-set-step-info {policy [.deprecation-indexing-ilm-policy], index [.ds-.logs-deprecation.elasticsearch-default-2022.04.09-000003], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@f8aaa8fa], ilm-set-step-info {policy [kibana-event-log-policy], index [.kibana-event-log-7.17.0-000001], currentStep [{"phase":"hot","action":"rollover","name":"check-rollover-ready"}]}[org.elasticsearch.xpack.ilm.SetStepInfoUpdateTask@b246ae5e]], which exceeds the warn threshold of [10s]
[2022-04-15T18:44:20,975][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@5db5e0c6] took [5066ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:44:34,388][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][154][12] duration [3s], collections [1]/[5.3s], total [3s]/[6.3s], memory [180.2mb]->[116.6mb]/[2gb], all_pools {[young] [68mb]->[0b]/[0b]}{[old] [98.7mb]->[110.9mb]/[2gb]}{[survivor] [13.4mb]->[5.6mb]/[0b]}
[2022-04-15T18:44:34,962][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][154] overhead, spent [3s] collecting in the last [5.3s]
[2022-04-15T18:44:34,987][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [5925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:44:55,672][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [6439ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:45:05,803][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57202}] took [10839ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:46:35,512][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5076ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:46:43,819][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5s/5075893122ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:46:51,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16897ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:46:56,798][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.8s/16896414874ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:03,630][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12575ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:07,048][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@6fe235c0] took [121063ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:47:09,857][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12575315599ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:14,125][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10350ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:15,956][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [10349ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:47:22,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10349790648ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:30,422][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13470ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:35,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [13470ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:47:36,538][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13470504449ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:42,813][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13704ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:49,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.7s/13704101318ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:47:55,465][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13686ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:01,763][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.6s/13685309744ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:09,862][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14702ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:15,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14702389375ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:22,498][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:26,873][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12572455168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:20,176][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [14703ms] which is above the warn threshold of [5s]
[2022-04-15T18:48:32,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10349ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:43,942][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.3s/10348976756ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:52,814][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19947ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:48:57,961][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [30296ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:49:00,306][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.9s/19947052169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:07,075][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14156ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:17,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14155048846ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:26,912][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19807ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:30,640][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [19807ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:49:34,604][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.8s/19807432467ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:41,967][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15297ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:49,317][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15297418544ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:54,662][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12713ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:49:59,775][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12712ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:50:02,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.7s/12712676850ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:10,818][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16113ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:17,241][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16113125027ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:22,350][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12020ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:29,077][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12019842305ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:34,684][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11888ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:41,058][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.8s/11887725293ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:49,142][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14096ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:54,564][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14s/14096523545ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:01,328][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12597ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:50:52,465][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [343332ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T18:51:08,890][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.5s/12596536300ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:19,988][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18749ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:28,265][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18.7s/18748848676ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:38,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18033ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:18,488][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [6.4m] publication of cluster state version [9891] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T18:51:45,467][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [18s/18033063319ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:51:53,111][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14649ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:00,546][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.6s/14649192228ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:07,463][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14146ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:15,949][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.1s/14145840497ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:24,037][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17190ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:31,197][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17189876446ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:34,008][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4b275c7e] took [149480ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:52:37,991][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13822ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:45,171][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.8s/13822520526ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:50,557][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12625ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:52:58,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12624921036ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:02,162][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12624ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:53:07,329][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16107ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:15,802][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.1s/16106886622ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:21,698][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15261ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:24,932][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.2s/15260558073ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:29,966][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:34,067][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.9s/7945581248ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:39,947][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9540ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:33,785][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [408] timed out after [57815ms]
[2022-04-15T18:53:44,192][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.5s/9539554154ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:41,355][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [9539ms] which is above the warn threshold of [5s]
[2022-04-15T18:53:50,513][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [10266ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:53:49,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10266ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:55,035][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10266620571ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:02,061][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11926ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:09,447][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.9s/11925978531ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:53:58,359][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/85567ms] ago, timed out [27.7s/27752ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [408]
[2022-04-15T18:54:15,426][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [11925ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:54:17,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15508ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:21,569][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.5s/15507620960ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:29,214][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11745ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:36,482][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11744891203ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:41,531][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12662ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:47,610][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [24407ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:54:49,071][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12662264407ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:54:56,643][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15165ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:02,338][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15164775618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:07,714][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10610ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:15,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.6s/10610435308ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:22,494][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14975ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:30,378][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.9s/14974763719ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:35,592][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9891}] took [3.4m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9891}]] took [276ms], [connecting to new nodes] took [1356ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [79ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [41378ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [10686ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [86ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [74ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [0ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [3116ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [1104ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [428ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [1ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [253ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [68747ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [219ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [19224ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [899ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [242ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [18427ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [1774ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [18873ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [69ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [15673ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [12170ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [2030ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [79ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [232ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [756ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [65ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [147ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T18:55:40,487][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17132ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:49,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [17.1s/17131500762ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:55:58,768][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [11.9m/719303ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [12.5m/750655ms]
[2022-04-15T18:55:58,768][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19581ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:04,168][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.5s/19581077344ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:12,406][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13331ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:19,230][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13331061506ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:28,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14726ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:36,017][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [14.7s/14726649479ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:47,993][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21029ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:53,968][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21s/21029125089ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:56:58,085][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9945ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:00,475][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@6537fcd8] took [136493ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:00,393][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.9s/9944582355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:03,863][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6003ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:06,725][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6s/6003225212ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:06,725][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [59s/59034ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-15T18:57:11,724][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7390ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:14,080][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7390ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:16,936][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [7.3s/7390154306ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:25,284][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13337ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:34,997][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.3s/13336370355ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:41,130][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16305ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:41,680][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [16304ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:43,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [16.3s/16304809725ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:46,970][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6140ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:49,784][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.1s/6139961081ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:52,161][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [6139ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:57:54,032][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6895ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:57:48,490][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6140ms] which is above the warn threshold of [5s]
[2022-04-15T18:57:59,519][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.8s/6895165533ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:03,992][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9864ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:09,718][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.8s/9864708841ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:16,688][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [22536ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:58:16,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12672ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:20,529][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12.6s/12671448706ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:26,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10404ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:28,051][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57204}] took [10404ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:58:34,056][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.4s/10404046759ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:40,976][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13420ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:42,458][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [13419ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:58:55,708][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [13.4s/13419878775ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:58:59,437][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:04,459][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.2s/19296255383ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:09,893][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57204}] took [19297ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:59:09,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10290ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:16,691][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [10.2s/10289723068ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:15,830][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [10289ms] which is above the warn threshold of [5000ms]
[2022-04-15T18:59:22,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12045ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:25,533][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [12s/12044892518ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:28,923][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6959ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:41,451][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.9s/6959486565ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:43,884][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15144ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:45,397][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [15.1s/15144118498ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T18:59:55,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11608ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:03,251][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.6s/11607929826ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:07,466][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:11,609][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.7s/11779268274ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:15,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8150ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:18,969][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [8.1s/8149491377ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:22,096][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6751ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:24,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [6.7s/6750758018ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:39,344][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9613ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:41,838][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.6s/9613468829ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:50,579][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11254ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:54,974][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [11.2s/11253467808ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:00:54,522][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/_license][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57204}] took [11254ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:00:54,970][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [206081ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-15T19:01:01,242][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@5115e147] took [94017ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:00:55,676][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5268ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:01,384][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][HEAD][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57206}] took [5268ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:01:01,896][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.2s/5268313169ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:05,124][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9329ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:01,896][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [3.7m] publication of cluster state version [9892] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:01:05,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9.3s/9329138051ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:01:05,621][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9954ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:01:08,178][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [17.4m/1048501ms] which is longer than the warn threshold of [300000ms]; there are currently [4] pending tasks, the oldest of which has age [14m/845109ms]
[2022-04-15T19:01:12,678][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][176][13] duration [1.9s], collections [1]/[1s], total [1.9s]/[8.3s], memory [184.6mb]->[204.6mb]/[2gb], all_pools {[young] [68mb]->[88mb]/[0b]}{[old] [110.9mb]->[110.9mb]/[2gb]}{[survivor] [5.6mb]->[5.6mb]/[0b]}
[2022-04-15T19:01:13,081][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][176] overhead, spent [1.9s] collecting in the last [1s]
[2022-04-15T19:01:18,707][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][178][14] duration [1.5s], collections [1]/[3.7s], total [1.5s]/[9.9s], memory [137.4mb]->[124.8mb]/[2gb], all_pools {[young] [16mb]->[12mb]/[0b]}{[old] [110.9mb]->[115.7mb]/[2gb]}{[survivor] [10.5mb]->[9mb]/[0b]}
[2022-04-15T19:01:19,216][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][178] overhead, spent [1.5s] collecting in the last [3.7s]
[2022-04-15T19:01:24,513][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][179][15] duration [2s], collections [1]/[2.5s], total [2s]/[12s], memory [124.8mb]->[208.8mb]/[2gb], all_pools {[young] [12mb]->[4mb]/[0b]}{[old] [115.7mb]->[115.7mb]/[2gb]}{[survivor] [9mb]->[13.7mb]/[0b]}
[2022-04-15T19:01:24,854][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][179] overhead, spent [2s] collecting in the last [2.5s]
[2022-04-15T19:01:30,141][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [11143ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [1] indices and skipped [51] unchanged indices
[2022-04-15T19:01:39,291][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [24.7s] publication of cluster state version [9893] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:01:45,419][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57206}] took [5595ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:00,909][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@24d5ea58] took [21424ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:01,973][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57210}] took [6651ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:23,126][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9066ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:28,308][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57214}] took [6804ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:33,844][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [5203ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:02:34,151][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [535] timed out after [26140ms]
[2022-04-15T19:03:03,216][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9352ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:03:20,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7200ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:03:40,431][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9893}] took [1.8m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9893}]] took [0ms], [connecting to new nodes] took [262ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [8105ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [6883ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [96ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [65ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [64ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [217ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [373ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [1973ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [530ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [79ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [41144ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [10684ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [590ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [521ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [14320ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [811ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [10619ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [74ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [6480ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [3959ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [938ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [0ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [142ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [140ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [476ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [0ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T19:04:56,890][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [32.2s/32202ms] to compute cluster state update for [shard-started StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.15][0]], allocationId [GekoCVcoSgiAIPPGQR_JzQ], primary term [2], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.15][0]], allocationId [GekoCVcoSgiAIPPGQR_JzQ], primary term [2], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}[StartedShardEntry{shardId [[logstash-2022.04.08][0]], allocationId [jZAvuBUaTY6js5KKcqqeDw], primary term [22], message [after existing store recovery; bootstrap_history_uuid=false]}], shard-started StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}[StartedShardEntry{shardId [[logstash-2022.04.07][0]], allocationId [vnxzsWpCRIuHI38zk4VBgA], primary term [24], message [master {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]}]], which exceeds the warn threshold of [10s]
[2022-04-15T19:04:51,259][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [2.8m/168599ms] ago, timed out [2.3m/142459ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [535]
[2022-04-15T19:06:41,507][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@1f7a1064] took [177710ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:06:27,864][INFO ][o.e.i.g.DatabaseNodeService] [tpotcluster-node-01] successfully reloaded changed geoip database file [/tmp/geoip-databases/t9hfPgy_RyC9LOJUxQUrSQ/GeoLite2-City.mmdb]
[2022-04-15T19:07:02,283][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@74802a97, interval=5s}] took [10928ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:07:31,423][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [12240ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:05,372][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9066ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:08:05,867][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [11067ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:09,272][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [9s/9066619793ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:08:09,270][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [10267ms] which is above the warn threshold of [5s]
[2022-04-15T19:08:05,594][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [586] timed out after [74520ms]
[2022-04-15T19:08:06,737][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [1.4m/85988ms] ago, timed out [11.4s/11468ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [586]
[2022-04-15T19:08:10,430][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [5480ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:27,029][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9205ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:08:48,537][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [9915ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:09:20,934][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57216}] took [11607ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:09:41,788][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [187703ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [3] indices and skipped [49] unchanged indices
[2022-04-15T19:09:47,534][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@49c9a0fe] took [54470ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:09:47,558][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [4.4m] publication of cluster state version [9894] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:10:07,051][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [8430ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:10:34,707][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [11292ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:10:54,810][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [8521ms] which is above the warn threshold of [5s]
[2022-04-15T19:12:23,911][WARN ][o.e.c.s.ClusterApplierService] [tpotcluster-node-01] cluster state applier task [Publication{term=250, version=9894}] took [2.5m] which is above the warn threshold of [30s]: [running task [Publication{term=250, version=9894}]] took [0ms], [connecting to new nodes] took [0ms], [applying settings] took [0ms], [org.elasticsearch.repositories.RepositoriesService@70e4fc16] took [0ms], [org.elasticsearch.indices.cluster.IndicesClusterStateService@40cc8fb4] took [125ms], [org.elasticsearch.script.ScriptService@34cad3c2] took [0ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [0ms], [org.elasticsearch.snapshots.RestoreService@23fc1f4e] took [0ms], [org.elasticsearch.ingest.IngestService@76985aa] took [199ms], [org.elasticsearch.action.ingest.IngestActionForwarder@655f1246] took [36ms], [org.elasticsearch.action.admin.cluster.repositories.cleanup.TransportCleanupRepositoryAction$$Lambda$4526/0x00000008016d1540@6bb2bcbd] took [0ms], [org.elasticsearch.indices.TimestampFieldMapperService@2877c8ea] took [0ms], [org.elasticsearch.tasks.TaskManager@63785c9b] took [0ms], [org.elasticsearch.snapshots.SnapshotsService@7e28bf90] took [73ms], [org.elasticsearch.cluster.InternalClusterInfoService@4fa32e3a] took [0ms], [org.elasticsearch.snapshots.InternalSnapshotsInfoService@27dcffe] took [0ms], [org.elasticsearch.indices.SystemIndexManager@7df843a1] took [331ms], [org.elasticsearch.xpack.shutdown.NodeSeenService@515ff05f] took [0ms], [org.elasticsearch.xpack.autoscaling.capacity.memory.AutoscalingMemoryInfoService$$Lambda$3179/0x0000000801306e58@33006b83] took [0ms], [org.elasticsearch.xpack.ccr.action.ShardFollowTaskCleaner@7b891420] took [0ms], [org.elasticsearch.xpack.enrich.EnrichPolicyMaintenanceService@7954f5c] took [0ms], [org.elasticsearch.xpack.transform.notifications.TransformAuditor$$Lambda$3197/0x000000080140e000@6511fa33] took [0ms], [org.elasticsearch.xpack.transform.TransformClusterStateListener@2dd37f79] took [0ms], [org.elasticsearch.xpack.stack.StackTemplateRegistry@42619992] took [69634ms], [org.elasticsearch.xpack.searchablesnapshots.cache.blob.BlobStoreCacheMaintenanceService@124d12b2] took [0ms], [org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots$RepositoryUuidWatcher@4593f695] took [0ms], [org.elasticsearch.xpack.watcher.support.WatcherIndexTemplateRegistry@3f31b417] took [22783ms], [org.elasticsearch.xpack.watcher.WatcherLifeCycleService@74ccc621] took [715ms], [org.elasticsearch.xpack.watcher.WatcherIndexingListener@7d2e601b] took [758ms], [org.elasticsearch.xpack.ilm.history.ILMHistoryTemplateRegistry@27531675] took [12041ms], [org.elasticsearch.xpack.ilm.IndexLifecycleService@742f4991] took [1544ms], [org.elasticsearch.xpack.core.slm.history.SnapshotLifecycleTemplateRegistry@15f08f84] took [14591ms], [org.elasticsearch.xpack.slm.SnapshotLifecycleService@79fbed5] took [0ms], [org.elasticsearch.xpack.slm.SnapshotRetentionService@27fd528e] took [0ms], [org.elasticsearch.xpack.deprecation.logging.DeprecationIndexingTemplateRegistry@11800668] took [14301ms], [org.elasticsearch.xpack.fleet.FleetTemplateRegistry@6102bc66] took [9414ms], [org.elasticsearch.cluster.metadata.SystemIndexMetadataUpgradeService@71a69f6c] took [0ms], [org.elasticsearch.cluster.metadata.TemplateUpgradeService@18a6a90a] took [3863ms], [org.elasticsearch.node.ResponseCollectorService@6c33d7f4] took [136ms], [org.elasticsearch.snapshots.SnapshotShardsService@1ecac41f] took [0ms], [org.elasticsearch.persistent.PersistentTasksClusterService@5b526555] took [0ms], [org.elasticsearch.shutdown.PluginShutdownService@761169cc] took [0ms], [org.elasticsearch.cluster.routing.DelayedAllocationService@486995d6] took [59ms], [org.elasticsearch.indices.store.IndicesStore@2b2721e1] took [183ms], [org.elasticsearch.persistent.PersistentTasksNodeService@38553b15] took [0ms], [org.elasticsearch.license.LicenseService@79237b37] took [0ms], [org.elasticsearch.xpack.ccr.action.AutoFollowCoordinator@6f5a9894] took [60ms], [org.elasticsearch.xpack.core.async.AsyncTaskMaintenanceService@4bb88d06] took [0ms], [org.elasticsearch.gateway.GatewayService@4ffbb098] took [0ms], [org.elasticsearch.indices.recovery.PeerRecoverySourceService@450438a4] took [0ms]
[2022-04-15T19:12:41,543][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] pending task queue has been nonempty for [8.6m/520271ms] which is longer than the warn threshold of [300000ms]; there are currently [9] pending tasks, the oldest of which has age [11.1m/670692ms]
[2022-04-15T19:12:57,421][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4d812620] took [135688ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:05,050][WARN ][o.e.c.s.MasterService    ] [tpotcluster-node-01] took [15.7s/15753ms] to compute cluster state update for [cluster_reroute(reroute after starting shards)], which exceeds the warn threshold of [10s]
[2022-04-15T19:13:08,075][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57216}] took [164408ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:25,606][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [6670ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:38,272][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57218}] took [5720ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:13:38,143][WARN ][o.e.g.PersistedClusterStateService] [tpotcluster-node-01] writing cluster state took [28234ms] which is above the warn threshold of [10s]; wrote global metadata [false] and metadata for [4] indices and skipped [48] unchanged indices
[2022-04-15T19:13:43,850][INFO ][o.e.c.c.C.CoordinatorPublication] [tpotcluster-node-01] after [35.1s] publication of cluster state version [9895] is still waiting for {tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true} [SENT_PUBLISH_REQUEST]
[2022-04-15T19:14:27,749][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57224}] took [9406ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:14:45,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@3988865f] took [59579ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:18,100][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20525ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:15:19,530][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.5s/20524726052ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:15:21,432][WARN ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][young][211][16] duration [13.7s], collections [1]/[30.1s], total [13.7s]/[25.7s], memory [205.4mb]->[136.2mb]/[2gb], all_pools {[young] [76mb]->[8mb]/[0b]}{[old] [115.7mb]->[120.9mb]/[2gb]}{[survivor] [13.7mb]->[11.2mb]/[0b]}
[2022-04-15T19:15:25,831][INFO ][o.e.m.j.JvmGcMonitorService] [tpotcluster-node-01] [gc][211] overhead, spent [13.7s] collecting in the last [30.1s]
[2022-04-15T19:15:28,505][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [10409ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:40,251][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][GET][/][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57226}] took [12262ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:41,057][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [7259ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:15:42,743][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [6649ms] which is above the warn threshold of [5s]
[2022-04-15T19:18:50,880][WARN ][o.e.h.AbstractHttpServerTransport] [tpotcluster-node-01] handling request [null][POST][/_bulk][Netty4HttpChannel{localAddress=/192.168.48.2:9200, remoteAddress=/192.168.48.5:57226}] took [48923ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:19:06,250][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@35354b1f] took [188017ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:20:37,417][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32063ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:10,000][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [32s/32063153765ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:24,455][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47543ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:27,229][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [91539ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:21:41,149][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47542955654ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:21:55,595][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31013ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:03,605][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [31s/31013618907ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:19,215][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23605ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:30,411][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.6s/23604145509ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:41,660][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21819ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:22:55,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [21.8s/21819102996ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:08,266][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25564ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:39,408][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [25.5s/25564011493ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:45,951][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39665ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:52,030][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [39.6s/39665831381ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:23:53,227][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [39665ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:23:38,179][WARN ][o.e.c.InternalClusterInfoService] [tpotcluster-node-01] failed to retrieve shard stats from node [t9hfPgy_RyC9LOJUxQUrSQ]: [tpotcluster-node-01][127.0.0.1:9300][indices:monitor/stats[n]] request_id [753] timed out after [216443ms]
[2022-04-15T19:24:06,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20296ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:24:10,671][WARN ][o.e.t.TransportService   ] [tpotcluster-node-01] Received response for a request that has timed out, sent [5m/301968ms] ago, timed out [1.4m/85525ms] ago, action [indices:monitor/stats[n]], node [{tpotcluster-node-01}{t9hfPgy_RyC9LOJUxQUrSQ}{EkMOxjLvTcCbtvSVPuzn8Q}{127.0.0.1}{127.0.0.1:9300}{cdfhimrstw}{xpack.installed=true, transform.node=true}], id [753]
[2022-04-15T19:24:17,270][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [20.2s/20295299618ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:24:30,212][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23887ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:24:39,365][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.8s/23887512805ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:16,614][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46067ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:25,807][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46s/46066807928ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:36,239][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19464ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:49,066][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [19.4s/19464267448ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:25:59,516][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23561ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:26:17,895][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [23.5s/23560624101ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:26:34,231][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34418ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:26:40,226][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [57978ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:26:51,269][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [34.4s/34418354432ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:27:29,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.1s/53193ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:27:40,868][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@1b7e5cb3, interval=5s}] took [53192ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:27:49,843][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.1s/53192378789ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:28:17,320][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47537ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:28:47,697][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [47.5s/47537136577ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:29:14,267][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.9s/58999ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:29:33,578][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [58.9s/58999100636ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:30:00,869][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46325ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:30:24,723][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [46.3s/46325342713ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:30:54,070][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53437ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:31:21,532][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.4s/53436342063ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:31:45,892][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51173ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:32:15,822][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [51.1s/51173605259ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:32:39,906][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.6s/53692ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:33:09,453][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [53.5s/53592511253ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:33:45,253][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64015ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:34:06,433][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [1m/64114500916ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:34:43,154][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59562ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:35:36,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [59.5s/59561714321ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:37:10,797][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/146589ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:40:22,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [2.4m/146588930168ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:40:08,446][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$5100/0x00000008017e6ea8@4db40ea9] took [581329ms] which is above the warn threshold of [5000ms]
[2022-04-15T19:42:59,776][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339077ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:45:42,005][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338730747731ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:48:43,185][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350566ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:51:33,965][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.8m/350506481508ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:54:11,052][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/323792ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T19:57:03,603][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.3m/323685946312ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T19:59:45,156][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/338760ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:02:35,409][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.6m/339128869347ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:05:17,634][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326402ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:07:33,734][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326109892487ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:10:19,552][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309130ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:12:43,882][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309565056075ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:15:13,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293438ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:17:34,520][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/293438112670ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:20:17,477][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305042ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:15:49,967][WARN ][o.e.m.f.FsHealthService  ] [tpotcluster-node-01] health check of [/data/elk/data/nodes/0] took [603003ms] which is above the warn threshold of [5s]
[2022-04-15T20:22:46,180][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/305041546419ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:25:19,007][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290570ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:27:45,592][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [4.8m/290027092567ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:30:19,232][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309037ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:32:48,103][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.1m/309163373271ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:35:18,303][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301135ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:38:00,217][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5m/301289647152ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:41:07,767][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/325866ms] on absolute clock which is above the warn threshold of [5000ms]
[2022-04-15T20:43:45,897][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] timer thread slept for [5.4m/326128119452ns] on relative clock which is above the warn threshold of [5000ms]
[2022-04-15T20:44:11,371][WARN ][o.e.t.ThreadPool         ] [tpotcluster-node-01] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@3afa4c01, interval=1s}] took [326128ms] which is above the warn threshold of [5000ms]
